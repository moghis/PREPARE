{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# load libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.model_selection import StratifiedGroupKFold,StratifiedShuffleSplit,KFold,train_test_split\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "from scipy.optimize import minimize\n",
    "from tqdm import tqdm\n",
    "from xgboost import XGBRegressor\n",
    "import copy\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "import optuna\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data\n",
    "train=pd.read_csv(\"dataset/train_features.csv\")\n",
    "y=pd.read_csv(\"dataset/train_labels.csv\")\n",
    "test=pd.read_csv(\"dataset/test_features.csv\")\n",
    "ss=pd.read_csv(\"dataset/submission_format.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>age_03</th>\n",
       "      <th>urban_03</th>\n",
       "      <th>married_03</th>\n",
       "      <th>n_mar_03</th>\n",
       "      <th>edu_gru_03</th>\n",
       "      <th>n_living_child_03</th>\n",
       "      <th>migration_03</th>\n",
       "      <th>glob_hlth_03</th>\n",
       "      <th>adl_dress_03</th>\n",
       "      <th>...</th>\n",
       "      <th>rrelgimp_12</th>\n",
       "      <th>rrfcntx_m_12</th>\n",
       "      <th>rsocact_m_12</th>\n",
       "      <th>rrelgwk_12</th>\n",
       "      <th>a16a_12</th>\n",
       "      <th>a21_12</th>\n",
       "      <th>a22_12</th>\n",
       "      <th>a33b_12</th>\n",
       "      <th>a34_12</th>\n",
       "      <th>j11_12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aace</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.somewhat important</td>\n",
       "      <td>9.Never</td>\n",
       "      <td>9.Never</td>\n",
       "      <td>0.No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Concrete 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aanz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.very important</td>\n",
       "      <td>9.Never</td>\n",
       "      <td>1.Almost every day</td>\n",
       "      <td>0.No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Concrete 2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 184 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    uid age_03 urban_03 married_03  n_mar_03 edu_gru_03 n_living_child_03  \\\n",
       "0  aace    NaN      NaN        NaN       NaN        NaN               NaN   \n",
       "1  aanz    NaN      NaN        NaN       NaN        NaN               NaN   \n",
       "\n",
       "   migration_03 glob_hlth_03  adl_dress_03  ...           rrelgimp_12  \\\n",
       "0           NaN          NaN           NaN  ...  2.somewhat important   \n",
       "1           NaN          NaN           NaN  ...      1.very important   \n",
       "\n",
       "   rrfcntx_m_12        rsocact_m_12  rrelgwk_12  a16a_12  a21_12  a22_12  \\\n",
       "0       9.Never             9.Never        0.No      NaN     NaN     NaN   \n",
       "1       9.Never  1.Almost every day        0.No      NaN     NaN     NaN   \n",
       "\n",
       "   a33b_12  a34_12      j11_12  \n",
       "0      NaN     NaN  Concrete 2  \n",
       "1      NaN     NaN  Concrete 2  \n",
       "\n",
       "[2 rows x 184 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>age_03</th>\n",
       "      <th>urban_03</th>\n",
       "      <th>married_03</th>\n",
       "      <th>n_mar_03</th>\n",
       "      <th>edu_gru_03</th>\n",
       "      <th>n_living_child_03</th>\n",
       "      <th>migration_03</th>\n",
       "      <th>glob_hlth_03</th>\n",
       "      <th>adl_dress_03</th>\n",
       "      <th>...</th>\n",
       "      <th>rrelgimp_12</th>\n",
       "      <th>rrfcntx_m_12</th>\n",
       "      <th>rsocact_m_12</th>\n",
       "      <th>rrelgwk_12</th>\n",
       "      <th>a16a_12</th>\n",
       "      <th>a21_12</th>\n",
       "      <th>a22_12</th>\n",
       "      <th>a33b_12</th>\n",
       "      <th>a34_12</th>\n",
       "      <th>j11_12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abxu</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wood, mosaic, or other covering 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aeol</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.very important</td>\n",
       "      <td>9.Never</td>\n",
       "      <td>9.Never</td>\n",
       "      <td>1.Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Concrete 2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 184 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    uid age_03 urban_03 married_03  n_mar_03 edu_gru_03 n_living_child_03  \\\n",
       "0  abxu    NaN      NaN        NaN       NaN        NaN               NaN   \n",
       "1  aeol    NaN      NaN        NaN       NaN        NaN               NaN   \n",
       "\n",
       "   migration_03 glob_hlth_03  adl_dress_03  ...       rrelgimp_12  \\\n",
       "0           NaN          NaN           NaN  ...               NaN   \n",
       "1           NaN          NaN           NaN  ...  1.very important   \n",
       "\n",
       "   rrfcntx_m_12  rsocact_m_12  rrelgwk_12  a16a_12  a21_12  a22_12  a33b_12  \\\n",
       "0           NaN           NaN         NaN      NaN     NaN     NaN      NaN   \n",
       "1       9.Never       9.Never       1.Yes      NaN     NaN     NaN      NaN   \n",
       "\n",
       "   a34_12                             j11_12  \n",
       "0     NaN  Wood, mosaic, or other covering 1  \n",
       "1     NaN                         Concrete 2  \n",
       "\n",
       "[2 rows x 184 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3276 entries, 0 to 3275\n",
      "Columns: 184 entries, uid to j11_12\n",
      "dtypes: float64(140), object(44)\n",
      "memory usage: 4.6+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let merge train and label\n",
    "merged_df = pd.merge(train, y, on='uid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       2021\n",
       "1       2021\n",
       "2       2016\n",
       "3       2021\n",
       "4       2021\n",
       "        ... \n",
       "4338    2021\n",
       "4339    2016\n",
       "4340    2021\n",
       "4341    2021\n",
       "4342    2021\n",
       "Name: year, Length: 4343, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df[\"year\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "uid                   0\n",
       "age_03             1456\n",
       "urban_03           1454\n",
       "married_03         1454\n",
       "n_mar_03           1482\n",
       "                   ... \n",
       "a33b_12            4288\n",
       "a34_12             1601\n",
       "j11_12               89\n",
       "year                  0\n",
       "composite_score       0\n",
       "Length: 186, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#so many missing values\n",
    "merged_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets merged test AND sample submission\n",
    "merged_test = pd.merge(test, ss, on='uid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       2016\n",
       "1       2016\n",
       "2       2021\n",
       "3       2016\n",
       "4       2021\n",
       "        ... \n",
       "1100    2016\n",
       "1101    2021\n",
       "1102    2016\n",
       "1103    2021\n",
       "1104    2021\n",
       "Name: year, Length: 1105, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_test[\"year\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: title={'center': 'Target Distribution'}, ylabel='Frequency'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "merged_df['composite_score'].plot(kind='hist',title='Target Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new features by combining 2003 and 2012 scores and numbering ordinal variables\n",
    "def feature_engineering(data):\n",
    "    data['rjob_hrswk_change'] = (data['rjob_hrswk_12'] - data['rjob_hrswk_03']).astype(float)\n",
    "    data['max_work_year']=data[['rjob_end_12','rjob_end_03']].max(axis=1).astype(float)\n",
    "    data['years_since_work']=(data['year']-data['max_work_year']).astype(float)\n",
    "    data['hincome_change']=(data['hincome_12']-data['hincome_03']).astype(float)\n",
    "    data['niadl_change']=(data['n_iadl_12']-data['n_iadl_03']).astype(float)\n",
    "    data['adl_change']=(data['n_adl_12']-data['n_adl_03']).astype(float)\n",
    "    data['depr_change']=(data['n_depr_12']-data['n_depr_03']).astype(float)\n",
    "    data['glob_hlth_03']=data['glob_hlth_03'].replace({'5. Poor':0, '4. Fair':1, '3. Good':2, '2. Very good': 3, '1. Excellent':4}).astype(float)\n",
    "    data['glob_hlth_12']=data['glob_hlth_12'].replace({'5. Poor':0, '4. Fair':1, '3. Good':2, '2. Very good': 3, '1. Excellent':4}).astype(float)\n",
    "    data['glob_hlth_change']=(data['glob_hlth_12']-data['glob_hlth_03']).astype(float)\n",
    "    data['bmi_03']=data['bmi_03'].replace({'1. Underweight': 1, '2. Normal weight': 2, '3. Overweight':3, '4. Obese':4, '5. Morbidly obese':5}).astype(float)\n",
    "    data['bmi_12']=data['bmi_12'].replace({'1. Underweight': 1, '2. Normal weight': 2, '3. Overweight':3, '4. Obese':4, '5. Morbidly obese':5}).astype(float)\n",
    "    data['bmi_change']=(data['bmi_12']-data['bmi_03']).astype(float)\n",
    "    data['employment_03']=data['employment_03'].replace({'1. Currently Working': 'Working', '2. Currently looking for work':'Looking for work', '3. Dedicated to household chores': 'House', '4. Retired, incapacitated, or does not work': 'No work'})\n",
    "    data['employment_12']=data['employment_12'].replace({'1. Currently Working': 'Working', '2. Currently looking for work':'Looking for work', '3. Dedicated to household chores': 'House', '4. Retired, incapacitated, or does not work': 'No work'})\n",
    "    data['memory_12']=data['memory_12'].replace({'5. Poor':0, '4. Fair':1, '3. Good':2, '2. Very good': 3, '1. Excellent':4}).astype(float)\n",
    "    data['edu_gru_03']=data['edu_gru_03'].replace({'0. No education':0,'1. 1–5 years':1, '2. 6 years':2, '3. 7–9 years':3,'4. 10+ years':4}).astype(float)\n",
    "    data['edu_gru_12']=data['edu_gru_12'].replace({'0. No education':0,'1. 1–5 years':1, '2. 6 years':2, '3. 7–9 years':3,'4. 10+ years':4}).astype(float)\n",
    "    data['edu_gru_change']=(data['edu_gru_12']-data['edu_gru_03']).astype(float)\n",
    "    data['illnesses_change']=(data['n_illnesses_12']-data['n_illnesses_03']).astype(float)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cat_cols(data):\n",
    "    # Get the columns with object datatype\n",
    "    cat_columns=[]\n",
    "    dummies=[]\n",
    "    for col in data.columns:\n",
    "        if data[col].dtype=='object' and 'uid' not in col:\n",
    "            cat_columns.append(col)\n",
    "            dummies.append(col)\n",
    "        elif data[col].dtype!='object' and 'uid' not in col and (data[col].max()==1.0):\n",
    "            cat_columns.append(col)\n",
    "            data[col].fillna(0, inplace=True)\n",
    "        else:\n",
    "            continue\n",
    "    return cat_columns, dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_cat_cols(train_data, cat_cols, dummy_cols):\n",
    "    enc = OneHotEncoder()\n",
    "    enc.fit(train_data[dummy_cols])\n",
    "    encoded_train_data=enc.transform(train_data[dummy_cols]).toarray()\n",
    "    feature_names = enc.get_feature_names_out(dummy_cols)\n",
    "    train_data.drop(columns=dummy_cols, inplace=True)\n",
    "    encoded_train_df = pd.DataFrame(encoded_train_data, columns=feature_names)\n",
    "    train_data[feature_names]=encoded_train_df[feature_names]\n",
    "    return train_data, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_943508/2658724754.py:10: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data['glob_hlth_03']=data['glob_hlth_03'].replace({'5. Poor':0, '4. Fair':1, '3. Good':2, '2. Very good': 3, '1. Excellent':4}).astype(float)\n",
      "/tmp/ipykernel_943508/2658724754.py:11: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data['glob_hlth_12']=data['glob_hlth_12'].replace({'5. Poor':0, '4. Fair':1, '3. Good':2, '2. Very good': 3, '1. Excellent':4}).astype(float)\n",
      "/tmp/ipykernel_943508/2658724754.py:13: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data['bmi_03']=data['bmi_03'].replace({'1. Underweight': 1, '2. Normal weight': 2, '3. Overweight':3, '4. Obese':4, '5. Morbidly obese':5}).astype(float)\n",
      "/tmp/ipykernel_943508/2658724754.py:14: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data['bmi_12']=data['bmi_12'].replace({'1. Underweight': 1, '2. Normal weight': 2, '3. Overweight':3, '4. Obese':4, '5. Morbidly obese':5}).astype(float)\n",
      "/tmp/ipykernel_943508/2658724754.py:18: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data['memory_12']=data['memory_12'].replace({'5. Poor':0, '4. Fair':1, '3. Good':2, '2. Very good': 3, '1. Excellent':4}).astype(float)\n",
      "/tmp/ipykernel_943508/2658724754.py:19: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data['edu_gru_03']=data['edu_gru_03'].replace({'0. No education':0,'1. 1–5 years':1, '2. 6 years':2, '3. 7–9 years':3,'4. 10+ years':4}).astype(float)\n",
      "/tmp/ipykernel_943508/2658724754.py:20: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data['edu_gru_12']=data['edu_gru_12'].replace({'0. No education':0,'1. 1–5 years':1, '2. 6 years':2, '3. 7–9 years':3,'4. 10+ years':4}).astype(float)\n",
      "/tmp/ipykernel_943508/53275523.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[col].fillna(0, inplace=True)\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_943508/3102208485.py:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data_processed[col].fillna(0, inplace=True)\n",
      "/tmp/ipykernel_943508/3102208485.py:16: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data_processed[col].fillna(0, inplace=True)\n",
      "/tmp/ipykernel_943508/3102208485.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data_processed[col].fillna(0, inplace=True)\n",
      "/tmp/ipykernel_943508/3102208485.py:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data_processed[col].fillna(0, inplace=True)\n",
      "/tmp/ipykernel_943508/3102208485.py:16: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data_processed[col].fillna(0, inplace=True)\n",
      "/tmp/ipykernel_943508/3102208485.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data_processed[col].fillna(0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_mar_03 33.83 % missing\n",
      "edu_gru_03 33.55 % missing\n",
      "glob_hlth_03 36.99 % missing\n",
      "n_adl_03 33.5 % missing\n",
      "n_iadl_03 36.99 % missing\n",
      "n_depr_03 37.1 % missing\n",
      "n_illnesses_03 33.31 % missing\n",
      "decis_personal_03 37.17 % missing\n",
      "glob_hlth_12 5.8 % missing\n",
      "n_iadl_12 5.84 % missing\n",
      "n_depr_12 6.52 % missing\n",
      "bmi_12 12.48 % missing\n",
      "memory_12 6.72 % missing\n",
      "rearnings_03 33.37 % missing\n",
      "searnings_03 49.98 % missing\n",
      "hincome_03 34.01 % missing\n",
      "hinc_business_03 32.98 % missing\n",
      "hinc_rent_03 32.98 % missing\n",
      "hinc_assets_03 32.98 % missing\n",
      "hinc_cap_03 32.98 % missing\n",
      "rinc_pension_03 33.37 % missing\n",
      "sinc_pension_03 49.98 % missing\n",
      "searnings_12 35.19 % missing\n",
      "sinc_pension_12 35.19 % missing\n",
      "hincome_change 37.28 % missing\n",
      "niadl_change 41.13 % missing\n",
      "adl_change 36.4 % missing\n",
      "depr_change 41.65 % missing\n",
      "glob_hlth_change 41.1 % missing\n",
      "edu_gru_change 36.09 % missing\n",
      "illnesses_change 36.29 % missing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_943508/3102208485.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data_processed[col].fillna(0, inplace=True)\n",
      "/tmp/ipykernel_943508/3102208485.py:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data_processed[col].fillna(0, inplace=True)\n",
      "/tmp/ipykernel_943508/3102208485.py:16: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data_processed[col].fillna(0, inplace=True)\n",
      "/tmp/ipykernel_943508/3102208485.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data_processed[col].fillna(0, inplace=True)\n",
      "/tmp/ipykernel_943508/3102208485.py:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data_processed[col].fillna(0, inplace=True)\n",
      "/tmp/ipykernel_943508/3102208485.py:16: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data_processed[col].fillna(0, inplace=True)\n",
      "/tmp/ipykernel_943508/3102208485.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data_processed[col].fillna(0, inplace=True)\n",
      "/tmp/ipykernel_943508/3102208485.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data_processed[col].fillna(0, inplace=True)\n",
      "/tmp/ipykernel_943508/3102208485.py:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data_processed[col].fillna(0, inplace=True)\n",
      "/tmp/ipykernel_943508/3102208485.py:16: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data_processed[col].fillna(0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "y=merged_df['composite_score']\n",
    "data_processed=pd.concat((merged_df,merged_test)).reset_index(drop=True).copy()\n",
    "data_processed = feature_engineering(data_processed)\n",
    "cat_cols, dummy_cols = get_cat_cols(data_processed)\n",
    "data_processed, dummy_feature_names=encode_cat_cols(data_processed, cat_cols, dummy_cols)\n",
    "data_processed=data_processed.drop(columns=['composite_score'],axis=1)\n",
    "\n",
    "for col in data_processed.columns: \n",
    "    if round((data_processed[col].isna().sum() /len(data_processed)*100), 2)>50:\n",
    "        data_processed.drop(columns=col, inplace=True)\n",
    "    elif round((data_processed[col].isna().sum() /len(data_processed)*100), 2)>5:\n",
    "        print(col,round((data_processed[col].isna().sum() /len(data_processed)*100), 2), '% missing')\n",
    "        data_processed[col].fillna(0, inplace=True)\n",
    "    else: \n",
    "        data_processed[col].fillna(0, inplace=True)\n",
    "        data_processed[col].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_943508/2658724754.py:10: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data['glob_hlth_03']=data['glob_hlth_03'].replace({'5. Poor':0, '4. Fair':1, '3. Good':2, '2. Very good': 3, '1. Excellent':4}).astype(float)\n",
      "/tmp/ipykernel_943508/2658724754.py:11: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data['glob_hlth_12']=data['glob_hlth_12'].replace({'5. Poor':0, '4. Fair':1, '3. Good':2, '2. Very good': 3, '1. Excellent':4}).astype(float)\n",
      "/tmp/ipykernel_943508/2658724754.py:13: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data['bmi_03']=data['bmi_03'].replace({'1. Underweight': 1, '2. Normal weight': 2, '3. Overweight':3, '4. Obese':4, '5. Morbidly obese':5}).astype(float)\n",
      "/tmp/ipykernel_943508/2658724754.py:14: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data['bmi_12']=data['bmi_12'].replace({'1. Underweight': 1, '2. Normal weight': 2, '3. Overweight':3, '4. Obese':4, '5. Morbidly obese':5}).astype(float)\n",
      "/tmp/ipykernel_943508/2658724754.py:18: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data['memory_12']=data['memory_12'].replace({'5. Poor':0, '4. Fair':1, '3. Good':2, '2. Very good': 3, '1. Excellent':4}).astype(float)\n",
      "/tmp/ipykernel_943508/2658724754.py:19: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data['edu_gru_03']=data['edu_gru_03'].replace({'0. No education':0,'1. 1–5 years':1, '2. 6 years':2, '3. 7–9 years':3,'4. 10+ years':4}).astype(float)\n",
      "/tmp/ipykernel_943508/2658724754.py:20: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data['edu_gru_12']=data['edu_gru_12'].replace({'0. No education':0,'1. 1–5 years':1, '2. 6 years':2, '3. 7–9 years':3,'4. 10+ years':4}).astype(float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age_03 5\n",
      "urban_03 2\n",
      "married_03 4\n",
      "n_living_child_03 5\n",
      "decis_famil_03 3\n",
      "employment_03 4\n",
      "age_12 5\n",
      "urban_12 2\n",
      "married_12 4\n",
      "n_living_child_12 5\n",
      "decis_famil_12 3\n",
      "decis_personal_12 3\n",
      "employment_12 4\n",
      "satis_ideal_12 3\n",
      "satis_excel_12 3\n",
      "satis_fine_12 3\n",
      "cosas_imp_12 3\n",
      "wouldnt_change_12 3\n",
      "ragender 2\n",
      "rameduc_m 4\n",
      "rafeduc_m 4\n",
      "sgender_03 2\n",
      "rjlocc_m_03 18\n",
      "rjobend_reason_03 5\n",
      "rrelgimp_03 3\n",
      "sgender_12 2\n",
      "rjlocc_m_12 18\n",
      "rjobend_reason_12 5\n",
      "rrelgimp_12 3\n",
      "rrfcntx_m_12 9\n",
      "rsocact_m_12 9\n",
      "rrelgwk_12 2\n",
      "a22_12 7\n",
      "a33b_12 3\n",
      "a34_12 2\n",
      "j11_12 3\n"
     ]
    }
   ],
   "source": [
    "y=merged_df['composite_score']\n",
    "data_tabnet=pd.concat((merged_df,merged_test)).reset_index(drop=True).copy()\n",
    "data_tabnet = feature_engineering(data_tabnet)\n",
    "data_tabnet=data_tabnet.drop(columns=['uid', 'composite_score'],axis=1)\n",
    "\n",
    "categorical_columns = []\n",
    "categorical_dims =  {}\n",
    "for col in data_tabnet.select_dtypes(include=['object', 'category']).columns:\n",
    "    print(col, data_tabnet[col].nunique())\n",
    "    l_enc = LabelEncoder()\n",
    "    data_tabnet[col] = data_tabnet[col].fillna(\"Missing\")\n",
    "    data_tabnet[col] = l_enc.fit_transform(data_tabnet[col].values)\n",
    "    categorical_columns.append(col)\n",
    "    categorical_dims[col] = len(l_enc.classes_)\n",
    "\n",
    "for col in data_tabnet.columns[data_tabnet.dtypes == 'float64']:\n",
    "    data_tabnet.fillna(data_tabnet.loc[:, col].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_943508/2658724754.py:10: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data['glob_hlth_03']=data['glob_hlth_03'].replace({'5. Poor':0, '4. Fair':1, '3. Good':2, '2. Very good': 3, '1. Excellent':4}).astype(float)\n",
      "/tmp/ipykernel_943508/2658724754.py:11: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data['glob_hlth_12']=data['glob_hlth_12'].replace({'5. Poor':0, '4. Fair':1, '3. Good':2, '2. Very good': 3, '1. Excellent':4}).astype(float)\n",
      "/tmp/ipykernel_943508/2658724754.py:13: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data['bmi_03']=data['bmi_03'].replace({'1. Underweight': 1, '2. Normal weight': 2, '3. Overweight':3, '4. Obese':4, '5. Morbidly obese':5}).astype(float)\n",
      "/tmp/ipykernel_943508/2658724754.py:14: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data['bmi_12']=data['bmi_12'].replace({'1. Underweight': 1, '2. Normal weight': 2, '3. Overweight':3, '4. Obese':4, '5. Morbidly obese':5}).astype(float)\n",
      "/tmp/ipykernel_943508/2658724754.py:18: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data['memory_12']=data['memory_12'].replace({'5. Poor':0, '4. Fair':1, '3. Good':2, '2. Very good': 3, '1. Excellent':4}).astype(float)\n",
      "/tmp/ipykernel_943508/2658724754.py:19: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data['edu_gru_03']=data['edu_gru_03'].replace({'0. No education':0,'1. 1–5 years':1, '2. 6 years':2, '3. 7–9 years':3,'4. 10+ years':4}).astype(float)\n",
      "/tmp/ipykernel_943508/2658724754.py:20: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data['edu_gru_12']=data['edu_gru_12'].replace({'0. No education':0,'1. 1–5 years':1, '2. 6 years':2, '3. 7–9 years':3,'4. 10+ years':4}).astype(float)\n"
     ]
    }
   ],
   "source": [
    "y=merged_df['composite_score']\n",
    "data=pd.concat((merged_df,merged_test)).reset_index(drop=True).copy()\n",
    "data = feature_engineering(data)\n",
    "data=data.drop(columns=['composite_score'],axis=1)\n",
    "\n",
    "# Get the columns with object datatype\n",
    "object_cols = data.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Convert the object columns to category dtype\n",
    "for col in object_cols:\n",
    "    #data[col] = data[col].astype('category').fillna(\"Missing\")\n",
    "    data[col] = pd.Categorical(data[col].fillna(\"Missing\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate train and test\n",
    "merged_df_processed=data_processed[:len(merged_df)]\n",
    "merged_test_processed=data_processed[len(merged_df):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate train and test\n",
    "merged_df=data[:len(merged_df)]\n",
    "merged_test=data[len(merged_df):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate train and test\n",
    "merged_df_tabnet=data_tabnet[:len(merged_df)]\n",
    "merged_test_tabnet=data_tabnet[len(merged_df):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [ col for col in merged_df_tabnet.columns] + [\"glob_hlth_03\", \"glob_hlth_12\", \"bmi_03\", \"bmi_12\", \"memory_12\", \"edu_gru_03\",  \"edu_gru_12\"]\n",
    "\n",
    "cat_idxs = [ i for i, f in enumerate(features) if f in categorical_columns]\n",
    "\n",
    "cat_dims = [ categorical_dims[f] for i, f in enumerate(features) if f in categorical_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4343, 196)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4343, 349)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df_processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_tabnet(trial, train_X, train_y, val_X, val_y):\n",
    "    # Identify categorical features\n",
    "    # Here we assume categorical features are of type 'category' or 'object'.\n",
    "    # If your categorical features are already integer-encoded, adjust accordingly.\n",
    "    cat_cols = [col for col in train_X.columns if str(train_X[col].dtype) in ['object', 'category']]\n",
    "    cat_idxs = [train_X.columns.get_loc(c) for c in cat_cols]\n",
    "    cat_dims = [train_X[c].nunique() for c in cat_cols]\n",
    "\n",
    "    # Convert data to NumPy arrays\n",
    "    X_train = train_X.values\n",
    "    y_train = train_y.values.reshape(-1, 1)\n",
    "    X_val = val_X.values\n",
    "    y_val = val_y.values.reshape(-1, 1)\n",
    "\n",
    "    # Suggest hyperparameters with Optuna\n",
    "    params = {\n",
    "        'n_d': trial.suggest_int('n_d', 8, 64),\n",
    "        'n_a': trial.suggest_int('n_a', 8, 64),\n",
    "        'n_steps': trial.suggest_int('n_steps', 3, 10),\n",
    "        'gamma': trial.suggest_uniform('gamma', 1.0, 2.0),\n",
    "        'lambda_sparse': trial.suggest_loguniform('lambda_sparse', 1e-6, 1e-1),\n",
    "        'cat_emb_dim': trial.suggest_int('cat_emb_dim', 1, 10),\n",
    "        'lr': trial.suggest_loguniform('lr', 1e-3, 1e-1),\n",
    "        'momentum': trial.suggest_uniform('momentum', 0.01, 0.4)\n",
    "    }\n",
    "\n",
    "    # Initialize TabNetRegressor\n",
    "    model = TabNetRegressor(\n",
    "        n_d=params['n_d'],\n",
    "        n_a=params['n_a'],\n",
    "        n_steps=params['n_steps'],\n",
    "        gamma=params['gamma'],\n",
    "        lambda_sparse=params['lambda_sparse'],\n",
    "        cat_idxs=cat_idxs,\n",
    "        cat_dims=cat_dims,\n",
    "        cat_emb_dim=params['cat_emb_dim'],\n",
    "        optimizer_params={'lr': params['lr']},\n",
    "        momentum=params['momentum'],\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    # Fit the model with early stopping\n",
    "    model.fit(\n",
    "        X_train=X_train, y_train=y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        max_epochs=1000,\n",
    "        patience=50,\n",
    "        batch_size=1024,\n",
    "        virtual_batch_size=128,\n",
    "        num_workers=0,\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    # Predict and evaluate\n",
    "    preds = model.predict(X_val).flatten()\n",
    "    rmse = root_mean_squared_error(y_val.flatten(), preds)\n",
    "    return rmse\n",
    "\n",
    "\n",
    "# Define an objective function for Optuna for each model\n",
    "def objective_lightgbm(trial, train_X, train_y, val_X, val_y):\n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        \"random_state\": 42,\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 5e-1),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 10, 200),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 20),\n",
    "        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.3, 1.0),\n",
    "        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.3, 1.0),\n",
    "        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-3, 50.0),\n",
    "        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-3, 50.0),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 1, 200),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 5000)\n",
    "    }\n",
    "\n",
    "    train_data = lgb.Dataset(train_X, label=train_y)\n",
    "    val_data = lgb.Dataset(val_X, label=val_y, reference=train_data)\n",
    "\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        valid_sets=[val_data],\n",
    "        num_boost_round=params['n_estimators'],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=500, verbose=False),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    preds = model.predict(val_X, num_iteration=model.best_iteration)\n",
    "    rmse = root_mean_squared_error(val_y, preds)\n",
    "    return rmse\n",
    "\n",
    "def objective_catboost(trial, train_X, train_y, val_X, val_y):\n",
    "    params = {\n",
    "        'depth': trial.suggest_int('depth', 3, 12),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 5e-1),\n",
    "        'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 50.0),\n",
    "        'random_strength': trial.suggest_uniform('random_strength', 0.0, 5.0),\n",
    "        'bagging_temperature': trial.suggest_uniform('bagging_temperature', 0.0, 1.0),\n",
    "        'iterations': trial.suggest_int('iterations', 50, 5000)\n",
    "    }\n",
    "\n",
    "    model = CatBoostRegressor(\n",
    "        **params,\n",
    "        loss_function='RMSE',\n",
    "        cat_features=train_X.select_dtypes(include=['object', 'category']).columns.to_list(),\n",
    "        verbose=0, early_stopping_rounds = 500,\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(train_X, train_y, eval_set=(val_X, val_y))\n",
    "\n",
    "    preds = model.predict(val_X)\n",
    "    rmse = root_mean_squared_error(val_y, preds)\n",
    "    return rmse\n",
    "\n",
    "def objective_xgboost(trial, train_X, train_y, val_X, val_y):\n",
    "    params = {\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 5e-1),\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 20),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 20),\n",
    "        'subsample': trial.suggest_uniform('subsample', 0.4, 1.0),\n",
    "        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.4, 1.0),\n",
    "        'gamma': trial.suggest_uniform('gamma', 0, 10),\n",
    "        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-3, 50.0),\n",
    "        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 50.0),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 5000)\n",
    "    }\n",
    "\n",
    "    model = XGBRegressor(**params,\n",
    "                         enable_categorical=True,\n",
    "                         eval_metric=root_mean_squared_error,\n",
    "                         early_stopping_rounds=500,\n",
    "                         random_state=42)\n",
    "    model.fit(\n",
    "        train_X, train_y,\n",
    "        eval_set=[(val_X, val_y)],\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    preds = model.predict(val_X)\n",
    "    rmse = root_mean_squared_error(val_y, preds)\n",
    "    return rmse\n",
    "\n",
    "def objective_randomforest(trial, train_X, train_y, val_X, val_y):\n",
    "    params = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 50, log=True),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 50),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 50),\n",
    "        'max_features': trial.suggest_uniform('max_features', 0.1, 1.0),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 1000)\n",
    "    }\n",
    "\n",
    "    model = RandomForestRegressor(**params, random_state=42)\n",
    "    model.fit(train_X, train_y)\n",
    "\n",
    "    preds = model.predict(val_X)\n",
    "    rmse = root_mean_squared_error(val_y, preds)\n",
    "    return rmse\n",
    "\n",
    "# Run Optuna for each model and store the best parameters\n",
    "best_params = {}\n",
    "for model_name, objective in zip(\n",
    "    ['TabNet', 'lightgbm', 'RandomForest', 'XGBoost', 'CatBoost'],\n",
    "    [objective_tabnet, objective_lightgbm, objective_randomforest, objective_xgboost, objective_catboost]\n",
    "):\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    if model_name == 'RandomForest':\n",
    "        unique_uids = merged_df_processed['uid'].unique()\n",
    "        train_ids, val_ids = train_test_split(unique_uids, test_size=0.2, random_state=42)\n",
    "        keep_train = merged_df_processed['uid'].isin(train_ids)\n",
    "        keep_val = merged_df_processed['uid'].isin(val_ids)\n",
    "        train_X, val_X = merged_df_processed[keep_train],merged_df_processed[keep_val]\n",
    "        train_X.drop(columns=['uid'], inplace=True)\n",
    "        val_X.drop(columns=['uid'], inplace=True)\n",
    "        train_y, val_y= y[keep_train], y[keep_val]\n",
    "        # train_X, val_X, train_y, val_y = train_test_split(merged_df_processed, y, test_size=0.2, random_state=42)\n",
    "    else:\n",
    "        unique_uids = merged_df['uid'].unique()\n",
    "        train_ids, val_ids = train_test_split(unique_uids, test_size=0.2, random_state=42)\n",
    "        keep_train = merged_df['uid'].isin(train_ids)\n",
    "        keep_val = merged_df['uid'].isin(val_ids)\n",
    "        train_X, val_X = merged_df[keep_train],merged_df[keep_val]\n",
    "        train_X.drop(columns=['uid'], inplace=True)\n",
    "        val_X.drop(columns=['uid'], inplace=True)\n",
    "        train_y, val_y= y[keep_train], y[keep_val]\n",
    "        # train_X, val_X, train_y, val_y = train_test_split(merged_df, y, test_size=0.2, random_state=42)\n",
    "    study.optimize(lambda trial: objective(trial, train_X, train_y, val_X, val_y), n_trials=100, n_jobs=-1, show_progress_bar=True)\n",
    "    best_params[model_name] = study.best_params\n",
    "    print(f\"Best params for {model_name}: {study.best_params}\")\n",
    "    print(f\"Best RMSE for {model_name}: {study.best_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/tmp/ipykernel_943508/2235415930.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_X.drop(columns=['uid'], inplace=True)\n",
      "/tmp/ipykernel_943508/2235415930.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  val_X.drop(columns=['uid'], inplace=True)\n",
      "/tmp/ipykernel_943508/2235415930.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_X_processed.drop(columns=['uid'], inplace=True)\n",
      "/tmp/ipykernel_943508/2235415930.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  val_X_processed.drop(columns=['uid'], inplace=True)\n",
      "/u/spa-d4/grad/mfe261/Projects/PREPARE/envs/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 27034.62688| val_0_rmse: 151.47995|  0:00:01s\n",
      "epoch 1  | loss: 21105.89099| val_0_rmse: 107.7636|  0:00:01s\n",
      "epoch 2  | loss: 10281.63585| val_0_rmse: 65.1513 |  0:00:02s\n",
      "epoch 3  | loss: 3577.35938| val_0_rmse: 57.26089|  0:00:02s\n",
      "epoch 4  | loss: 2874.17829| val_0_rmse: 56.52067|  0:00:03s\n",
      "epoch 5  | loss: 2613.2498| val_0_rmse: 57.80175|  0:00:04s\n",
      "epoch 6  | loss: 2321.20288| val_0_rmse: 55.02645|  0:00:04s\n",
      "epoch 7  | loss: 2250.99685| val_0_rmse: 58.46734|  0:00:05s\n",
      "epoch 8  | loss: 2150.04614| val_0_rmse: 58.02267|  0:00:06s\n",
      "epoch 9  | loss: 2068.24385| val_0_rmse: 55.21499|  0:00:06s\n",
      "epoch 10 | loss: 2017.62246| val_0_rmse: 59.1336 |  0:00:07s\n",
      "epoch 11 | loss: 1931.20032| val_0_rmse: 49.20462|  0:00:07s\n",
      "epoch 12 | loss: 1923.8201| val_0_rmse: 51.25371|  0:00:08s\n",
      "epoch 13 | loss: 1840.9865| val_0_rmse: 53.93081|  0:00:09s\n",
      "epoch 14 | loss: 1907.15897| val_0_rmse: 46.90437|  0:00:09s\n",
      "epoch 15 | loss: 1837.4624| val_0_rmse: 46.59108|  0:00:10s\n",
      "epoch 16 | loss: 1789.3505| val_0_rmse: 46.55614|  0:00:11s\n",
      "epoch 17 | loss: 1827.235| val_0_rmse: 47.37499|  0:00:11s\n",
      "epoch 18 | loss: 1774.79018| val_0_rmse: 45.99385|  0:00:12s\n",
      "epoch 19 | loss: 1719.40908| val_0_rmse: 43.94747|  0:00:12s\n",
      "epoch 20 | loss: 1772.06857| val_0_rmse: 42.8375 |  0:00:13s\n",
      "epoch 21 | loss: 1670.26238| val_0_rmse: 44.82332|  0:00:14s\n",
      "epoch 22 | loss: 1618.14439| val_0_rmse: 43.35649|  0:00:14s\n",
      "epoch 23 | loss: 1638.61543| val_0_rmse: 42.63365|  0:00:15s\n",
      "epoch 24 | loss: 1662.69511| val_0_rmse: 41.01827|  0:00:16s\n",
      "epoch 25 | loss: 1608.88193| val_0_rmse: 42.47895|  0:00:16s\n",
      "epoch 26 | loss: 1565.65225| val_0_rmse: 41.22386|  0:00:17s\n",
      "epoch 27 | loss: 1537.68163| val_0_rmse: 42.26102|  0:00:17s\n",
      "epoch 28 | loss: 1531.08591| val_0_rmse: 41.89288|  0:00:18s\n",
      "epoch 29 | loss: 1536.15399| val_0_rmse: 41.36684|  0:00:19s\n",
      "epoch 30 | loss: 1463.94675| val_0_rmse: 41.13564|  0:00:19s\n",
      "epoch 31 | loss: 1503.29758| val_0_rmse: 40.5259 |  0:00:20s\n",
      "epoch 32 | loss: 1458.75505| val_0_rmse: 40.85482|  0:00:20s\n",
      "epoch 33 | loss: 1440.85 | val_0_rmse: 40.22519|  0:00:21s\n",
      "epoch 34 | loss: 1448.99778| val_0_rmse: 41.13735|  0:00:22s\n",
      "epoch 35 | loss: 1393.12137| val_0_rmse: 40.85268|  0:00:22s\n",
      "epoch 36 | loss: 1380.58978| val_0_rmse: 41.13301|  0:00:23s\n",
      "epoch 37 | loss: 1400.9375| val_0_rmse: 41.40018|  0:00:24s\n",
      "epoch 38 | loss: 1355.02147| val_0_rmse: 41.37734|  0:00:24s\n",
      "epoch 39 | loss: 1340.25405| val_0_rmse: 41.0754 |  0:00:25s\n",
      "epoch 40 | loss: 1306.96016| val_0_rmse: 40.57241|  0:00:25s\n",
      "epoch 41 | loss: 1310.07855| val_0_rmse: 40.47627|  0:00:26s\n",
      "epoch 42 | loss: 1263.85701| val_0_rmse: 40.40476|  0:00:27s\n",
      "epoch 43 | loss: 1262.71764| val_0_rmse: 41.06628|  0:00:27s\n",
      "epoch 44 | loss: 1262.92478| val_0_rmse: 40.37179|  0:00:28s\n",
      "epoch 45 | loss: 1233.77047| val_0_rmse: 40.79027|  0:00:29s\n",
      "epoch 46 | loss: 1189.76801| val_0_rmse: 41.07954|  0:00:29s\n",
      "epoch 47 | loss: 1227.95852| val_0_rmse: 41.34279|  0:00:30s\n",
      "epoch 48 | loss: 1190.3353| val_0_rmse: 40.7639 |  0:00:30s\n",
      "epoch 49 | loss: 1130.41853| val_0_rmse: 40.73466|  0:00:31s\n",
      "epoch 50 | loss: 1159.29994| val_0_rmse: 41.93963|  0:00:32s\n",
      "epoch 51 | loss: 1152.71531| val_0_rmse: 40.7577 |  0:00:32s\n",
      "epoch 52 | loss: 1098.76096| val_0_rmse: 40.73633|  0:00:33s\n",
      "epoch 53 | loss: 1111.19099| val_0_rmse: 39.85521|  0:00:34s\n",
      "epoch 54 | loss: 1098.71839| val_0_rmse: 41.33071|  0:00:34s\n",
      "epoch 55 | loss: 1149.29138| val_0_rmse: 40.81839|  0:00:35s\n",
      "epoch 56 | loss: 1130.61199| val_0_rmse: 40.83713|  0:00:35s\n",
      "epoch 57 | loss: 1098.1546| val_0_rmse: 41.24248|  0:00:36s\n",
      "epoch 58 | loss: 1057.72181| val_0_rmse: 41.80885|  0:00:37s\n",
      "epoch 59 | loss: 1065.05188| val_0_rmse: 41.02954|  0:00:37s\n",
      "epoch 60 | loss: 1046.70534| val_0_rmse: 41.09718|  0:00:38s\n",
      "epoch 61 | loss: 1055.03794| val_0_rmse: 40.86945|  0:00:38s\n",
      "epoch 62 | loss: 1063.27332| val_0_rmse: 40.89467|  0:00:39s\n",
      "epoch 63 | loss: 973.83798| val_0_rmse: 40.44797|  0:00:40s\n",
      "epoch 64 | loss: 978.00333| val_0_rmse: 41.47979|  0:00:40s\n",
      "epoch 65 | loss: 987.65713| val_0_rmse: 41.91867|  0:00:41s\n",
      "epoch 66 | loss: 929.54046| val_0_rmse: 42.21432|  0:00:41s\n",
      "epoch 67 | loss: 959.77303| val_0_rmse: 43.00932|  0:00:42s\n",
      "epoch 68 | loss: 993.7313| val_0_rmse: 41.0132 |  0:00:43s\n",
      "epoch 69 | loss: 991.30039| val_0_rmse: 41.40931|  0:00:43s\n",
      "epoch 70 | loss: 923.11009| val_0_rmse: 41.64653|  0:00:44s\n",
      "epoch 71 | loss: 889.81242| val_0_rmse: 41.48865|  0:00:44s\n",
      "epoch 72 | loss: 944.65254| val_0_rmse: 41.87194|  0:00:45s\n",
      "epoch 73 | loss: 944.13493| val_0_rmse: 41.66381|  0:00:46s\n",
      "\n",
      "Early stopping occurred at epoch 73 with best_epoch = 53 and best_val_0_rmse = 39.85521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/spa-d4/grad/mfe261/Projects/PREPARE/envs/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabNet rmse:  39.85521157275987\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000949 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1924\n",
      "[LightGBM] [Info] Number of data points in the train set: 3474, number of used features: 195\n",
      "[LightGBM] [Info] Start training from score 157.136730\n",
      "Training until validation scores don't improve for 500 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/spa-d4/grad/mfe261/Projects/PREPARE/envs/lib/python3.9/site-packages/lightgbm/engine.py:204: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1327]\tvalid_0's rmse: 36.6658\n",
      "0:\tlearn: 61.1063627\ttest: 58.7987195\tbest: 58.7987195 (0)\ttotal: 64.4ms\tremaining: 10m 43s\n",
      "100:\tlearn: 45.4018050\ttest: 45.7213248\tbest: 45.7213248 (100)\ttotal: 2.17s\tremaining: 3m 32s\n",
      "200:\tlearn: 39.1611918\ttest: 41.5629577\tbest: 41.5629577 (200)\ttotal: 4.52s\tremaining: 3m 40s\n",
      "300:\tlearn: 35.6709280\ttest: 39.8673791\tbest: 39.8673791 (300)\ttotal: 7.06s\tremaining: 3m 47s\n",
      "400:\tlearn: 33.4245825\ttest: 39.1252152\tbest: 39.1252152 (400)\ttotal: 9.34s\tremaining: 3m 43s\n",
      "500:\tlearn: 31.6828754\ttest: 38.6249708\tbest: 38.6249708 (500)\ttotal: 11.6s\tremaining: 3m 40s\n",
      "600:\tlearn: 30.2828383\ttest: 38.3301718\tbest: 38.3301718 (600)\ttotal: 13.9s\tremaining: 3m 37s\n",
      "700:\tlearn: 29.0905266\ttest: 38.0912449\tbest: 38.0912449 (700)\ttotal: 16.3s\tremaining: 3m 36s\n",
      "800:\tlearn: 28.1201608\ttest: 37.9189765\tbest: 37.9186493 (797)\ttotal: 18.7s\tremaining: 3m 35s\n",
      "900:\tlearn: 27.2881546\ttest: 37.7823027\tbest: 37.7823027 (900)\ttotal: 21.3s\tremaining: 3m 34s\n",
      "1000:\tlearn: 26.5601132\ttest: 37.6718643\tbest: 37.6718643 (1000)\ttotal: 23.8s\tremaining: 3m 33s\n",
      "1100:\tlearn: 25.8432547\ttest: 37.5451850\tbest: 37.5451850 (1100)\ttotal: 26.5s\tremaining: 3m 33s\n",
      "1200:\tlearn: 25.0879363\ttest: 37.4191861\tbest: 37.4191861 (1200)\ttotal: 29s\tremaining: 3m 32s\n",
      "1300:\tlearn: 24.5114714\ttest: 37.3523100\tbest: 37.3517712 (1299)\ttotal: 31.6s\tremaining: 3m 31s\n",
      "1400:\tlearn: 23.8556889\ttest: 37.2772641\tbest: 37.2772641 (1400)\ttotal: 34.2s\tremaining: 3m 30s\n",
      "1500:\tlearn: 23.2676141\ttest: 37.1973785\tbest: 37.1971363 (1497)\ttotal: 36.9s\tremaining: 3m 29s\n",
      "1600:\tlearn: 22.6316235\ttest: 37.1242786\tbest: 37.1242786 (1600)\ttotal: 39.7s\tremaining: 3m 28s\n",
      "1700:\tlearn: 22.0306706\ttest: 37.0653512\tbest: 37.0653512 (1700)\ttotal: 42.6s\tremaining: 3m 27s\n",
      "1800:\tlearn: 21.4123429\ttest: 37.0181769\tbest: 37.0176532 (1799)\ttotal: 45.4s\tremaining: 3m 26s\n",
      "1900:\tlearn: 20.8287093\ttest: 36.9704238\tbest: 36.9704238 (1900)\ttotal: 48.1s\tremaining: 3m 24s\n",
      "2000:\tlearn: 20.2644220\ttest: 36.9201956\tbest: 36.9190264 (1997)\ttotal: 50.9s\tremaining: 3m 23s\n",
      "2100:\tlearn: 19.7064833\ttest: 36.8830066\tbest: 36.8830066 (2100)\ttotal: 53.7s\tremaining: 3m 22s\n",
      "2200:\tlearn: 19.2060835\ttest: 36.8619674\tbest: 36.8619674 (2200)\ttotal: 56.7s\tremaining: 3m 21s\n",
      "2300:\tlearn: 18.7271193\ttest: 36.8267152\tbest: 36.8249019 (2289)\ttotal: 59.5s\tremaining: 3m 19s\n",
      "2400:\tlearn: 18.2821830\ttest: 36.7843638\tbest: 36.7842804 (2399)\ttotal: 1m 2s\tremaining: 3m 17s\n",
      "2500:\tlearn: 17.8336832\ttest: 36.7528661\tbest: 36.7528661 (2500)\ttotal: 1m 5s\tremaining: 3m 15s\n",
      "2600:\tlearn: 17.4159685\ttest: 36.7178225\tbest: 36.7154807 (2598)\ttotal: 1m 7s\tremaining: 3m 13s\n",
      "2700:\tlearn: 17.0338788\ttest: 36.7020377\tbest: 36.7002880 (2697)\ttotal: 1m 10s\tremaining: 3m 11s\n",
      "2800:\tlearn: 16.6899498\ttest: 36.6741071\tbest: 36.6740067 (2789)\ttotal: 1m 13s\tremaining: 3m 9s\n",
      "2900:\tlearn: 16.3329726\ttest: 36.6529170\tbest: 36.6501876 (2890)\ttotal: 1m 16s\tremaining: 3m 7s\n",
      "3000:\tlearn: 16.0203168\ttest: 36.6361722\tbest: 36.6361722 (3000)\ttotal: 1m 19s\tremaining: 3m 4s\n",
      "3100:\tlearn: 15.6554168\ttest: 36.6194663\tbest: 36.6194663 (3100)\ttotal: 1m 22s\tremaining: 3m 2s\n",
      "3200:\tlearn: 15.3242391\ttest: 36.6182526\tbest: 36.6154378 (3193)\ttotal: 1m 24s\tremaining: 3m\n",
      "3300:\tlearn: 14.9758916\ttest: 36.6059518\tbest: 36.6055823 (3296)\ttotal: 1m 27s\tremaining: 2m 58s\n",
      "3400:\tlearn: 14.6417866\ttest: 36.6003520\tbest: 36.5986102 (3360)\ttotal: 1m 30s\tremaining: 2m 56s\n",
      "3500:\tlearn: 14.3351357\ttest: 36.5828244\tbest: 36.5823587 (3498)\ttotal: 1m 33s\tremaining: 2m 53s\n",
      "3600:\tlearn: 14.0551309\ttest: 36.5710047\tbest: 36.5708821 (3597)\ttotal: 1m 36s\tremaining: 2m 51s\n",
      "3700:\tlearn: 13.7383868\ttest: 36.5513628\tbest: 36.5511456 (3697)\ttotal: 1m 39s\tremaining: 2m 48s\n",
      "3800:\tlearn: 13.4487215\ttest: 36.5457637\tbest: 36.5453685 (3799)\ttotal: 1m 41s\tremaining: 2m 46s\n",
      "3900:\tlearn: 13.1833718\ttest: 36.5383563\tbest: 36.5382966 (3899)\ttotal: 1m 44s\tremaining: 2m 43s\n",
      "4000:\tlearn: 12.8751119\ttest: 36.5304218\tbest: 36.5304218 (4000)\ttotal: 1m 47s\tremaining: 2m 41s\n",
      "4100:\tlearn: 12.6099469\ttest: 36.5197553\tbest: 36.5197145 (4085)\ttotal: 1m 50s\tremaining: 2m 38s\n",
      "4200:\tlearn: 12.3440517\ttest: 36.5125821\tbest: 36.5122701 (4192)\ttotal: 1m 53s\tremaining: 2m 36s\n",
      "4300:\tlearn: 12.0916970\ttest: 36.5086202\tbest: 36.5060030 (4242)\ttotal: 1m 56s\tremaining: 2m 33s\n",
      "4400:\tlearn: 11.8389277\ttest: 36.5099859\tbest: 36.5056890 (4314)\ttotal: 1m 58s\tremaining: 2m 31s\n",
      "4500:\tlearn: 11.6132145\ttest: 36.5125149\tbest: 36.5056890 (4314)\ttotal: 2m 1s\tremaining: 2m 28s\n",
      "4600:\tlearn: 11.4122669\ttest: 36.5027846\tbest: 36.5013633 (4598)\ttotal: 2m 4s\tremaining: 2m 26s\n",
      "4700:\tlearn: 11.1905402\ttest: 36.4965048\tbest: 36.4965048 (4700)\ttotal: 2m 7s\tremaining: 2m 23s\n",
      "4800:\tlearn: 10.9732119\ttest: 36.4905884\tbest: 36.4897357 (4794)\ttotal: 2m 10s\tremaining: 2m 21s\n",
      "4900:\tlearn: 10.7756793\ttest: 36.4848277\tbest: 36.4847636 (4899)\ttotal: 2m 13s\tremaining: 2m 18s\n",
      "5000:\tlearn: 10.5805070\ttest: 36.4754136\tbest: 36.4751644 (4959)\ttotal: 2m 16s\tremaining: 2m 16s\n",
      "5100:\tlearn: 10.3896523\ttest: 36.4675021\tbest: 36.4670118 (5096)\ttotal: 2m 18s\tremaining: 2m 13s\n",
      "5200:\tlearn: 10.2154241\ttest: 36.4643949\tbest: 36.4643949 (5200)\ttotal: 2m 21s\tremaining: 2m 10s\n",
      "5300:\tlearn: 10.0277983\ttest: 36.4644612\tbest: 36.4619808 (5213)\ttotal: 2m 24s\tremaining: 2m 8s\n",
      "5400:\tlearn: 9.8631517\ttest: 36.4682772\tbest: 36.4619808 (5213)\ttotal: 2m 27s\tremaining: 2m 5s\n",
      "5500:\tlearn: 9.7143199\ttest: 36.4663267\tbest: 36.4619808 (5213)\ttotal: 2m 30s\tremaining: 2m 3s\n",
      "5600:\tlearn: 9.5598397\ttest: 36.4628477\tbest: 36.4619808 (5213)\ttotal: 2m 33s\tremaining: 2m\n",
      "5700:\tlearn: 9.3847942\ttest: 36.4543396\tbest: 36.4543396 (5700)\ttotal: 2m 36s\tremaining: 1m 57s\n",
      "5800:\tlearn: 9.2294835\ttest: 36.4554525\tbest: 36.4536415 (5787)\ttotal: 2m 39s\tremaining: 1m 55s\n",
      "5900:\tlearn: 9.0886046\ttest: 36.4521128\tbest: 36.4508332 (5882)\ttotal: 2m 42s\tremaining: 1m 52s\n",
      "6000:\tlearn: 8.9329394\ttest: 36.4458146\tbest: 36.4457065 (5994)\ttotal: 2m 44s\tremaining: 1m 49s\n",
      "6100:\tlearn: 8.7882824\ttest: 36.4400284\tbest: 36.4397557 (6092)\ttotal: 2m 47s\tremaining: 1m 47s\n",
      "6200:\tlearn: 8.6527807\ttest: 36.4377366\tbest: 36.4363519 (6174)\ttotal: 2m 50s\tremaining: 1m 44s\n",
      "6300:\tlearn: 8.5043050\ttest: 36.4367379\tbest: 36.4360962 (6298)\ttotal: 2m 53s\tremaining: 1m 41s\n",
      "6400:\tlearn: 8.3834074\ttest: 36.4392399\tbest: 36.4360962 (6298)\ttotal: 2m 56s\tremaining: 1m 39s\n",
      "6500:\tlearn: 8.2477693\ttest: 36.4369013\tbest: 36.4351378 (6479)\ttotal: 2m 59s\tremaining: 1m 36s\n",
      "6600:\tlearn: 8.1170733\ttest: 36.4299224\tbest: 36.4298876 (6599)\ttotal: 3m 2s\tremaining: 1m 33s\n",
      "6700:\tlearn: 7.9959984\ttest: 36.4273543\tbest: 36.4261986 (6678)\ttotal: 3m 5s\tremaining: 1m 31s\n",
      "6800:\tlearn: 7.8623158\ttest: 36.4259453\tbest: 36.4241954 (6730)\ttotal: 3m 8s\tremaining: 1m 28s\n",
      "6900:\tlearn: 7.7383256\ttest: 36.4247437\tbest: 36.4232076 (6855)\ttotal: 3m 10s\tremaining: 1m 25s\n",
      "7000:\tlearn: 7.6226325\ttest: 36.4284216\tbest: 36.4232076 (6855)\ttotal: 3m 13s\tremaining: 1m 23s\n",
      "7100:\tlearn: 7.4982183\ttest: 36.4261993\tbest: 36.4232076 (6855)\ttotal: 3m 16s\tremaining: 1m 20s\n",
      "7200:\tlearn: 7.3690354\ttest: 36.4224704\tbest: 36.4215189 (7185)\ttotal: 3m 19s\tremaining: 1m 17s\n",
      "7300:\tlearn: 7.2413060\ttest: 36.4249093\tbest: 36.4203812 (7214)\ttotal: 3m 22s\tremaining: 1m 14s\n",
      "7400:\tlearn: 7.1232952\ttest: 36.4212010\tbest: 36.4203812 (7214)\ttotal: 3m 25s\tremaining: 1m 12s\n",
      "7500:\tlearn: 7.0007959\ttest: 36.4197228\tbest: 36.4190125 (7494)\ttotal: 3m 28s\tremaining: 1m 9s\n",
      "7600:\tlearn: 6.8726653\ttest: 36.4136076\tbest: 36.4129354 (7585)\ttotal: 3m 31s\tremaining: 1m 6s\n",
      "7700:\tlearn: 6.7485389\ttest: 36.4108699\tbest: 36.4085896 (7685)\ttotal: 3m 33s\tremaining: 1m 3s\n",
      "7800:\tlearn: 6.6383102\ttest: 36.4108596\tbest: 36.4085896 (7685)\ttotal: 3m 36s\tremaining: 1m 1s\n",
      "7900:\tlearn: 6.5051498\ttest: 36.4082170\tbest: 36.4079068 (7833)\ttotal: 3m 39s\tremaining: 58.3s\n",
      "8000:\tlearn: 6.3924275\ttest: 36.4061374\tbest: 36.4048914 (7978)\ttotal: 3m 42s\tremaining: 55.6s\n",
      "8100:\tlearn: 6.2891236\ttest: 36.4063851\tbest: 36.4048914 (7978)\ttotal: 3m 45s\tremaining: 52.9s\n",
      "8200:\tlearn: 6.1818182\ttest: 36.4070768\tbest: 36.4048914 (7978)\ttotal: 3m 48s\tremaining: 50.1s\n",
      "8300:\tlearn: 6.0805403\ttest: 36.4086672\tbest: 36.4048914 (7978)\ttotal: 3m 51s\tremaining: 47.3s\n",
      "8400:\tlearn: 5.9804729\ttest: 36.4080097\tbest: 36.4048914 (7978)\ttotal: 3m 54s\tremaining: 44.6s\n",
      "Stopped by overfitting detector  (500 iterations wait)\n",
      "\n",
      "bestTest = 36.40489136\n",
      "bestIteration = 7978\n",
      "\n",
      "Shrink model to first 7979 iterations.\n",
      "[0]\tvalidation_0-rmse:58.77700\tvalidation_0-root_mean_squared_error:58.77700\n",
      "[100]\tvalidation_0-rmse:45.86832\tvalidation_0-root_mean_squared_error:45.86832\n",
      "[200]\tvalidation_0-rmse:42.32089\tvalidation_0-root_mean_squared_error:42.32089\n",
      "[300]\tvalidation_0-rmse:40.79543\tvalidation_0-root_mean_squared_error:40.79543\n",
      "[400]\tvalidation_0-rmse:39.94743\tvalidation_0-root_mean_squared_error:39.94743\n",
      "[500]\tvalidation_0-rmse:39.53036\tvalidation_0-root_mean_squared_error:39.53036\n",
      "[600]\tvalidation_0-rmse:39.27665\tvalidation_0-root_mean_squared_error:39.27665\n",
      "[700]\tvalidation_0-rmse:39.08285\tvalidation_0-root_mean_squared_error:39.08285\n",
      "[800]\tvalidation_0-rmse:38.90079\tvalidation_0-root_mean_squared_error:38.90079\n",
      "[900]\tvalidation_0-rmse:38.78109\tvalidation_0-root_mean_squared_error:38.78109\n",
      "[1000]\tvalidation_0-rmse:38.69750\tvalidation_0-root_mean_squared_error:38.69751\n",
      "[1100]\tvalidation_0-rmse:38.63121\tvalidation_0-root_mean_squared_error:38.63121\n",
      "[1200]\tvalidation_0-rmse:38.55950\tvalidation_0-root_mean_squared_error:38.55950\n",
      "[1300]\tvalidation_0-rmse:38.52585\tvalidation_0-root_mean_squared_error:38.52585\n",
      "[1400]\tvalidation_0-rmse:38.49047\tvalidation_0-root_mean_squared_error:38.49047\n",
      "[1500]\tvalidation_0-rmse:38.42440\tvalidation_0-root_mean_squared_error:38.42440\n",
      "[1600]\tvalidation_0-rmse:38.36475\tvalidation_0-root_mean_squared_error:38.36475\n",
      "[1700]\tvalidation_0-rmse:38.31906\tvalidation_0-root_mean_squared_error:38.31906\n",
      "[1800]\tvalidation_0-rmse:38.26835\tvalidation_0-root_mean_squared_error:38.26835\n",
      "[1900]\tvalidation_0-rmse:38.22781\tvalidation_0-root_mean_squared_error:38.22781\n",
      "[2000]\tvalidation_0-rmse:38.18228\tvalidation_0-root_mean_squared_error:38.18228\n",
      "[2100]\tvalidation_0-rmse:38.13738\tvalidation_0-root_mean_squared_error:38.13738\n",
      "[2200]\tvalidation_0-rmse:38.08401\tvalidation_0-root_mean_squared_error:38.08402\n",
      "[2300]\tvalidation_0-rmse:38.04016\tvalidation_0-root_mean_squared_error:38.04015\n",
      "[2400]\tvalidation_0-rmse:37.99269\tvalidation_0-root_mean_squared_error:37.99269\n",
      "[2500]\tvalidation_0-rmse:37.94439\tvalidation_0-root_mean_squared_error:37.94439\n",
      "[2600]\tvalidation_0-rmse:37.90597\tvalidation_0-root_mean_squared_error:37.90597\n",
      "[2700]\tvalidation_0-rmse:37.86533\tvalidation_0-root_mean_squared_error:37.86533\n",
      "[2800]\tvalidation_0-rmse:37.83376\tvalidation_0-root_mean_squared_error:37.83376\n",
      "[2900]\tvalidation_0-rmse:37.81394\tvalidation_0-root_mean_squared_error:37.81394\n",
      "[3000]\tvalidation_0-rmse:37.77655\tvalidation_0-root_mean_squared_error:37.77655\n",
      "[3100]\tvalidation_0-rmse:37.74773\tvalidation_0-root_mean_squared_error:37.74774\n",
      "[3200]\tvalidation_0-rmse:37.72128\tvalidation_0-root_mean_squared_error:37.72128\n",
      "[3300]\tvalidation_0-rmse:37.70841\tvalidation_0-root_mean_squared_error:37.70841\n",
      "[3400]\tvalidation_0-rmse:37.70729\tvalidation_0-root_mean_squared_error:37.70729\n",
      "[3500]\tvalidation_0-rmse:37.69547\tvalidation_0-root_mean_squared_error:37.69546\n",
      "[3600]\tvalidation_0-rmse:37.69304\tvalidation_0-root_mean_squared_error:37.69304\n",
      "[3700]\tvalidation_0-rmse:37.68464\tvalidation_0-root_mean_squared_error:37.68464\n",
      "[3800]\tvalidation_0-rmse:37.68693\tvalidation_0-root_mean_squared_error:37.68693\n",
      "[3900]\tvalidation_0-rmse:37.69270\tvalidation_0-root_mean_squared_error:37.69270\n",
      "[4000]\tvalidation_0-rmse:37.68761\tvalidation_0-root_mean_squared_error:37.68761\n",
      "[4100]\tvalidation_0-rmse:37.68607\tvalidation_0-root_mean_squared_error:37.68607\n",
      "[4183]\tvalidation_0-rmse:37.68721\tvalidation_0-root_mean_squared_error:37.68721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [06:35, 395.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest rmse:  37.40206908073587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_943508/2235415930.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_X.drop(columns=['uid'], inplace=True)\n",
      "/tmp/ipykernel_943508/2235415930.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  val_X.drop(columns=['uid'], inplace=True)\n",
      "/tmp/ipykernel_943508/2235415930.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_X_processed.drop(columns=['uid'], inplace=True)\n",
      "/tmp/ipykernel_943508/2235415930.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  val_X_processed.drop(columns=['uid'], inplace=True)\n",
      "/u/spa-d4/grad/mfe261/Projects/PREPARE/envs/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 26937.40133| val_0_rmse: 153.03143|  0:00:00s\n",
      "epoch 1  | loss: 20698.94513| val_0_rmse: 113.49322|  0:00:01s\n",
      "epoch 2  | loss: 9412.53349| val_0_rmse: 65.82485|  0:00:01s\n",
      "epoch 3  | loss: 3200.23072| val_0_rmse: 48.60226|  0:00:02s\n",
      "epoch 4  | loss: 2582.9005| val_0_rmse: 55.08369|  0:00:03s\n",
      "epoch 5  | loss: 2458.6756| val_0_rmse: 47.92819|  0:00:03s\n",
      "epoch 6  | loss: 2341.74626| val_0_rmse: 48.28047|  0:00:04s\n",
      "epoch 7  | loss: 2250.45025| val_0_rmse: 54.6949 |  0:00:04s\n",
      "epoch 8  | loss: 2224.95042| val_0_rmse: 46.68572|  0:00:05s\n",
      "epoch 9  | loss: 2172.1892| val_0_rmse: 47.25666|  0:00:06s\n",
      "epoch 10 | loss: 2128.96574| val_0_rmse: 44.67372|  0:00:06s\n",
      "epoch 11 | loss: 2117.22338| val_0_rmse: 45.68985|  0:00:07s\n",
      "epoch 12 | loss: 2001.11372| val_0_rmse: 44.12134|  0:00:07s\n",
      "epoch 13 | loss: 1974.75716| val_0_rmse: 43.24651|  0:00:08s\n",
      "epoch 14 | loss: 1891.68799| val_0_rmse: 42.55016|  0:00:09s\n",
      "epoch 15 | loss: 1903.72789| val_0_rmse: 43.38971|  0:00:09s\n",
      "epoch 16 | loss: 1849.00611| val_0_rmse: 45.30139|  0:00:10s\n",
      "epoch 17 | loss: 1758.40661| val_0_rmse: 42.2596 |  0:00:10s\n",
      "epoch 18 | loss: 1718.49102| val_0_rmse: 41.97452|  0:00:11s\n",
      "epoch 19 | loss: 1729.0519| val_0_rmse: 41.25993|  0:00:12s\n",
      "epoch 20 | loss: 1658.8517| val_0_rmse: 41.95329|  0:00:12s\n",
      "epoch 21 | loss: 1670.2686| val_0_rmse: 41.92418|  0:00:13s\n",
      "epoch 22 | loss: 1633.9427| val_0_rmse: 40.51179|  0:00:14s\n",
      "epoch 23 | loss: 1604.48572| val_0_rmse: 41.71243|  0:00:14s\n",
      "epoch 24 | loss: 1580.03568| val_0_rmse: 41.79785|  0:00:15s\n",
      "epoch 25 | loss: 1557.5344| val_0_rmse: 41.57916|  0:00:15s\n",
      "epoch 26 | loss: 1560.80374| val_0_rmse: 41.18799|  0:00:16s\n",
      "epoch 27 | loss: 1509.8893| val_0_rmse: 41.35338|  0:00:17s\n",
      "epoch 28 | loss: 1472.27236| val_0_rmse: 40.79801|  0:00:17s\n",
      "epoch 29 | loss: 1504.36943| val_0_rmse: 41.08771|  0:00:18s\n",
      "epoch 30 | loss: 1464.33353| val_0_rmse: 41.05423|  0:00:18s\n",
      "epoch 31 | loss: 1435.43172| val_0_rmse: 41.33535|  0:00:19s\n",
      "epoch 32 | loss: 1478.1749| val_0_rmse: 40.8836 |  0:00:19s\n",
      "epoch 33 | loss: 1438.95023| val_0_rmse: 41.19893|  0:00:20s\n",
      "epoch 34 | loss: 1387.80426| val_0_rmse: 40.19066|  0:00:21s\n",
      "epoch 35 | loss: 1334.43335| val_0_rmse: 40.59836|  0:00:21s\n",
      "epoch 36 | loss: 1388.86142| val_0_rmse: 41.42101|  0:00:22s\n",
      "epoch 37 | loss: 1340.14263| val_0_rmse: 40.89513|  0:00:23s\n",
      "epoch 38 | loss: 1300.08636| val_0_rmse: 40.53398|  0:00:23s\n",
      "epoch 39 | loss: 1311.11984| val_0_rmse: 40.14877|  0:00:24s\n",
      "epoch 40 | loss: 1308.34841| val_0_rmse: 41.06323|  0:00:24s\n",
      "epoch 41 | loss: 1255.32289| val_0_rmse: 40.34347|  0:00:25s\n",
      "epoch 42 | loss: 1248.88619| val_0_rmse: 40.71273|  0:00:26s\n",
      "epoch 43 | loss: 1231.5258| val_0_rmse: 40.78219|  0:00:26s\n",
      "epoch 44 | loss: 1205.47689| val_0_rmse: 41.00976|  0:00:27s\n",
      "epoch 45 | loss: 1195.12158| val_0_rmse: 41.35099|  0:00:27s\n",
      "epoch 46 | loss: 1181.5072| val_0_rmse: 40.52459|  0:00:28s\n",
      "epoch 47 | loss: 1139.50878| val_0_rmse: 41.80137|  0:00:29s\n",
      "epoch 48 | loss: 1153.50882| val_0_rmse: 41.09133|  0:00:29s\n",
      "epoch 49 | loss: 1135.83814| val_0_rmse: 40.92486|  0:00:30s\n",
      "epoch 50 | loss: 1109.06878| val_0_rmse: 41.56217|  0:00:30s\n",
      "epoch 51 | loss: 1102.54581| val_0_rmse: 41.54483|  0:00:31s\n",
      "epoch 52 | loss: 1081.00202| val_0_rmse: 41.2598 |  0:00:32s\n",
      "epoch 53 | loss: 1068.40736| val_0_rmse: 42.32869|  0:00:32s\n",
      "epoch 54 | loss: 1080.27087| val_0_rmse: 42.21097|  0:00:33s\n",
      "epoch 55 | loss: 1018.66298| val_0_rmse: 42.79066|  0:00:33s\n",
      "epoch 56 | loss: 1048.89447| val_0_rmse: 41.51178|  0:00:34s\n",
      "epoch 57 | loss: 1051.27609| val_0_rmse: 41.60974|  0:00:35s\n",
      "epoch 58 | loss: 989.24109| val_0_rmse: 42.11188|  0:00:35s\n",
      "epoch 59 | loss: 1039.91905| val_0_rmse: 41.55256|  0:00:36s\n",
      "\n",
      "Early stopping occurred at epoch 59 with best_epoch = 39 and best_val_0_rmse = 40.14877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/spa-d4/grad/mfe261/Projects/PREPARE/envs/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabNet rmse:  40.148766974512824\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018466 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1918\n",
      "[LightGBM] [Info] Number of data points in the train set: 3474, number of used features: 195\n",
      "[LightGBM] [Info] Start training from score 157.271445\n",
      "Training until validation scores don't improve for 500 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/spa-d4/grad/mfe261/Projects/PREPARE/envs/lib/python3.9/site-packages/lightgbm/engine.py:204: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2284]\tvalid_0's rmse: 36.4195\n",
      "0:\tlearn: 59.9907902\ttest: 63.2094615\tbest: 63.2094615 (0)\ttotal: 20.1ms\tremaining: 3m 21s\n",
      "100:\tlearn: 45.1310654\ttest: 47.6070807\tbest: 47.6070807 (100)\ttotal: 2.11s\tremaining: 3m 26s\n",
      "200:\tlearn: 38.9875127\ttest: 42.4352906\tbest: 42.4352906 (200)\ttotal: 4.37s\tremaining: 3m 32s\n",
      "300:\tlearn: 35.5235392\ttest: 40.2150772\tbest: 40.2150772 (300)\ttotal: 6.63s\tremaining: 3m 33s\n",
      "400:\tlearn: 33.1813938\ttest: 39.1194870\tbest: 39.1194870 (400)\ttotal: 9.09s\tremaining: 3m 37s\n",
      "500:\tlearn: 31.5564557\ttest: 38.5151816\tbest: 38.5151816 (500)\ttotal: 11.4s\tremaining: 3m 36s\n",
      "600:\tlearn: 30.1657989\ttest: 38.1346801\tbest: 38.1346801 (600)\ttotal: 13.8s\tremaining: 3m 35s\n",
      "700:\tlearn: 28.9884767\ttest: 37.8868718\tbest: 37.8868718 (700)\ttotal: 16.1s\tremaining: 3m 33s\n",
      "800:\tlearn: 27.9626990\ttest: 37.6847878\tbest: 37.6847878 (800)\ttotal: 18.6s\tremaining: 3m 33s\n",
      "900:\tlearn: 27.0020625\ttest: 37.5285275\tbest: 37.5285275 (900)\ttotal: 21.1s\tremaining: 3m 33s\n",
      "1000:\tlearn: 26.1789213\ttest: 37.3949454\tbest: 37.3949454 (1000)\ttotal: 23.7s\tremaining: 3m 32s\n",
      "1100:\tlearn: 25.2739659\ttest: 37.2801841\tbest: 37.2801841 (1100)\ttotal: 26.3s\tremaining: 3m 32s\n",
      "1200:\tlearn: 24.5910880\ttest: 37.1738886\tbest: 37.1738886 (1200)\ttotal: 29s\tremaining: 3m 32s\n",
      "1300:\tlearn: 23.9229557\ttest: 37.1011125\tbest: 37.1011125 (1300)\ttotal: 31.9s\tremaining: 3m 33s\n",
      "1400:\tlearn: 23.2107274\ttest: 37.0359035\tbest: 37.0338701 (1394)\ttotal: 34.8s\tremaining: 3m 33s\n",
      "1500:\tlearn: 22.4723053\ttest: 36.9681937\tbest: 36.9681937 (1500)\ttotal: 37.6s\tremaining: 3m 32s\n",
      "1600:\tlearn: 21.7900849\ttest: 36.8963978\tbest: 36.8963978 (1600)\ttotal: 40.4s\tremaining: 3m 31s\n",
      "1700:\tlearn: 21.1769653\ttest: 36.8317941\tbest: 36.8317941 (1700)\ttotal: 43.2s\tremaining: 3m 30s\n",
      "1800:\tlearn: 20.6030087\ttest: 36.7861175\tbest: 36.7861175 (1800)\ttotal: 46s\tremaining: 3m 29s\n",
      "1900:\tlearn: 20.0193219\ttest: 36.7312130\tbest: 36.7304825 (1898)\ttotal: 48.8s\tremaining: 3m 27s\n",
      "2000:\tlearn: 19.5513313\ttest: 36.6885105\tbest: 36.6883625 (1999)\ttotal: 51.7s\tremaining: 3m 26s\n",
      "2100:\tlearn: 19.0681863\ttest: 36.6502640\tbest: 36.6495567 (2096)\ttotal: 54.5s\tremaining: 3m 25s\n",
      "2200:\tlearn: 18.6045094\ttest: 36.6120420\tbest: 36.6115132 (2199)\ttotal: 57.4s\tremaining: 3m 23s\n",
      "2300:\tlearn: 18.1793490\ttest: 36.5778623\tbest: 36.5768814 (2292)\ttotal: 1m\tremaining: 3m 21s\n",
      "2400:\tlearn: 17.7048012\ttest: 36.5463645\tbest: 36.5409355 (2387)\ttotal: 1m 3s\tremaining: 3m 19s\n",
      "2500:\tlearn: 17.2999409\ttest: 36.5265045\tbest: 36.5237537 (2494)\ttotal: 1m 5s\tremaining: 3m 17s\n",
      "2600:\tlearn: 16.8953681\ttest: 36.4961068\tbest: 36.4961068 (2600)\ttotal: 1m 8s\tremaining: 3m 15s\n",
      "2700:\tlearn: 16.5428661\ttest: 36.4791450\tbest: 36.4791450 (2700)\ttotal: 1m 11s\tremaining: 3m 13s\n",
      "2800:\tlearn: 16.1688695\ttest: 36.4612613\tbest: 36.4608686 (2793)\ttotal: 1m 14s\tremaining: 3m 11s\n",
      "2900:\tlearn: 15.8076183\ttest: 36.4404568\tbest: 36.4400381 (2890)\ttotal: 1m 17s\tremaining: 3m 9s\n",
      "3000:\tlearn: 15.4547479\ttest: 36.4253251\tbest: 36.4249902 (2990)\ttotal: 1m 20s\tremaining: 3m 7s\n",
      "3100:\tlearn: 15.1365554\ttest: 36.4153550\tbest: 36.4129423 (3072)\ttotal: 1m 23s\tremaining: 3m 5s\n",
      "3200:\tlearn: 14.7879822\ttest: 36.4001245\tbest: 36.3997037 (3199)\ttotal: 1m 26s\tremaining: 3m 3s\n",
      "3300:\tlearn: 14.4672965\ttest: 36.3846611\tbest: 36.3846611 (3300)\ttotal: 1m 29s\tremaining: 3m\n",
      "3400:\tlearn: 14.1469940\ttest: 36.3681624\tbest: 36.3670616 (3395)\ttotal: 1m 32s\tremaining: 2m 58s\n",
      "3500:\tlearn: 13.8277447\ttest: 36.3588251\tbest: 36.3545265 (3470)\ttotal: 1m 35s\tremaining: 2m 56s\n",
      "3600:\tlearn: 13.5311917\ttest: 36.3511320\tbest: 36.3511320 (3600)\ttotal: 1m 37s\tremaining: 2m 54s\n",
      "3700:\tlearn: 13.2854990\ttest: 36.3447670\tbest: 36.3447299 (3695)\ttotal: 1m 40s\tremaining: 2m 51s\n",
      "3800:\tlearn: 13.0163706\ttest: 36.3457997\tbest: 36.3412732 (3743)\ttotal: 1m 43s\tremaining: 2m 49s\n",
      "3900:\tlearn: 12.7409523\ttest: 36.3409790\tbest: 36.3403734 (3896)\ttotal: 1m 46s\tremaining: 2m 46s\n",
      "4000:\tlearn: 12.4728173\ttest: 36.3282538\tbest: 36.3267504 (3995)\ttotal: 1m 49s\tremaining: 2m 44s\n",
      "4100:\tlearn: 12.2375275\ttest: 36.3237203\tbest: 36.3226103 (4077)\ttotal: 1m 52s\tremaining: 2m 41s\n",
      "4200:\tlearn: 12.0168977\ttest: 36.3191085\tbest: 36.3160062 (4183)\ttotal: 1m 55s\tremaining: 2m 39s\n",
      "4300:\tlearn: 11.7894965\ttest: 36.3155086\tbest: 36.3135651 (4293)\ttotal: 1m 58s\tremaining: 2m 36s\n",
      "4400:\tlearn: 11.5743875\ttest: 36.3217897\tbest: 36.3135651 (4293)\ttotal: 2m 1s\tremaining: 2m 34s\n",
      "4500:\tlearn: 11.3726377\ttest: 36.3272920\tbest: 36.3135651 (4293)\ttotal: 2m 4s\tremaining: 2m 31s\n",
      "4600:\tlearn: 11.1520175\ttest: 36.3301030\tbest: 36.3135651 (4293)\ttotal: 2m 7s\tremaining: 2m 29s\n",
      "4700:\tlearn: 10.9414716\ttest: 36.3368980\tbest: 36.3135651 (4293)\ttotal: 2m 9s\tremaining: 2m 26s\n",
      "Stopped by overfitting detector  (500 iterations wait)\n",
      "\n",
      "bestTest = 36.31356508\n",
      "bestIteration = 4293\n",
      "\n",
      "Shrink model to first 4294 iterations.\n",
      "[0]\tvalidation_0-rmse:63.17007\tvalidation_0-root_mean_squared_error:63.17006\n",
      "[100]\tvalidation_0-rmse:47.40418\tvalidation_0-root_mean_squared_error:47.40419\n",
      "[200]\tvalidation_0-rmse:42.70518\tvalidation_0-root_mean_squared_error:42.70518\n",
      "[300]\tvalidation_0-rmse:40.81539\tvalidation_0-root_mean_squared_error:40.81540\n",
      "[400]\tvalidation_0-rmse:39.93084\tvalidation_0-root_mean_squared_error:39.93084\n",
      "[500]\tvalidation_0-rmse:39.46957\tvalidation_0-root_mean_squared_error:39.46957\n",
      "[600]\tvalidation_0-rmse:39.17220\tvalidation_0-root_mean_squared_error:39.17220\n",
      "[700]\tvalidation_0-rmse:39.00297\tvalidation_0-root_mean_squared_error:39.00297\n",
      "[800]\tvalidation_0-rmse:38.85987\tvalidation_0-root_mean_squared_error:38.85987\n",
      "[900]\tvalidation_0-rmse:38.75698\tvalidation_0-root_mean_squared_error:38.75698\n",
      "[1000]\tvalidation_0-rmse:38.65962\tvalidation_0-root_mean_squared_error:38.65962\n",
      "[1100]\tvalidation_0-rmse:38.55989\tvalidation_0-root_mean_squared_error:38.55989\n",
      "[1200]\tvalidation_0-rmse:38.49445\tvalidation_0-root_mean_squared_error:38.49445\n",
      "[1300]\tvalidation_0-rmse:38.42029\tvalidation_0-root_mean_squared_error:38.42029\n",
      "[1400]\tvalidation_0-rmse:38.35200\tvalidation_0-root_mean_squared_error:38.35200\n",
      "[1500]\tvalidation_0-rmse:38.28205\tvalidation_0-root_mean_squared_error:38.28205\n",
      "[1600]\tvalidation_0-rmse:38.22644\tvalidation_0-root_mean_squared_error:38.22644\n",
      "[1700]\tvalidation_0-rmse:38.16844\tvalidation_0-root_mean_squared_error:38.16844\n",
      "[1800]\tvalidation_0-rmse:38.11379\tvalidation_0-root_mean_squared_error:38.11379\n",
      "[1900]\tvalidation_0-rmse:38.08466\tvalidation_0-root_mean_squared_error:38.08466\n",
      "[2000]\tvalidation_0-rmse:38.03793\tvalidation_0-root_mean_squared_error:38.03793\n",
      "[2100]\tvalidation_0-rmse:37.98479\tvalidation_0-root_mean_squared_error:37.98479\n",
      "[2200]\tvalidation_0-rmse:37.93565\tvalidation_0-root_mean_squared_error:37.93565\n",
      "[2300]\tvalidation_0-rmse:37.90794\tvalidation_0-root_mean_squared_error:37.90794\n",
      "[2400]\tvalidation_0-rmse:37.88449\tvalidation_0-root_mean_squared_error:37.88449\n",
      "[2500]\tvalidation_0-rmse:37.85838\tvalidation_0-root_mean_squared_error:37.85838\n",
      "[2600]\tvalidation_0-rmse:37.83728\tvalidation_0-root_mean_squared_error:37.83728\n",
      "[2700]\tvalidation_0-rmse:37.81999\tvalidation_0-root_mean_squared_error:37.81999\n",
      "[2800]\tvalidation_0-rmse:37.79840\tvalidation_0-root_mean_squared_error:37.79840\n",
      "[2900]\tvalidation_0-rmse:37.80404\tvalidation_0-root_mean_squared_error:37.80404\n",
      "[3000]\tvalidation_0-rmse:37.81509\tvalidation_0-root_mean_squared_error:37.81509\n",
      "[3100]\tvalidation_0-rmse:37.81095\tvalidation_0-root_mean_squared_error:37.81095\n",
      "[3200]\tvalidation_0-rmse:37.84249\tvalidation_0-root_mean_squared_error:37.84249\n",
      "[3300]\tvalidation_0-rmse:37.82886\tvalidation_0-root_mean_squared_error:37.82886\n",
      "[3301]\tvalidation_0-rmse:37.82763\tvalidation_0-root_mean_squared_error:37.82763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [11:10, 324.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest rmse:  36.69250054018981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_943508/2235415930.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_X.drop(columns=['uid'], inplace=True)\n",
      "/tmp/ipykernel_943508/2235415930.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  val_X.drop(columns=['uid'], inplace=True)\n",
      "/tmp/ipykernel_943508/2235415930.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_X_processed.drop(columns=['uid'], inplace=True)\n",
      "/tmp/ipykernel_943508/2235415930.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  val_X_processed.drop(columns=['uid'], inplace=True)\n",
      "/u/spa-d4/grad/mfe261/Projects/PREPARE/envs/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 26796.30346| val_0_rmse: 158.844 |  0:00:00s\n",
      "epoch 1  | loss: 20926.82863| val_0_rmse: 113.26237|  0:00:01s\n",
      "epoch 2  | loss: 9554.13862| val_0_rmse: 65.23194|  0:00:01s\n",
      "epoch 3  | loss: 2736.38697| val_0_rmse: 50.08222|  0:00:02s\n",
      "epoch 4  | loss: 2331.7134| val_0_rmse: 47.66988|  0:00:03s\n",
      "epoch 5  | loss: 2287.67306| val_0_rmse: 49.12825|  0:00:03s\n",
      "epoch 6  | loss: 2224.84608| val_0_rmse: 49.83205|  0:00:04s\n",
      "epoch 7  | loss: 2256.10851| val_0_rmse: 47.26639|  0:00:04s\n",
      "epoch 8  | loss: 2208.86642| val_0_rmse: 47.08586|  0:00:05s\n",
      "epoch 9  | loss: 2187.28738| val_0_rmse: 47.48021|  0:00:06s\n",
      "epoch 10 | loss: 2221.95541| val_0_rmse: 47.16441|  0:00:06s\n",
      "epoch 11 | loss: 2172.94931| val_0_rmse: 48.6149 |  0:00:07s\n",
      "epoch 12 | loss: 2158.90586| val_0_rmse: 47.73388|  0:00:08s\n",
      "epoch 13 | loss: 2195.72532| val_0_rmse: 51.03846|  0:00:08s\n",
      "epoch 14 | loss: 2149.44209| val_0_rmse: 47.5841 |  0:00:09s\n",
      "epoch 15 | loss: 2073.88732| val_0_rmse: 48.47351|  0:00:09s\n",
      "epoch 16 | loss: 2059.77865| val_0_rmse: 51.12824|  0:00:10s\n",
      "epoch 17 | loss: 1971.79522| val_0_rmse: 44.23844|  0:00:11s\n",
      "epoch 18 | loss: 1915.65715| val_0_rmse: 43.39194|  0:00:11s\n",
      "epoch 19 | loss: 1932.68704| val_0_rmse: 45.62359|  0:00:12s\n",
      "epoch 20 | loss: 1905.31864| val_0_rmse: 42.83676|  0:00:12s\n",
      "epoch 21 | loss: 1894.55227| val_0_rmse: 47.05975|  0:00:13s\n",
      "epoch 22 | loss: 1904.39344| val_0_rmse: 42.52555|  0:00:14s\n",
      "epoch 23 | loss: 1770.39876| val_0_rmse: 42.59224|  0:00:14s\n",
      "epoch 24 | loss: 1798.44227| val_0_rmse: 41.71117|  0:00:15s\n",
      "epoch 25 | loss: 1805.93287| val_0_rmse: 43.37788|  0:00:16s\n",
      "epoch 26 | loss: 1777.47774| val_0_rmse: 43.11231|  0:00:16s\n",
      "epoch 27 | loss: 1790.16269| val_0_rmse: 42.35862|  0:00:17s\n",
      "epoch 28 | loss: 1703.07897| val_0_rmse: 42.44384|  0:00:17s\n",
      "epoch 29 | loss: 1686.42688| val_0_rmse: 42.88447|  0:00:18s\n",
      "epoch 30 | loss: 1694.85888| val_0_rmse: 42.36697|  0:00:19s\n",
      "epoch 31 | loss: 1649.60748| val_0_rmse: 42.53699|  0:00:19s\n",
      "epoch 32 | loss: 1643.56743| val_0_rmse: 41.81933|  0:00:20s\n",
      "epoch 33 | loss: 1631.31944| val_0_rmse: 42.20005|  0:00:20s\n",
      "epoch 34 | loss: 1568.02493| val_0_rmse: 41.61117|  0:00:21s\n",
      "epoch 35 | loss: 1604.76151| val_0_rmse: 41.59374|  0:00:22s\n",
      "epoch 36 | loss: 1594.13227| val_0_rmse: 41.9282 |  0:00:22s\n",
      "epoch 37 | loss: 1557.83591| val_0_rmse: 41.43045|  0:00:23s\n",
      "epoch 38 | loss: 1529.72084| val_0_rmse: 41.54318|  0:00:24s\n",
      "epoch 39 | loss: 1538.43608| val_0_rmse: 42.11644|  0:00:24s\n",
      "epoch 40 | loss: 1543.85267| val_0_rmse: 41.55951|  0:00:25s\n",
      "epoch 41 | loss: 1508.27709| val_0_rmse: 41.31704|  0:00:25s\n",
      "epoch 42 | loss: 1497.12612| val_0_rmse: 41.72899|  0:00:26s\n",
      "epoch 43 | loss: 1470.67577| val_0_rmse: 41.98176|  0:00:27s\n",
      "epoch 44 | loss: 1422.68548| val_0_rmse: 41.67265|  0:00:27s\n",
      "epoch 45 | loss: 1396.90601| val_0_rmse: 42.15318|  0:00:28s\n",
      "epoch 46 | loss: 1386.42843| val_0_rmse: 41.58909|  0:00:28s\n",
      "epoch 47 | loss: 1378.31502| val_0_rmse: 41.57904|  0:00:29s\n",
      "epoch 48 | loss: 1420.38754| val_0_rmse: 41.16145|  0:00:30s\n",
      "epoch 49 | loss: 1381.31964| val_0_rmse: 41.24927|  0:00:30s\n",
      "epoch 50 | loss: 1355.58266| val_0_rmse: 41.07306|  0:00:31s\n",
      "epoch 51 | loss: 1319.24823| val_0_rmse: 41.3536 |  0:00:31s\n",
      "epoch 52 | loss: 1285.29089| val_0_rmse: 41.24252|  0:00:32s\n",
      "epoch 53 | loss: 1305.05882| val_0_rmse: 41.16541|  0:00:33s\n",
      "epoch 54 | loss: 1270.76356| val_0_rmse: 41.34547|  0:00:33s\n",
      "epoch 55 | loss: 1245.81753| val_0_rmse: 41.70211|  0:00:34s\n",
      "epoch 56 | loss: 1214.70622| val_0_rmse: 41.76265|  0:00:35s\n",
      "epoch 57 | loss: 1216.94517| val_0_rmse: 41.45062|  0:00:35s\n",
      "epoch 58 | loss: 1201.66506| val_0_rmse: 41.3661 |  0:00:36s\n",
      "epoch 59 | loss: 1190.11064| val_0_rmse: 40.91036|  0:00:36s\n",
      "epoch 60 | loss: 1138.88605| val_0_rmse: 41.85371|  0:00:37s\n",
      "epoch 61 | loss: 1120.27405| val_0_rmse: 41.19359|  0:00:38s\n",
      "epoch 62 | loss: 1148.41086| val_0_rmse: 42.22725|  0:00:38s\n",
      "epoch 63 | loss: 1124.29606| val_0_rmse: 41.93632|  0:00:39s\n",
      "epoch 64 | loss: 1098.82041| val_0_rmse: 41.30588|  0:00:39s\n",
      "epoch 65 | loss: 1089.30819| val_0_rmse: 41.54275|  0:00:40s\n",
      "epoch 66 | loss: 1073.9705| val_0_rmse: 42.65861|  0:00:41s\n",
      "epoch 67 | loss: 1068.55892| val_0_rmse: 42.23876|  0:00:41s\n",
      "epoch 68 | loss: 1035.04966| val_0_rmse: 41.86169|  0:00:42s\n",
      "epoch 69 | loss: 1041.7745| val_0_rmse: 43.17347|  0:00:43s\n",
      "epoch 70 | loss: 1009.42807| val_0_rmse: 42.3894 |  0:00:43s\n",
      "epoch 71 | loss: 1002.9386| val_0_rmse: 42.43504|  0:00:44s\n",
      "epoch 72 | loss: 987.76123| val_0_rmse: 42.59521|  0:00:44s\n",
      "epoch 73 | loss: 964.86786| val_0_rmse: 42.48457|  0:00:45s\n",
      "epoch 74 | loss: 951.66859| val_0_rmse: 42.74365|  0:00:46s\n",
      "epoch 75 | loss: 966.74228| val_0_rmse: 42.79952|  0:00:46s\n",
      "epoch 76 | loss: 935.44182| val_0_rmse: 44.31709|  0:00:47s\n",
      "epoch 77 | loss: 896.85094| val_0_rmse: 41.99522|  0:00:47s\n",
      "epoch 78 | loss: 907.02265| val_0_rmse: 42.93845|  0:00:48s\n",
      "epoch 79 | loss: 878.338 | val_0_rmse: 41.87501|  0:00:49s\n",
      "\n",
      "Early stopping occurred at epoch 79 with best_epoch = 59 and best_val_0_rmse = 40.91036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/spa-d4/grad/mfe261/Projects/PREPARE/envs/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabNet rmse:  40.91035912554759\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001001 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1932\n",
      "[LightGBM] [Info] Number of data points in the train set: 3474, number of used features: 195\n",
      "[LightGBM] [Info] Start training from score 156.375360\n",
      "Training until validation scores don't improve for 500 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/spa-d4/grad/mfe261/Projects/PREPARE/envs/lib/python3.9/site-packages/lightgbm/engine.py:204: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2065]\tvalid_0's rmse: 38.9767\n",
      "0:\tlearn: 60.9116757\ttest: 59.6139346\tbest: 59.6139346 (0)\ttotal: 17.5ms\tremaining: 2m 54s\n",
      "100:\tlearn: 45.2551866\ttest: 46.8663004\tbest: 46.8663004 (100)\ttotal: 2.05s\tremaining: 3m 20s\n",
      "200:\tlearn: 39.0381907\ttest: 42.8832168\tbest: 42.8832168 (200)\ttotal: 4.29s\tremaining: 3m 29s\n",
      "300:\tlearn: 35.5866766\ttest: 41.2763239\tbest: 41.2763239 (300)\ttotal: 6.66s\tremaining: 3m 34s\n",
      "400:\tlearn: 33.2124296\ttest: 40.4566551\tbest: 40.4566551 (400)\ttotal: 8.97s\tremaining: 3m 34s\n",
      "500:\tlearn: 31.4563911\ttest: 39.9630781\tbest: 39.9630781 (500)\ttotal: 11.4s\tremaining: 3m 35s\n",
      "600:\tlearn: 30.0788486\ttest: 39.6401711\tbest: 39.6401711 (600)\ttotal: 13.8s\tremaining: 3m 35s\n",
      "700:\tlearn: 28.8538656\ttest: 39.3996568\tbest: 39.3996568 (700)\ttotal: 16.3s\tremaining: 3m 35s\n",
      "800:\tlearn: 27.7510599\ttest: 39.2196675\tbest: 39.2196675 (800)\ttotal: 18.7s\tremaining: 3m 34s\n",
      "900:\tlearn: 26.7858254\ttest: 39.0791816\tbest: 39.0791816 (900)\ttotal: 21.2s\tremaining: 3m 33s\n",
      "1000:\tlearn: 25.8619697\ttest: 38.9420669\tbest: 38.9420669 (1000)\ttotal: 23.7s\tremaining: 3m 32s\n",
      "1100:\tlearn: 25.0546035\ttest: 38.8494228\tbest: 38.8494228 (1100)\ttotal: 26.3s\tremaining: 3m 32s\n",
      "1200:\tlearn: 24.2995565\ttest: 38.7633846\tbest: 38.7633846 (1200)\ttotal: 28.9s\tremaining: 3m 31s\n",
      "1300:\tlearn: 23.4295372\ttest: 38.6470881\tbest: 38.6470881 (1300)\ttotal: 31.7s\tremaining: 3m 31s\n",
      "1400:\tlearn: 22.6651172\ttest: 38.5697838\tbest: 38.5697838 (1400)\ttotal: 34.4s\tremaining: 3m 31s\n",
      "1500:\tlearn: 21.9476878\ttest: 38.5016027\tbest: 38.5006733 (1498)\ttotal: 37.2s\tremaining: 3m 30s\n",
      "1600:\tlearn: 21.2838135\ttest: 38.4652003\tbest: 38.4640166 (1590)\ttotal: 40.1s\tremaining: 3m 30s\n",
      "1700:\tlearn: 20.6833984\ttest: 38.4165939\tbest: 38.4162533 (1699)\ttotal: 42.9s\tremaining: 3m 29s\n",
      "1800:\tlearn: 20.1172286\ttest: 38.3836982\tbest: 38.3836982 (1800)\ttotal: 45.9s\tremaining: 3m 29s\n",
      "1900:\tlearn: 19.5815354\ttest: 38.3412120\tbest: 38.3412120 (1900)\ttotal: 48.8s\tremaining: 3m 27s\n",
      "2000:\tlearn: 19.0287136\ttest: 38.2891162\tbest: 38.2891162 (2000)\ttotal: 51.7s\tremaining: 3m 26s\n",
      "2100:\tlearn: 18.5532187\ttest: 38.2534880\tbest: 38.2528913 (2094)\ttotal: 54.4s\tremaining: 3m 24s\n",
      "2200:\tlearn: 18.1050653\ttest: 38.2071525\tbest: 38.2071525 (2200)\ttotal: 57.2s\tremaining: 3m 22s\n",
      "2300:\tlearn: 17.6978431\ttest: 38.1794114\tbest: 38.1793616 (2299)\ttotal: 1m\tremaining: 3m 20s\n",
      "2400:\tlearn: 17.2715827\ttest: 38.1620969\tbest: 38.1593886 (2394)\ttotal: 1m 2s\tremaining: 3m 19s\n",
      "2500:\tlearn: 16.9103913\ttest: 38.1430912\tbest: 38.1421832 (2498)\ttotal: 1m 5s\tremaining: 3m 17s\n",
      "2600:\tlearn: 16.5556814\ttest: 38.1069357\tbest: 38.1069357 (2600)\ttotal: 1m 8s\tremaining: 3m 15s\n",
      "2700:\tlearn: 16.1266450\ttest: 38.0832023\tbest: 38.0832023 (2700)\ttotal: 1m 11s\tremaining: 3m 12s\n",
      "2800:\tlearn: 15.7845427\ttest: 38.0668410\tbest: 38.0650413 (2792)\ttotal: 1m 14s\tremaining: 3m 10s\n",
      "2900:\tlearn: 15.4454913\ttest: 38.0489021\tbest: 38.0481753 (2894)\ttotal: 1m 17s\tremaining: 3m 8s\n",
      "3000:\tlearn: 15.1029891\ttest: 38.0322993\tbest: 38.0315504 (2989)\ttotal: 1m 19s\tremaining: 3m 6s\n",
      "3100:\tlearn: 14.7750189\ttest: 38.0148789\tbest: 38.0110691 (3093)\ttotal: 1m 22s\tremaining: 3m 3s\n",
      "3200:\tlearn: 14.4637473\ttest: 38.0040291\tbest: 38.0040291 (3200)\ttotal: 1m 25s\tremaining: 3m 1s\n",
      "3300:\tlearn: 14.1552577\ttest: 37.9865036\tbest: 37.9865036 (3300)\ttotal: 1m 28s\tremaining: 2m 59s\n",
      "3400:\tlearn: 13.8648230\ttest: 37.9700725\tbest: 37.9698610 (3386)\ttotal: 1m 31s\tremaining: 2m 56s\n",
      "3500:\tlearn: 13.5537841\ttest: 37.9567264\tbest: 37.9554693 (3495)\ttotal: 1m 34s\tremaining: 2m 54s\n",
      "3600:\tlearn: 13.2909343\ttest: 37.9421570\tbest: 37.9409096 (3595)\ttotal: 1m 36s\tremaining: 2m 52s\n",
      "3700:\tlearn: 12.9882449\ttest: 37.9285925\tbest: 37.9280274 (3689)\ttotal: 1m 39s\tremaining: 2m 49s\n",
      "3800:\tlearn: 12.7100583\ttest: 37.9264212\tbest: 37.9256869 (3780)\ttotal: 1m 42s\tremaining: 2m 47s\n",
      "3900:\tlearn: 12.4305391\ttest: 37.9062355\tbest: 37.9061142 (3899)\ttotal: 1m 45s\tremaining: 2m 45s\n",
      "4000:\tlearn: 12.1264169\ttest: 37.8953746\tbest: 37.8953746 (4000)\ttotal: 1m 48s\tremaining: 2m 42s\n",
      "4100:\tlearn: 11.8669558\ttest: 37.8873551\tbest: 37.8854659 (4094)\ttotal: 1m 51s\tremaining: 2m 39s\n",
      "4200:\tlearn: 11.5908973\ttest: 37.8825784\tbest: 37.8822273 (4198)\ttotal: 1m 54s\tremaining: 2m 37s\n",
      "4300:\tlearn: 11.3361974\ttest: 37.8738469\tbest: 37.8715738 (4295)\ttotal: 1m 56s\tremaining: 2m 34s\n",
      "4400:\tlearn: 11.0970233\ttest: 37.8607288\tbest: 37.8587910 (4391)\ttotal: 1m 59s\tremaining: 2m 32s\n",
      "4500:\tlearn: 10.9017623\ttest: 37.8593322\tbest: 37.8570512 (4487)\ttotal: 2m 2s\tremaining: 2m 29s\n",
      "4600:\tlearn: 10.6960094\ttest: 37.8479908\tbest: 37.8479908 (4600)\ttotal: 2m 5s\tremaining: 2m 27s\n",
      "4700:\tlearn: 10.4701961\ttest: 37.8473968\tbest: 37.8434684 (4660)\ttotal: 2m 8s\tremaining: 2m 24s\n",
      "4800:\tlearn: 10.2809691\ttest: 37.8367387\tbest: 37.8364196 (4797)\ttotal: 2m 11s\tremaining: 2m 22s\n",
      "4900:\tlearn: 10.0653273\ttest: 37.8310238\tbest: 37.8306033 (4897)\ttotal: 2m 13s\tremaining: 2m 19s\n",
      "5000:\tlearn: 9.8653093\ttest: 37.8192870\tbest: 37.8188291 (4993)\ttotal: 2m 16s\tremaining: 2m 16s\n",
      "5100:\tlearn: 9.6872666\ttest: 37.8083094\tbest: 37.8074537 (5098)\ttotal: 2m 19s\tremaining: 2m 14s\n",
      "5200:\tlearn: 9.4950029\ttest: 37.7998495\tbest: 37.7998495 (5200)\ttotal: 2m 22s\tremaining: 2m 11s\n",
      "5300:\tlearn: 9.3105076\ttest: 37.7972648\tbest: 37.7967939 (5238)\ttotal: 2m 25s\tremaining: 2m 9s\n",
      "5400:\tlearn: 9.1204404\ttest: 37.7967464\tbest: 37.7928618 (5371)\ttotal: 2m 28s\tremaining: 2m 6s\n",
      "5500:\tlearn: 8.9458417\ttest: 37.7853655\tbest: 37.7848732 (5497)\ttotal: 2m 31s\tremaining: 2m 3s\n",
      "5600:\tlearn: 8.7757699\ttest: 37.7775372\tbest: 37.7762802 (5589)\ttotal: 2m 33s\tremaining: 2m\n",
      "5700:\tlearn: 8.6260999\ttest: 37.7758991\tbest: 37.7748496 (5693)\ttotal: 2m 36s\tremaining: 1m 58s\n",
      "5800:\tlearn: 8.4678593\ttest: 37.7727100\tbest: 37.7725864 (5798)\ttotal: 2m 39s\tremaining: 1m 55s\n",
      "5900:\tlearn: 8.3188016\ttest: 37.7775078\tbest: 37.7722829 (5806)\ttotal: 2m 42s\tremaining: 1m 52s\n",
      "6000:\tlearn: 8.1668209\ttest: 37.7715749\tbest: 37.7713264 (5996)\ttotal: 2m 45s\tremaining: 1m 50s\n",
      "6100:\tlearn: 8.0136267\ttest: 37.7651764\tbest: 37.7651642 (6096)\ttotal: 2m 48s\tremaining: 1m 47s\n",
      "6200:\tlearn: 7.8573984\ttest: 37.7627756\tbest: 37.7624438 (6199)\ttotal: 2m 51s\tremaining: 1m 45s\n",
      "6300:\tlearn: 7.7241700\ttest: 37.7577072\tbest: 37.7574129 (6299)\ttotal: 2m 54s\tremaining: 1m 42s\n",
      "6400:\tlearn: 7.5798816\ttest: 37.7510183\tbest: 37.7510183 (6400)\ttotal: 2m 57s\tremaining: 1m 39s\n",
      "6500:\tlearn: 7.4427083\ttest: 37.7505403\tbest: 37.7499664 (6477)\ttotal: 3m\tremaining: 1m 36s\n",
      "6600:\tlearn: 7.3153936\ttest: 37.7480258\tbest: 37.7480258 (6600)\ttotal: 3m 3s\tremaining: 1m 34s\n",
      "6700:\tlearn: 7.1810110\ttest: 37.7506841\tbest: 37.7476888 (6601)\ttotal: 3m 5s\tremaining: 1m 31s\n",
      "6800:\tlearn: 7.0537606\ttest: 37.7486987\tbest: 37.7467437 (6773)\ttotal: 3m 8s\tremaining: 1m 28s\n",
      "6900:\tlearn: 6.9324730\ttest: 37.7461853\tbest: 37.7447025 (6883)\ttotal: 3m 11s\tremaining: 1m 26s\n",
      "7000:\tlearn: 6.8229859\ttest: 37.7450686\tbest: 37.7441539 (6993)\ttotal: 3m 14s\tremaining: 1m 23s\n",
      "7100:\tlearn: 6.6865944\ttest: 37.7468562\tbest: 37.7441539 (6993)\ttotal: 3m 17s\tremaining: 1m 20s\n",
      "7200:\tlearn: 6.5819702\ttest: 37.7484170\tbest: 37.7441539 (6993)\ttotal: 3m 20s\tremaining: 1m 17s\n",
      "7300:\tlearn: 6.4658855\ttest: 37.7485384\tbest: 37.7441539 (6993)\ttotal: 3m 23s\tremaining: 1m 15s\n",
      "7400:\tlearn: 6.3480046\ttest: 37.7463624\tbest: 37.7441539 (6993)\ttotal: 3m 26s\tremaining: 1m 12s\n",
      "Stopped by overfitting detector  (500 iterations wait)\n",
      "\n",
      "bestTest = 37.74415388\n",
      "bestIteration = 6993\n",
      "\n",
      "Shrink model to first 6994 iterations.\n",
      "[0]\tvalidation_0-rmse:59.59052\tvalidation_0-root_mean_squared_error:59.59052\n",
      "[100]\tvalidation_0-rmse:46.87435\tvalidation_0-root_mean_squared_error:46.87435\n",
      "[200]\tvalidation_0-rmse:43.34095\tvalidation_0-root_mean_squared_error:43.34095\n",
      "[300]\tvalidation_0-rmse:41.89472\tvalidation_0-root_mean_squared_error:41.89472\n",
      "[400]\tvalidation_0-rmse:41.16109\tvalidation_0-root_mean_squared_error:41.16109\n",
      "[500]\tvalidation_0-rmse:40.74059\tvalidation_0-root_mean_squared_error:40.74059\n",
      "[600]\tvalidation_0-rmse:40.48264\tvalidation_0-root_mean_squared_error:40.48264\n",
      "[700]\tvalidation_0-rmse:40.31726\tvalidation_0-root_mean_squared_error:40.31726\n",
      "[800]\tvalidation_0-rmse:40.18951\tvalidation_0-root_mean_squared_error:40.18951\n",
      "[900]\tvalidation_0-rmse:40.11079\tvalidation_0-root_mean_squared_error:40.11079\n",
      "[1000]\tvalidation_0-rmse:40.02462\tvalidation_0-root_mean_squared_error:40.02462\n",
      "[1100]\tvalidation_0-rmse:39.94500\tvalidation_0-root_mean_squared_error:39.94500\n",
      "[1200]\tvalidation_0-rmse:39.88492\tvalidation_0-root_mean_squared_error:39.88492\n",
      "[1300]\tvalidation_0-rmse:39.81741\tvalidation_0-root_mean_squared_error:39.81741\n",
      "[1400]\tvalidation_0-rmse:39.76882\tvalidation_0-root_mean_squared_error:39.76881\n",
      "[1500]\tvalidation_0-rmse:39.71366\tvalidation_0-root_mean_squared_error:39.71366\n",
      "[1600]\tvalidation_0-rmse:39.66257\tvalidation_0-root_mean_squared_error:39.66257\n",
      "[1700]\tvalidation_0-rmse:39.61295\tvalidation_0-root_mean_squared_error:39.61295\n",
      "[1800]\tvalidation_0-rmse:39.57717\tvalidation_0-root_mean_squared_error:39.57717\n",
      "[1900]\tvalidation_0-rmse:39.54128\tvalidation_0-root_mean_squared_error:39.54128\n",
      "[2000]\tvalidation_0-rmse:39.50244\tvalidation_0-root_mean_squared_error:39.50244\n",
      "[2100]\tvalidation_0-rmse:39.46971\tvalidation_0-root_mean_squared_error:39.46971\n",
      "[2200]\tvalidation_0-rmse:39.43502\tvalidation_0-root_mean_squared_error:39.43501\n",
      "[2300]\tvalidation_0-rmse:39.41233\tvalidation_0-root_mean_squared_error:39.41233\n",
      "[2400]\tvalidation_0-rmse:39.39549\tvalidation_0-root_mean_squared_error:39.39549\n",
      "[2500]\tvalidation_0-rmse:39.37579\tvalidation_0-root_mean_squared_error:39.37579\n",
      "[2600]\tvalidation_0-rmse:39.36064\tvalidation_0-root_mean_squared_error:39.36064\n",
      "[2700]\tvalidation_0-rmse:39.34967\tvalidation_0-root_mean_squared_error:39.34967\n",
      "[2800]\tvalidation_0-rmse:39.34740\tvalidation_0-root_mean_squared_error:39.34740\n",
      "[2900]\tvalidation_0-rmse:39.34508\tvalidation_0-root_mean_squared_error:39.34508\n",
      "[3000]\tvalidation_0-rmse:39.34368\tvalidation_0-root_mean_squared_error:39.34368\n",
      "[3100]\tvalidation_0-rmse:39.33467\tvalidation_0-root_mean_squared_error:39.33467\n",
      "[3200]\tvalidation_0-rmse:39.32854\tvalidation_0-root_mean_squared_error:39.32853\n",
      "[3300]\tvalidation_0-rmse:39.31650\tvalidation_0-root_mean_squared_error:39.31650\n",
      "[3400]\tvalidation_0-rmse:39.30878\tvalidation_0-root_mean_squared_error:39.30878\n",
      "[3500]\tvalidation_0-rmse:39.29525\tvalidation_0-root_mean_squared_error:39.29525\n",
      "[3600]\tvalidation_0-rmse:39.28518\tvalidation_0-root_mean_squared_error:39.28518\n",
      "[3700]\tvalidation_0-rmse:39.28206\tvalidation_0-root_mean_squared_error:39.28205\n",
      "[3800]\tvalidation_0-rmse:39.27098\tvalidation_0-root_mean_squared_error:39.27098\n",
      "[3900]\tvalidation_0-rmse:39.26748\tvalidation_0-root_mean_squared_error:39.26747\n",
      "[4000]\tvalidation_0-rmse:39.25649\tvalidation_0-root_mean_squared_error:39.25649\n",
      "[4100]\tvalidation_0-rmse:39.25728\tvalidation_0-root_mean_squared_error:39.25729\n",
      "[4200]\tvalidation_0-rmse:39.25667\tvalidation_0-root_mean_squared_error:39.25666\n",
      "[4300]\tvalidation_0-rmse:39.24910\tvalidation_0-root_mean_squared_error:39.24910\n",
      "[4400]\tvalidation_0-rmse:39.25845\tvalidation_0-root_mean_squared_error:39.25845\n",
      "[4500]\tvalidation_0-rmse:39.26184\tvalidation_0-root_mean_squared_error:39.26184\n",
      "[4600]\tvalidation_0-rmse:39.26137\tvalidation_0-root_mean_squared_error:39.26137\n",
      "[4700]\tvalidation_0-rmse:39.26398\tvalidation_0-root_mean_squared_error:39.26398\n",
      "[4800]\tvalidation_0-rmse:39.26399\tvalidation_0-root_mean_squared_error:39.26399\n",
      "[4803]\tvalidation_0-rmse:39.26501\tvalidation_0-root_mean_squared_error:39.26501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [17:23, 346.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest rmse:  39.21013864482763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_943508/2235415930.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_X.drop(columns=['uid'], inplace=True)\n",
      "/tmp/ipykernel_943508/2235415930.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  val_X.drop(columns=['uid'], inplace=True)\n",
      "/tmp/ipykernel_943508/2235415930.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_X_processed.drop(columns=['uid'], inplace=True)\n",
      "/tmp/ipykernel_943508/2235415930.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  val_X_processed.drop(columns=['uid'], inplace=True)\n",
      "/u/spa-d4/grad/mfe261/Projects/PREPARE/envs/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 26932.67245| val_0_rmse: 153.17664|  0:00:00s\n",
      "epoch 1  | loss: 21048.53653| val_0_rmse: 107.83884|  0:00:01s\n",
      "epoch 2  | loss: 10658.34189| val_0_rmse: 70.4447 |  0:00:01s\n",
      "epoch 3  | loss: 3152.79966| val_0_rmse: 51.53293|  0:00:02s\n",
      "epoch 4  | loss: 2408.53721| val_0_rmse: 49.50357|  0:00:03s\n",
      "epoch 5  | loss: 2330.11542| val_0_rmse: 50.74617|  0:00:03s\n",
      "epoch 6  | loss: 2265.66523| val_0_rmse: 48.30674|  0:00:04s\n",
      "epoch 7  | loss: 2258.42169| val_0_rmse: 46.20687|  0:00:04s\n",
      "epoch 8  | loss: 2257.98776| val_0_rmse: 48.60777|  0:00:05s\n",
      "epoch 9  | loss: 2263.98357| val_0_rmse: 48.39067|  0:00:06s\n",
      "epoch 10 | loss: 2181.4696| val_0_rmse: 46.73265|  0:00:06s\n",
      "epoch 11 | loss: 2185.38603| val_0_rmse: 46.19904|  0:00:07s\n",
      "epoch 12 | loss: 2182.78446| val_0_rmse: 46.32594|  0:00:08s\n",
      "epoch 13 | loss: 2125.01167| val_0_rmse: 45.47919|  0:00:08s\n",
      "epoch 14 | loss: 2083.50343| val_0_rmse: 45.28146|  0:00:09s\n",
      "epoch 15 | loss: 2071.993| val_0_rmse: 45.34933|  0:00:09s\n",
      "epoch 16 | loss: 2048.33171| val_0_rmse: 45.76153|  0:00:10s\n",
      "epoch 17 | loss: 1952.14204| val_0_rmse: 44.2792 |  0:00:11s\n",
      "epoch 18 | loss: 1920.11262| val_0_rmse: 43.57527|  0:00:11s\n",
      "epoch 19 | loss: 1876.71303| val_0_rmse: 43.38761|  0:00:12s\n",
      "epoch 20 | loss: 1818.87959| val_0_rmse: 43.19122|  0:00:12s\n",
      "epoch 21 | loss: 1781.55951| val_0_rmse: 43.43032|  0:00:13s\n",
      "epoch 22 | loss: 1784.19242| val_0_rmse: 44.60956|  0:00:14s\n",
      "epoch 23 | loss: 1737.53578| val_0_rmse: 42.6335 |  0:00:14s\n",
      "epoch 24 | loss: 1810.38049| val_0_rmse: 42.97237|  0:00:15s\n",
      "epoch 25 | loss: 1755.325| val_0_rmse: 42.57654|  0:00:15s\n",
      "epoch 26 | loss: 1687.73905| val_0_rmse: 41.18512|  0:00:16s\n",
      "epoch 27 | loss: 1670.83734| val_0_rmse: 41.44934|  0:00:17s\n",
      "epoch 28 | loss: 1729.18996| val_0_rmse: 41.4051 |  0:00:17s\n",
      "epoch 29 | loss: 1688.32753| val_0_rmse: 43.74204|  0:00:18s\n",
      "epoch 30 | loss: 1661.90418| val_0_rmse: 41.28671|  0:00:19s\n",
      "epoch 31 | loss: 1700.91682| val_0_rmse: 41.90322|  0:00:19s\n",
      "epoch 32 | loss: 1664.01834| val_0_rmse: 41.3656 |  0:00:20s\n",
      "epoch 33 | loss: 1648.03575| val_0_rmse: 41.93834|  0:00:20s\n",
      "epoch 34 | loss: 1601.97386| val_0_rmse: 42.49554|  0:00:21s\n",
      "epoch 35 | loss: 1555.15427| val_0_rmse: 41.29842|  0:00:22s\n",
      "epoch 36 | loss: 1557.6603| val_0_rmse: 40.8735 |  0:00:22s\n",
      "epoch 37 | loss: 1550.31595| val_0_rmse: 40.68349|  0:00:23s\n",
      "epoch 38 | loss: 1509.2682| val_0_rmse: 40.72719|  0:00:23s\n",
      "epoch 39 | loss: 1525.35063| val_0_rmse: 41.96882|  0:00:24s\n",
      "epoch 40 | loss: 1510.92085| val_0_rmse: 41.31274|  0:00:25s\n",
      "epoch 41 | loss: 1480.63871| val_0_rmse: 40.58879|  0:00:25s\n",
      "epoch 42 | loss: 1496.27625| val_0_rmse: 40.74638|  0:00:26s\n",
      "epoch 43 | loss: 1458.55761| val_0_rmse: 40.94785|  0:00:27s\n",
      "epoch 44 | loss: 1469.50145| val_0_rmse: 40.25121|  0:00:27s\n",
      "epoch 45 | loss: 1457.56822| val_0_rmse: 40.13425|  0:00:28s\n",
      "epoch 46 | loss: 1416.55051| val_0_rmse: 40.22479|  0:00:28s\n",
      "epoch 47 | loss: 1372.81927| val_0_rmse: 41.39486|  0:00:29s\n",
      "epoch 48 | loss: 1403.79825| val_0_rmse: 40.23566|  0:00:30s\n",
      "epoch 49 | loss: 1378.85555| val_0_rmse: 40.40938|  0:00:30s\n",
      "epoch 50 | loss: 1338.58286| val_0_rmse: 41.57039|  0:00:31s\n",
      "epoch 51 | loss: 1327.9132| val_0_rmse: 40.9034 |  0:00:31s\n",
      "epoch 52 | loss: 1317.79037| val_0_rmse: 40.74919|  0:00:32s\n",
      "epoch 53 | loss: 1352.95063| val_0_rmse: 40.44424|  0:00:33s\n",
      "epoch 54 | loss: 1300.65875| val_0_rmse: 39.91793|  0:00:33s\n",
      "epoch 55 | loss: 1298.9926| val_0_rmse: 40.79422|  0:00:34s\n",
      "epoch 56 | loss: 1244.68553| val_0_rmse: 40.61101|  0:00:34s\n",
      "epoch 57 | loss: 1260.71252| val_0_rmse: 39.78398|  0:00:35s\n",
      "epoch 58 | loss: 1226.66647| val_0_rmse: 40.27598|  0:00:36s\n",
      "epoch 59 | loss: 1220.52879| val_0_rmse: 39.62965|  0:00:36s\n",
      "epoch 60 | loss: 1173.76245| val_0_rmse: 39.13453|  0:00:37s\n",
      "epoch 61 | loss: 1194.63326| val_0_rmse: 39.49074|  0:00:38s\n",
      "epoch 62 | loss: 1189.02072| val_0_rmse: 40.16183|  0:00:38s\n",
      "epoch 63 | loss: 1164.21449| val_0_rmse: 40.06113|  0:00:39s\n",
      "epoch 64 | loss: 1165.41177| val_0_rmse: 40.32164|  0:00:39s\n",
      "epoch 65 | loss: 1140.23258| val_0_rmse: 40.26772|  0:00:40s\n",
      "epoch 66 | loss: 1085.04618| val_0_rmse: 40.30048|  0:00:41s\n",
      "epoch 67 | loss: 1098.85742| val_0_rmse: 40.01643|  0:00:41s\n",
      "epoch 68 | loss: 1029.68435| val_0_rmse: 40.73214|  0:00:42s\n",
      "epoch 69 | loss: 1084.71226| val_0_rmse: 40.92121|  0:00:42s\n",
      "epoch 70 | loss: 1060.12979| val_0_rmse: 40.12611|  0:00:43s\n",
      "epoch 71 | loss: 1039.8434| val_0_rmse: 40.23488|  0:00:44s\n",
      "epoch 72 | loss: 1010.26667| val_0_rmse: 41.40985|  0:00:44s\n",
      "epoch 73 | loss: 1012.23512| val_0_rmse: 40.70686|  0:00:45s\n",
      "epoch 74 | loss: 1016.82667| val_0_rmse: 40.06045|  0:00:45s\n",
      "epoch 75 | loss: 988.94319| val_0_rmse: 40.5595 |  0:00:46s\n",
      "epoch 76 | loss: 954.58081| val_0_rmse: 40.70002|  0:00:47s\n",
      "epoch 77 | loss: 942.95976| val_0_rmse: 40.67664|  0:00:47s\n",
      "epoch 78 | loss: 948.52707| val_0_rmse: 41.00198|  0:00:48s\n",
      "epoch 79 | loss: 961.98273| val_0_rmse: 40.48682|  0:00:49s\n",
      "epoch 80 | loss: 947.52854| val_0_rmse: 40.44751|  0:00:49s\n",
      "\n",
      "Early stopping occurred at epoch 80 with best_epoch = 60 and best_val_0_rmse = 39.13453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/spa-d4/grad/mfe261/Projects/PREPARE/envs/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabNet rmse:  39.13452946108786\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000957 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1924\n",
      "[LightGBM] [Info] Number of data points in the train set: 3475, number of used features: 195\n",
      "[LightGBM] [Info] Start training from score 157.237122\n",
      "Training until validation scores don't improve for 500 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/spa-d4/grad/mfe261/Projects/PREPARE/envs/lib/python3.9/site-packages/lightgbm/engine.py:204: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[3802]\tvalid_0's rmse: 36.3882\n",
      "0:\tlearn: 60.6043997\ttest: 60.8094701\tbest: 60.8094701 (0)\ttotal: 20.7ms\tremaining: 3m 27s\n",
      "100:\tlearn: 45.2304994\ttest: 46.8621180\tbest: 46.8621180 (100)\ttotal: 2.19s\tremaining: 3m 34s\n",
      "200:\tlearn: 39.0570311\ttest: 42.5034486\tbest: 42.5034486 (200)\ttotal: 4.48s\tremaining: 3m 38s\n",
      "300:\tlearn: 35.6824354\ttest: 40.7313926\tbest: 40.7313926 (300)\ttotal: 6.82s\tremaining: 3m 39s\n",
      "400:\tlearn: 33.4889507\ttest: 39.9037931\tbest: 39.9037931 (400)\ttotal: 9.15s\tremaining: 3m 38s\n",
      "500:\tlearn: 31.7965815\ttest: 39.3931466\tbest: 39.3931466 (500)\ttotal: 11.5s\tremaining: 3m 38s\n",
      "600:\tlearn: 30.3798993\ttest: 39.0556815\tbest: 39.0556815 (600)\ttotal: 13.9s\tremaining: 3m 37s\n",
      "700:\tlearn: 29.1462259\ttest: 38.7950023\tbest: 38.7950023 (700)\ttotal: 16.3s\tremaining: 3m 36s\n",
      "800:\tlearn: 28.1344076\ttest: 38.5992684\tbest: 38.5992684 (800)\ttotal: 18.8s\tremaining: 3m 36s\n",
      "900:\tlearn: 27.1589611\ttest: 38.4292030\tbest: 38.4290308 (897)\ttotal: 21.3s\tremaining: 3m 35s\n",
      "1000:\tlearn: 26.4320711\ttest: 38.3153082\tbest: 38.3153082 (1000)\ttotal: 24.1s\tremaining: 3m 36s\n",
      "1100:\tlearn: 25.6126536\ttest: 38.1805872\tbest: 38.1805872 (1100)\ttotal: 26.8s\tremaining: 3m 36s\n",
      "1200:\tlearn: 24.7570987\ttest: 38.0474406\tbest: 38.0474406 (1200)\ttotal: 29.9s\tremaining: 3m 38s\n",
      "1300:\tlearn: 23.9859867\ttest: 37.9599479\tbest: 37.9599479 (1300)\ttotal: 32.7s\tremaining: 3m 38s\n",
      "1400:\tlearn: 23.2241881\ttest: 37.8607541\tbest: 37.8605066 (1399)\ttotal: 35.5s\tremaining: 3m 37s\n",
      "1500:\tlearn: 22.5083934\ttest: 37.7791271\tbest: 37.7789790 (1499)\ttotal: 38.4s\tremaining: 3m 37s\n",
      "1600:\tlearn: 21.7510697\ttest: 37.7123502\tbest: 37.7116391 (1594)\ttotal: 41.3s\tremaining: 3m 36s\n",
      "1700:\tlearn: 21.0454574\ttest: 37.6314546\tbest: 37.6309444 (1696)\ttotal: 44.2s\tremaining: 3m 35s\n",
      "1800:\tlearn: 20.3981911\ttest: 37.5564297\tbest: 37.5559727 (1799)\ttotal: 47s\tremaining: 3m 33s\n",
      "1900:\tlearn: 19.7887565\ttest: 37.5056221\tbest: 37.5056221 (1900)\ttotal: 49.8s\tremaining: 3m 32s\n",
      "2000:\tlearn: 19.2523289\ttest: 37.4480604\tbest: 37.4480604 (2000)\ttotal: 52.6s\tremaining: 3m 30s\n",
      "2100:\tlearn: 18.7020391\ttest: 37.3962331\tbest: 37.3960260 (2099)\ttotal: 55.5s\tremaining: 3m 28s\n",
      "2200:\tlearn: 18.1983622\ttest: 37.3575676\tbest: 37.3575676 (2200)\ttotal: 58.5s\tremaining: 3m 27s\n",
      "2300:\tlearn: 17.7165329\ttest: 37.3167205\tbest: 37.3159838 (2291)\ttotal: 1m 1s\tremaining: 3m 25s\n",
      "2400:\tlearn: 17.2707856\ttest: 37.2948820\tbest: 37.2948820 (2400)\ttotal: 1m 4s\tremaining: 3m 23s\n",
      "2500:\tlearn: 16.8447402\ttest: 37.2644203\tbest: 37.2633790 (2497)\ttotal: 1m 7s\tremaining: 3m 21s\n",
      "2600:\tlearn: 16.4134635\ttest: 37.2316495\tbest: 37.2302604 (2595)\ttotal: 1m 10s\tremaining: 3m 19s\n",
      "2700:\tlearn: 15.9914059\ttest: 37.1992746\tbest: 37.1980009 (2697)\ttotal: 1m 12s\tremaining: 3m 17s\n",
      "2800:\tlearn: 15.5785278\ttest: 37.1685262\tbest: 37.1685262 (2800)\ttotal: 1m 15s\tremaining: 3m 14s\n",
      "2900:\tlearn: 15.1826018\ttest: 37.1335487\tbest: 37.1335487 (2900)\ttotal: 1m 18s\tremaining: 3m 12s\n",
      "3000:\tlearn: 14.8470120\ttest: 37.1107700\tbest: 37.1107700 (3000)\ttotal: 1m 21s\tremaining: 3m 10s\n",
      "3100:\tlearn: 14.4559452\ttest: 37.0893247\tbest: 37.0893247 (3100)\ttotal: 1m 24s\tremaining: 3m 7s\n",
      "3200:\tlearn: 14.1069310\ttest: 37.0726488\tbest: 37.0721274 (3199)\ttotal: 1m 27s\tremaining: 3m 5s\n",
      "3300:\tlearn: 13.8027622\ttest: 37.0645249\tbest: 37.0645249 (3300)\ttotal: 1m 30s\tremaining: 3m 3s\n",
      "3400:\tlearn: 13.4863052\ttest: 37.0547713\tbest: 37.0538597 (3397)\ttotal: 1m 33s\tremaining: 3m 1s\n",
      "3500:\tlearn: 13.2032821\ttest: 37.0407893\tbest: 37.0407893 (3500)\ttotal: 1m 36s\tremaining: 2m 58s\n",
      "3600:\tlearn: 12.9438902\ttest: 37.0246730\tbest: 37.0244734 (3599)\ttotal: 1m 39s\tremaining: 2m 56s\n",
      "3700:\tlearn: 12.7214510\ttest: 37.0163170\tbest: 37.0144918 (3695)\ttotal: 1m 42s\tremaining: 2m 53s\n",
      "3800:\tlearn: 12.4767553\ttest: 37.0055185\tbest: 37.0051744 (3777)\ttotal: 1m 45s\tremaining: 2m 51s\n",
      "3900:\tlearn: 12.2750958\ttest: 37.0053095\tbest: 37.0038061 (3831)\ttotal: 1m 48s\tremaining: 2m 49s\n",
      "4000:\tlearn: 12.0266633\ttest: 37.0015581\tbest: 37.0015581 (4000)\ttotal: 1m 51s\tremaining: 2m 46s\n",
      "4100:\tlearn: 11.7770743\ttest: 36.9946152\tbest: 36.9908513 (4080)\ttotal: 1m 53s\tremaining: 2m 43s\n",
      "4200:\tlearn: 11.5529558\ttest: 36.9854187\tbest: 36.9854187 (4200)\ttotal: 1m 56s\tremaining: 2m 41s\n",
      "4300:\tlearn: 11.3612000\ttest: 36.9764391\tbest: 36.9762867 (4299)\ttotal: 1m 59s\tremaining: 2m 38s\n",
      "4400:\tlearn: 11.1857952\ttest: 36.9689988\tbest: 36.9670316 (4352)\ttotal: 2m 2s\tremaining: 2m 36s\n",
      "4500:\tlearn: 10.9894449\ttest: 36.9626823\tbest: 36.9614599 (4479)\ttotal: 2m 5s\tremaining: 2m 33s\n",
      "4600:\tlearn: 10.7995540\ttest: 36.9607927\tbest: 36.9606310 (4587)\ttotal: 2m 8s\tremaining: 2m 31s\n",
      "4700:\tlearn: 10.6178529\ttest: 36.9548804\tbest: 36.9541520 (4689)\ttotal: 2m 11s\tremaining: 2m 28s\n",
      "4800:\tlearn: 10.4292882\ttest: 36.9436047\tbest: 36.9434259 (4791)\ttotal: 2m 14s\tremaining: 2m 25s\n",
      "4900:\tlearn: 10.2606043\ttest: 36.9384889\tbest: 36.9381320 (4885)\ttotal: 2m 17s\tremaining: 2m 23s\n",
      "5000:\tlearn: 10.1071057\ttest: 36.9296630\tbest: 36.9294959 (4999)\ttotal: 2m 20s\tremaining: 2m 20s\n",
      "5100:\tlearn: 9.9432274\ttest: 36.9299507\tbest: 36.9283111 (5028)\ttotal: 2m 23s\tremaining: 2m 17s\n",
      "5200:\tlearn: 9.7998378\ttest: 36.9278547\tbest: 36.9278198 (5198)\ttotal: 2m 26s\tremaining: 2m 15s\n",
      "5300:\tlearn: 9.6379611\ttest: 36.9249613\tbest: 36.9244207 (5297)\ttotal: 2m 29s\tremaining: 2m 12s\n",
      "5400:\tlearn: 9.4930051\ttest: 36.9212937\tbest: 36.9201758 (5382)\ttotal: 2m 32s\tremaining: 2m 10s\n",
      "5500:\tlearn: 9.3327246\ttest: 36.9164338\tbest: 36.9163216 (5496)\ttotal: 2m 35s\tremaining: 2m 7s\n",
      "5600:\tlearn: 9.1678677\ttest: 36.9177789\tbest: 36.9163216 (5496)\ttotal: 2m 38s\tremaining: 2m 4s\n",
      "5700:\tlearn: 9.0355429\ttest: 36.9164011\tbest: 36.9136448 (5659)\ttotal: 2m 41s\tremaining: 2m 1s\n",
      "5800:\tlearn: 8.8876264\ttest: 36.9135699\tbest: 36.9131968 (5777)\ttotal: 2m 44s\tremaining: 1m 59s\n",
      "5900:\tlearn: 8.7443398\ttest: 36.9113097\tbest: 36.9105214 (5896)\ttotal: 2m 47s\tremaining: 1m 56s\n",
      "6000:\tlearn: 8.6423110\ttest: 36.9096317\tbest: 36.9096317 (6000)\ttotal: 2m 50s\tremaining: 1m 53s\n",
      "6100:\tlearn: 8.4969697\ttest: 36.9061562\tbest: 36.9059138 (6096)\ttotal: 2m 53s\tremaining: 1m 51s\n",
      "6200:\tlearn: 8.3622191\ttest: 36.9033498\tbest: 36.9023971 (6145)\ttotal: 2m 56s\tremaining: 1m 48s\n",
      "6300:\tlearn: 8.2567373\ttest: 36.8983416\tbest: 36.8981212 (6298)\ttotal: 2m 59s\tremaining: 1m 45s\n",
      "6400:\tlearn: 8.1416633\ttest: 36.8927536\tbest: 36.8927405 (6397)\ttotal: 3m 2s\tremaining: 1m 42s\n",
      "6500:\tlearn: 8.0365918\ttest: 36.8938773\tbest: 36.8912402 (6466)\ttotal: 3m 5s\tremaining: 1m 39s\n",
      "6600:\tlearn: 7.9202118\ttest: 36.8951158\tbest: 36.8912402 (6466)\ttotal: 3m 8s\tremaining: 1m 37s\n",
      "6700:\tlearn: 7.7998065\ttest: 36.8926170\tbest: 36.8907357 (6679)\ttotal: 3m 11s\tremaining: 1m 34s\n",
      "6800:\tlearn: 7.6887332\ttest: 36.8970592\tbest: 36.8907357 (6679)\ttotal: 3m 14s\tremaining: 1m 31s\n",
      "6900:\tlearn: 7.5636499\ttest: 36.8964219\tbest: 36.8907357 (6679)\ttotal: 3m 17s\tremaining: 1m 28s\n",
      "7000:\tlearn: 7.4592894\ttest: 36.8939375\tbest: 36.8907357 (6679)\ttotal: 3m 20s\tremaining: 1m 25s\n",
      "7100:\tlearn: 7.3654372\ttest: 36.8906570\tbest: 36.8906570 (7100)\ttotal: 3m 23s\tremaining: 1m 23s\n",
      "7200:\tlearn: 7.2630368\ttest: 36.8877351\tbest: 36.8866525 (7156)\ttotal: 3m 26s\tremaining: 1m 20s\n",
      "7300:\tlearn: 7.1706114\ttest: 36.8850707\tbest: 36.8850541 (7299)\ttotal: 3m 29s\tremaining: 1m 17s\n",
      "7400:\tlearn: 7.0896186\ttest: 36.8852571\tbest: 36.8840270 (7359)\ttotal: 3m 32s\tremaining: 1m 14s\n",
      "7500:\tlearn: 6.9915237\ttest: 36.8806738\tbest: 36.8806674 (7471)\ttotal: 3m 35s\tremaining: 1m 11s\n",
      "7600:\tlearn: 6.8872494\ttest: 36.8754182\tbest: 36.8750967 (7578)\ttotal: 3m 38s\tremaining: 1m 8s\n",
      "7700:\tlearn: 6.7941296\ttest: 36.8743069\tbest: 36.8725543 (7671)\ttotal: 3m 41s\tremaining: 1m 6s\n",
      "7800:\tlearn: 6.7056360\ttest: 36.8734683\tbest: 36.8725483 (7754)\ttotal: 3m 44s\tremaining: 1m 3s\n",
      "7900:\tlearn: 6.6253943\ttest: 36.8688846\tbest: 36.8688136 (7899)\ttotal: 3m 47s\tremaining: 1m\n",
      "8000:\tlearn: 6.5419986\ttest: 36.8718696\tbest: 36.8687401 (7909)\ttotal: 3m 50s\tremaining: 57.6s\n",
      "8100:\tlearn: 6.4572192\ttest: 36.8702172\tbest: 36.8687401 (7909)\ttotal: 3m 53s\tremaining: 54.7s\n",
      "8200:\tlearn: 6.3712962\ttest: 36.8665750\tbest: 36.8663630 (8165)\ttotal: 3m 56s\tremaining: 51.8s\n",
      "8300:\tlearn: 6.2887840\ttest: 36.8671568\tbest: 36.8660094 (8265)\ttotal: 3m 59s\tremaining: 49s\n",
      "8400:\tlearn: 6.1924493\ttest: 36.8673015\tbest: 36.8660094 (8265)\ttotal: 4m 2s\tremaining: 46.1s\n",
      "8500:\tlearn: 6.1042034\ttest: 36.8651084\tbest: 36.8642145 (8460)\ttotal: 4m 5s\tremaining: 43.2s\n",
      "8600:\tlearn: 6.0292474\ttest: 36.8646674\tbest: 36.8639544 (8596)\ttotal: 4m 8s\tremaining: 40.3s\n",
      "8700:\tlearn: 5.9565626\ttest: 36.8646246\tbest: 36.8639544 (8596)\ttotal: 4m 10s\tremaining: 37.5s\n",
      "8800:\tlearn: 5.8776997\ttest: 36.8655701\tbest: 36.8639544 (8596)\ttotal: 4m 13s\tremaining: 34.6s\n",
      "8900:\tlearn: 5.7994378\ttest: 36.8667238\tbest: 36.8639544 (8596)\ttotal: 4m 16s\tremaining: 31.7s\n",
      "9000:\tlearn: 5.7310817\ttest: 36.8648677\tbest: 36.8639544 (8596)\ttotal: 4m 19s\tremaining: 28.8s\n",
      "9100:\tlearn: 5.6456126\ttest: 36.8626282\tbest: 36.8621129 (9094)\ttotal: 4m 22s\tremaining: 26s\n",
      "9200:\tlearn: 5.5626598\ttest: 36.8615372\tbest: 36.8612819 (9198)\ttotal: 4m 25s\tremaining: 23.1s\n",
      "9300:\tlearn: 5.4947242\ttest: 36.8593270\tbest: 36.8589712 (9286)\ttotal: 4m 28s\tremaining: 20.2s\n",
      "9400:\tlearn: 5.4272620\ttest: 36.8617672\tbest: 36.8589104 (9356)\ttotal: 4m 31s\tremaining: 17.3s\n",
      "9500:\tlearn: 5.3427433\ttest: 36.8607005\tbest: 36.8589104 (9356)\ttotal: 4m 34s\tremaining: 14.4s\n",
      "9600:\tlearn: 5.2713905\ttest: 36.8625673\tbest: 36.8589104 (9356)\ttotal: 4m 37s\tremaining: 11.5s\n",
      "9700:\tlearn: 5.2008701\ttest: 36.8639384\tbest: 36.8589104 (9356)\ttotal: 4m 40s\tremaining: 8.65s\n",
      "9800:\tlearn: 5.1340905\ttest: 36.8649672\tbest: 36.8589104 (9356)\ttotal: 4m 43s\tremaining: 5.76s\n",
      "Stopped by overfitting detector  (500 iterations wait)\n",
      "\n",
      "bestTest = 36.85891042\n",
      "bestIteration = 9356\n",
      "\n",
      "Shrink model to first 9357 iterations.\n",
      "[0]\tvalidation_0-rmse:60.77724\tvalidation_0-root_mean_squared_error:60.77724\n",
      "[100]\tvalidation_0-rmse:46.91135\tvalidation_0-root_mean_squared_error:46.91135\n",
      "[200]\tvalidation_0-rmse:43.12649\tvalidation_0-root_mean_squared_error:43.12649\n",
      "[300]\tvalidation_0-rmse:41.58477\tvalidation_0-root_mean_squared_error:41.58477\n",
      "[400]\tvalidation_0-rmse:40.78114\tvalidation_0-root_mean_squared_error:40.78114\n",
      "[500]\tvalidation_0-rmse:40.31230\tvalidation_0-root_mean_squared_error:40.31230\n",
      "[600]\tvalidation_0-rmse:40.00877\tvalidation_0-root_mean_squared_error:40.00877\n",
      "[700]\tvalidation_0-rmse:39.77871\tvalidation_0-root_mean_squared_error:39.77871\n",
      "[800]\tvalidation_0-rmse:39.64332\tvalidation_0-root_mean_squared_error:39.64331\n",
      "[900]\tvalidation_0-rmse:39.57374\tvalidation_0-root_mean_squared_error:39.57374\n",
      "[1000]\tvalidation_0-rmse:39.50706\tvalidation_0-root_mean_squared_error:39.50706\n",
      "[1100]\tvalidation_0-rmse:39.45727\tvalidation_0-root_mean_squared_error:39.45727\n",
      "[1200]\tvalidation_0-rmse:39.41307\tvalidation_0-root_mean_squared_error:39.41307\n",
      "[1300]\tvalidation_0-rmse:39.36814\tvalidation_0-root_mean_squared_error:39.36814\n",
      "[1400]\tvalidation_0-rmse:39.33533\tvalidation_0-root_mean_squared_error:39.33533\n",
      "[1500]\tvalidation_0-rmse:39.28987\tvalidation_0-root_mean_squared_error:39.28988\n",
      "[1600]\tvalidation_0-rmse:39.17589\tvalidation_0-root_mean_squared_error:39.17589\n",
      "[1700]\tvalidation_0-rmse:39.06637\tvalidation_0-root_mean_squared_error:39.06637\n",
      "[1800]\tvalidation_0-rmse:38.99101\tvalidation_0-root_mean_squared_error:38.99101\n",
      "[1900]\tvalidation_0-rmse:38.94468\tvalidation_0-root_mean_squared_error:38.94467\n",
      "[2000]\tvalidation_0-rmse:38.88892\tvalidation_0-root_mean_squared_error:38.88892\n",
      "[2100]\tvalidation_0-rmse:38.82742\tvalidation_0-root_mean_squared_error:38.82742\n",
      "[2200]\tvalidation_0-rmse:38.77338\tvalidation_0-root_mean_squared_error:38.77338\n",
      "[2300]\tvalidation_0-rmse:38.71348\tvalidation_0-root_mean_squared_error:38.71348\n",
      "[2400]\tvalidation_0-rmse:38.65210\tvalidation_0-root_mean_squared_error:38.65210\n",
      "[2500]\tvalidation_0-rmse:38.60781\tvalidation_0-root_mean_squared_error:38.60782\n",
      "[2600]\tvalidation_0-rmse:38.55549\tvalidation_0-root_mean_squared_error:38.55549\n",
      "[2700]\tvalidation_0-rmse:38.50320\tvalidation_0-root_mean_squared_error:38.50320\n",
      "[2800]\tvalidation_0-rmse:38.46236\tvalidation_0-root_mean_squared_error:38.46236\n",
      "[2900]\tvalidation_0-rmse:38.41919\tvalidation_0-root_mean_squared_error:38.41919\n",
      "[3000]\tvalidation_0-rmse:38.37975\tvalidation_0-root_mean_squared_error:38.37974\n",
      "[3100]\tvalidation_0-rmse:38.34970\tvalidation_0-root_mean_squared_error:38.34970\n",
      "[3200]\tvalidation_0-rmse:38.32056\tvalidation_0-root_mean_squared_error:38.32056\n",
      "[3300]\tvalidation_0-rmse:38.28456\tvalidation_0-root_mean_squared_error:38.28456\n",
      "[3400]\tvalidation_0-rmse:38.25250\tvalidation_0-root_mean_squared_error:38.25250\n",
      "[3500]\tvalidation_0-rmse:38.23315\tvalidation_0-root_mean_squared_error:38.23315\n",
      "[3600]\tvalidation_0-rmse:38.19881\tvalidation_0-root_mean_squared_error:38.19881\n",
      "[3700]\tvalidation_0-rmse:38.17198\tvalidation_0-root_mean_squared_error:38.17198\n",
      "[3800]\tvalidation_0-rmse:38.14627\tvalidation_0-root_mean_squared_error:38.14627\n",
      "[3900]\tvalidation_0-rmse:38.12230\tvalidation_0-root_mean_squared_error:38.12230\n",
      "[4000]\tvalidation_0-rmse:38.09453\tvalidation_0-root_mean_squared_error:38.09453\n",
      "[4100]\tvalidation_0-rmse:38.07228\tvalidation_0-root_mean_squared_error:38.07228\n",
      "[4200]\tvalidation_0-rmse:38.04464\tvalidation_0-root_mean_squared_error:38.04464\n",
      "[4300]\tvalidation_0-rmse:38.02942\tvalidation_0-root_mean_squared_error:38.02942\n",
      "[4400]\tvalidation_0-rmse:38.01285\tvalidation_0-root_mean_squared_error:38.01285\n",
      "[4500]\tvalidation_0-rmse:37.99529\tvalidation_0-root_mean_squared_error:37.99529\n",
      "[4600]\tvalidation_0-rmse:37.96554\tvalidation_0-root_mean_squared_error:37.96554\n",
      "[4700]\tvalidation_0-rmse:37.93464\tvalidation_0-root_mean_squared_error:37.93464\n",
      "[4800]\tvalidation_0-rmse:37.91160\tvalidation_0-root_mean_squared_error:37.91160\n",
      "[4900]\tvalidation_0-rmse:37.89456\tvalidation_0-root_mean_squared_error:37.89456\n",
      "[5000]\tvalidation_0-rmse:37.84931\tvalidation_0-root_mean_squared_error:37.84932\n",
      "[5100]\tvalidation_0-rmse:37.82347\tvalidation_0-root_mean_squared_error:37.82347\n",
      "[5200]\tvalidation_0-rmse:37.80177\tvalidation_0-root_mean_squared_error:37.80177\n",
      "[5300]\tvalidation_0-rmse:37.78259\tvalidation_0-root_mean_squared_error:37.78259\n",
      "[5400]\tvalidation_0-rmse:37.75300\tvalidation_0-root_mean_squared_error:37.75300\n",
      "[5500]\tvalidation_0-rmse:37.72406\tvalidation_0-root_mean_squared_error:37.72406\n",
      "[5600]\tvalidation_0-rmse:37.69416\tvalidation_0-root_mean_squared_error:37.69416\n",
      "[5700]\tvalidation_0-rmse:37.65067\tvalidation_0-root_mean_squared_error:37.65067\n",
      "[5800]\tvalidation_0-rmse:37.62557\tvalidation_0-root_mean_squared_error:37.62557\n",
      "[5900]\tvalidation_0-rmse:37.59761\tvalidation_0-root_mean_squared_error:37.59761\n",
      "[6000]\tvalidation_0-rmse:37.56615\tvalidation_0-root_mean_squared_error:37.56615\n",
      "[6100]\tvalidation_0-rmse:37.53846\tvalidation_0-root_mean_squared_error:37.53846\n",
      "[6200]\tvalidation_0-rmse:37.51041\tvalidation_0-root_mean_squared_error:37.51041\n",
      "[6300]\tvalidation_0-rmse:37.49707\tvalidation_0-root_mean_squared_error:37.49707\n",
      "[6400]\tvalidation_0-rmse:37.48505\tvalidation_0-root_mean_squared_error:37.48505\n",
      "[6500]\tvalidation_0-rmse:37.46239\tvalidation_0-root_mean_squared_error:37.46239\n",
      "[6600]\tvalidation_0-rmse:37.44707\tvalidation_0-root_mean_squared_error:37.44707\n",
      "[6700]\tvalidation_0-rmse:37.43931\tvalidation_0-root_mean_squared_error:37.43931\n",
      "[6800]\tvalidation_0-rmse:37.44375\tvalidation_0-root_mean_squared_error:37.44375\n",
      "[6900]\tvalidation_0-rmse:37.44573\tvalidation_0-root_mean_squared_error:37.44572\n",
      "[7000]\tvalidation_0-rmse:37.43661\tvalidation_0-root_mean_squared_error:37.43661\n",
      "[7100]\tvalidation_0-rmse:37.43329\tvalidation_0-root_mean_squared_error:37.43329\n",
      "[7200]\tvalidation_0-rmse:37.43638\tvalidation_0-root_mean_squared_error:37.43638\n",
      "[7300]\tvalidation_0-rmse:37.41753\tvalidation_0-root_mean_squared_error:37.41753\n",
      "[7400]\tvalidation_0-rmse:37.40752\tvalidation_0-root_mean_squared_error:37.40752\n",
      "[7500]\tvalidation_0-rmse:37.39805\tvalidation_0-root_mean_squared_error:37.39805\n",
      "[7600]\tvalidation_0-rmse:37.39073\tvalidation_0-root_mean_squared_error:37.39073\n",
      "[7700]\tvalidation_0-rmse:37.39709\tvalidation_0-root_mean_squared_error:37.39709\n",
      "[7800]\tvalidation_0-rmse:37.39815\tvalidation_0-root_mean_squared_error:37.39815\n",
      "[7900]\tvalidation_0-rmse:37.37633\tvalidation_0-root_mean_squared_error:37.37633\n",
      "[8000]\tvalidation_0-rmse:37.36489\tvalidation_0-root_mean_squared_error:37.36489\n",
      "[8100]\tvalidation_0-rmse:37.35361\tvalidation_0-root_mean_squared_error:37.35361\n",
      "[8200]\tvalidation_0-rmse:37.34336\tvalidation_0-root_mean_squared_error:37.34336\n",
      "[8300]\tvalidation_0-rmse:37.34134\tvalidation_0-root_mean_squared_error:37.34134\n",
      "[8400]\tvalidation_0-rmse:37.33822\tvalidation_0-root_mean_squared_error:37.33822\n",
      "[8500]\tvalidation_0-rmse:37.33669\tvalidation_0-root_mean_squared_error:37.33669\n",
      "[8600]\tvalidation_0-rmse:37.33689\tvalidation_0-root_mean_squared_error:37.33689\n",
      "[8700]\tvalidation_0-rmse:37.33009\tvalidation_0-root_mean_squared_error:37.33009\n",
      "[8800]\tvalidation_0-rmse:37.31816\tvalidation_0-root_mean_squared_error:37.31816\n",
      "[8900]\tvalidation_0-rmse:37.31057\tvalidation_0-root_mean_squared_error:37.31057\n",
      "[9000]\tvalidation_0-rmse:37.30245\tvalidation_0-root_mean_squared_error:37.30245\n",
      "[9100]\tvalidation_0-rmse:37.30767\tvalidation_0-root_mean_squared_error:37.30767\n",
      "[9200]\tvalidation_0-rmse:37.30583\tvalidation_0-root_mean_squared_error:37.30583\n",
      "[9300]\tvalidation_0-rmse:37.30573\tvalidation_0-root_mean_squared_error:37.30573\n",
      "[9400]\tvalidation_0-rmse:37.30139\tvalidation_0-root_mean_squared_error:37.30139\n",
      "[9500]\tvalidation_0-rmse:37.31078\tvalidation_0-root_mean_squared_error:37.31078\n",
      "[9600]\tvalidation_0-rmse:37.31222\tvalidation_0-root_mean_squared_error:37.31222\n",
      "[9700]\tvalidation_0-rmse:37.31495\tvalidation_0-root_mean_squared_error:37.31495\n",
      "[9800]\tvalidation_0-rmse:37.31126\tvalidation_0-root_mean_squared_error:37.31126\n",
      "[9872]\tvalidation_0-rmse:37.31308\tvalidation_0-root_mean_squared_error:37.31308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [25:18, 397.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest rmse:  37.18997965804012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_943508/2235415930.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_X.drop(columns=['uid'], inplace=True)\n",
      "/tmp/ipykernel_943508/2235415930.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  val_X.drop(columns=['uid'], inplace=True)\n",
      "/tmp/ipykernel_943508/2235415930.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_X_processed.drop(columns=['uid'], inplace=True)\n",
      "/tmp/ipykernel_943508/2235415930.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  val_X_processed.drop(columns=['uid'], inplace=True)\n",
      "/u/spa-d4/grad/mfe261/Projects/PREPARE/envs/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 26922.07321| val_0_rmse: 145.06559|  0:00:00s\n",
      "epoch 1  | loss: 20759.9188| val_0_rmse: 124.02239|  0:00:01s\n",
      "epoch 2  | loss: 9701.48501| val_0_rmse: 72.50898|  0:00:01s\n",
      "epoch 3  | loss: 3161.59002| val_0_rmse: 63.76739|  0:00:02s\n",
      "epoch 4  | loss: 2679.16259| val_0_rmse: 59.13793|  0:00:03s\n",
      "epoch 5  | loss: 2364.08683| val_0_rmse: 56.28936|  0:00:04s\n",
      "epoch 6  | loss: 2205.73842| val_0_rmse: 50.99976|  0:00:04s\n",
      "epoch 7  | loss: 2063.44127| val_0_rmse: 52.59684|  0:00:05s\n",
      "epoch 8  | loss: 1953.22035| val_0_rmse: 57.45335|  0:00:06s\n",
      "epoch 9  | loss: 1955.15904| val_0_rmse: 56.17068|  0:00:06s\n",
      "epoch 10 | loss: 1881.86737| val_0_rmse: 54.37781|  0:00:07s\n",
      "epoch 11 | loss: 1813.6242| val_0_rmse: 54.42677|  0:00:08s\n",
      "epoch 12 | loss: 1769.41052| val_0_rmse: 50.66613|  0:00:08s\n",
      "epoch 13 | loss: 1723.11006| val_0_rmse: 46.5792 |  0:00:09s\n",
      "epoch 14 | loss: 1700.39319| val_0_rmse: 48.07808|  0:00:09s\n",
      "epoch 15 | loss: 1660.17491| val_0_rmse: 47.26999|  0:00:10s\n",
      "epoch 16 | loss: 1654.68082| val_0_rmse: 46.89767|  0:00:11s\n",
      "epoch 17 | loss: 1600.98456| val_0_rmse: 47.43523|  0:00:11s\n",
      "epoch 18 | loss: 1590.01332| val_0_rmse: 45.10958|  0:00:12s\n",
      "epoch 19 | loss: 1547.35456| val_0_rmse: 44.90835|  0:00:13s\n",
      "epoch 20 | loss: 1485.83877| val_0_rmse: 44.00821|  0:00:13s\n",
      "epoch 21 | loss: 1462.12774| val_0_rmse: 44.06763|  0:00:14s\n",
      "epoch 22 | loss: 1477.61493| val_0_rmse: 45.20002|  0:00:14s\n",
      "epoch 23 | loss: 1411.07323| val_0_rmse: 44.10928|  0:00:15s\n",
      "epoch 24 | loss: 1460.45035| val_0_rmse: 44.19811|  0:00:16s\n",
      "epoch 25 | loss: 1407.90386| val_0_rmse: 44.6684 |  0:00:16s\n",
      "epoch 26 | loss: 1339.47788| val_0_rmse: 43.49638|  0:00:17s\n",
      "epoch 27 | loss: 1354.24044| val_0_rmse: 43.74519|  0:00:17s\n",
      "epoch 28 | loss: 1301.90773| val_0_rmse: 44.12063|  0:00:18s\n",
      "epoch 29 | loss: 1282.1218| val_0_rmse: 44.36167|  0:00:19s\n",
      "epoch 30 | loss: 1279.93983| val_0_rmse: 43.82711|  0:00:19s\n",
      "epoch 31 | loss: 1237.55837| val_0_rmse: 44.39411|  0:00:20s\n",
      "epoch 32 | loss: 1240.82849| val_0_rmse: 44.37014|  0:00:21s\n",
      "epoch 33 | loss: 1247.82686| val_0_rmse: 44.17279|  0:00:21s\n",
      "epoch 34 | loss: 1206.40014| val_0_rmse: 44.98526|  0:00:22s\n",
      "epoch 35 | loss: 1196.07189| val_0_rmse: 44.4103 |  0:00:22s\n",
      "epoch 36 | loss: 1171.56523| val_0_rmse: 44.27438|  0:00:23s\n",
      "epoch 37 | loss: 1117.14931| val_0_rmse: 44.85954|  0:00:24s\n",
      "epoch 38 | loss: 1180.57776| val_0_rmse: 44.12733|  0:00:24s\n",
      "epoch 39 | loss: 1154.81089| val_0_rmse: 44.39542|  0:00:25s\n",
      "epoch 40 | loss: 1081.92249| val_0_rmse: 44.8978 |  0:00:25s\n",
      "epoch 41 | loss: 1101.26573| val_0_rmse: 44.94403|  0:00:26s\n",
      "epoch 42 | loss: 1062.74491| val_0_rmse: 44.84758|  0:00:27s\n",
      "epoch 43 | loss: 1067.08 | val_0_rmse: 44.95144|  0:00:27s\n",
      "epoch 44 | loss: 1125.7951| val_0_rmse: 45.46948|  0:00:28s\n",
      "epoch 45 | loss: 1043.08825| val_0_rmse: 46.24458|  0:00:28s\n",
      "epoch 46 | loss: 1054.61317| val_0_rmse: 44.55426|  0:00:29s\n",
      "\n",
      "Early stopping occurred at epoch 46 with best_epoch = 26 and best_val_0_rmse = 43.49638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/spa-d4/grad/mfe261/Projects/PREPARE/envs/lib/python3.9/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabNet rmse:  43.49638381976668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/spa-d4/grad/mfe261/Projects/PREPARE/envs/lib/python3.9/site-packages/lightgbm/engine.py:204: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.224264 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1910\n",
      "[LightGBM] [Info] Number of data points in the train set: 3475, number of used features: 195\n",
      "[LightGBM] [Info] Start training from score 157.063309\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1809]\tvalid_0's rmse: 40.3567\n",
      "0:\tlearn: 60.6020805\ttest: 60.8146220\tbest: 60.8146220 (0)\ttotal: 54.2ms\tremaining: 9m 2s\n",
      "100:\tlearn: 44.7876389\ttest: 47.7821271\tbest: 47.7821271 (100)\ttotal: 2.25s\tremaining: 3m 40s\n",
      "200:\tlearn: 38.4413773\ttest: 43.9840904\tbest: 43.9840904 (200)\ttotal: 4.51s\tremaining: 3m 39s\n",
      "300:\tlearn: 34.9888618\ttest: 42.4981671\tbest: 42.4981671 (300)\ttotal: 6.95s\tremaining: 3m 43s\n",
      "400:\tlearn: 32.6991160\ttest: 41.7931370\tbest: 41.7931370 (400)\ttotal: 9.27s\tremaining: 3m 41s\n",
      "500:\tlearn: 31.1878619\ttest: 41.4408054\tbest: 41.4408054 (500)\ttotal: 11.6s\tremaining: 3m 40s\n",
      "600:\tlearn: 29.8130834\ttest: 41.1529627\tbest: 41.1519197 (599)\ttotal: 14.1s\tremaining: 3m 40s\n",
      "700:\tlearn: 28.5678955\ttest: 40.9410598\tbest: 40.9410598 (700)\ttotal: 16.6s\tremaining: 3m 40s\n",
      "800:\tlearn: 27.5068798\ttest: 40.8145177\tbest: 40.8145177 (800)\ttotal: 19.1s\tremaining: 3m 39s\n",
      "900:\tlearn: 26.4914383\ttest: 40.6691942\tbest: 40.6691942 (900)\ttotal: 21.7s\tremaining: 3m 39s\n",
      "1000:\tlearn: 25.5765152\ttest: 40.5798557\tbest: 40.5788398 (995)\ttotal: 24.4s\tremaining: 3m 38s\n",
      "1100:\tlearn: 24.7858424\ttest: 40.4909331\tbest: 40.4909331 (1100)\ttotal: 27s\tremaining: 3m 38s\n",
      "1200:\tlearn: 24.0217345\ttest: 40.4066948\tbest: 40.4060410 (1199)\ttotal: 29.7s\tremaining: 3m 37s\n",
      "1300:\tlearn: 23.3282575\ttest: 40.3056248\tbest: 40.3056248 (1300)\ttotal: 32.4s\tremaining: 3m 36s\n",
      "1400:\tlearn: 22.6154646\ttest: 40.2040256\tbest: 40.2040256 (1400)\ttotal: 35.2s\tremaining: 3m 35s\n",
      "1500:\tlearn: 21.9878008\ttest: 40.1390745\tbest: 40.1390745 (1500)\ttotal: 38s\tremaining: 3m 34s\n",
      "1600:\tlearn: 21.3563536\ttest: 40.0831464\tbest: 40.0815617 (1595)\ttotal: 40.8s\tremaining: 3m 33s\n",
      "1700:\tlearn: 20.7844982\ttest: 40.0428846\tbest: 40.0428846 (1700)\ttotal: 43.6s\tremaining: 3m 32s\n",
      "1800:\tlearn: 20.2367566\ttest: 40.0092052\tbest: 40.0092052 (1800)\ttotal: 46.4s\tremaining: 3m 31s\n",
      "1900:\tlearn: 19.7420993\ttest: 39.9741033\tbest: 39.9741033 (1900)\ttotal: 49.2s\tremaining: 3m 29s\n",
      "2000:\tlearn: 19.2529603\ttest: 39.9408195\tbest: 39.9408195 (2000)\ttotal: 52.2s\tremaining: 3m 28s\n",
      "2100:\tlearn: 18.7771042\ttest: 39.9147452\tbest: 39.9145079 (2091)\ttotal: 55s\tremaining: 3m 26s\n",
      "2200:\tlearn: 18.3062973\ttest: 39.8881893\tbest: 39.8852027 (2197)\ttotal: 57.9s\tremaining: 3m 25s\n",
      "2300:\tlearn: 17.8701153\ttest: 39.8558860\tbest: 39.8553454 (2299)\ttotal: 1m\tremaining: 3m 23s\n",
      "2400:\tlearn: 17.4196055\ttest: 39.8172192\tbest: 39.8168238 (2398)\ttotal: 1m 3s\tremaining: 3m 20s\n",
      "2500:\tlearn: 17.0548776\ttest: 39.8043725\tbest: 39.7980933 (2478)\ttotal: 1m 6s\tremaining: 3m 19s\n",
      "2600:\tlearn: 16.6672816\ttest: 39.7962510\tbest: 39.7960354 (2595)\ttotal: 1m 9s\tremaining: 3m 17s\n",
      "2700:\tlearn: 16.2809615\ttest: 39.7818844\tbest: 39.7801440 (2693)\ttotal: 1m 12s\tremaining: 3m 14s\n",
      "2800:\tlearn: 15.9442100\ttest: 39.7606711\tbest: 39.7603760 (2797)\ttotal: 1m 15s\tremaining: 3m 12s\n",
      "2900:\tlearn: 15.6221206\ttest: 39.7549912\tbest: 39.7546523 (2894)\ttotal: 1m 17s\tremaining: 3m 10s\n",
      "3000:\tlearn: 15.3253460\ttest: 39.7592081\tbest: 39.7531690 (2930)\ttotal: 1m 20s\tremaining: 3m 8s\n",
      "3100:\tlearn: 15.0418358\ttest: 39.7446480\tbest: 39.7446480 (3100)\ttotal: 1m 23s\tremaining: 3m 6s\n",
      "3200:\tlearn: 14.7213858\ttest: 39.7307240\tbest: 39.7298655 (3194)\ttotal: 1m 26s\tremaining: 3m 4s\n",
      "3300:\tlearn: 14.4490571\ttest: 39.7247248\tbest: 39.7233333 (3240)\ttotal: 1m 29s\tremaining: 3m 1s\n",
      "3400:\tlearn: 14.1719063\ttest: 39.7241786\tbest: 39.7233333 (3240)\ttotal: 1m 32s\tremaining: 2m 59s\n",
      "3500:\tlearn: 13.8966300\ttest: 39.7230954\tbest: 39.7220905 (3469)\ttotal: 1m 35s\tremaining: 2m 56s\n",
      "3600:\tlearn: 13.5918342\ttest: 39.7127573\tbest: 39.7116190 (3599)\ttotal: 1m 38s\tremaining: 2m 54s\n",
      "3700:\tlearn: 13.3440176\ttest: 39.6939004\tbest: 39.6939004 (3700)\ttotal: 1m 41s\tremaining: 2m 52s\n",
      "3800:\tlearn: 13.0936435\ttest: 39.6823896\tbest: 39.6823896 (3800)\ttotal: 1m 44s\tremaining: 2m 49s\n",
      "3900:\tlearn: 12.8644992\ttest: 39.6746895\tbest: 39.6746895 (3900)\ttotal: 1m 47s\tremaining: 2m 47s\n",
      "4000:\tlearn: 12.6203249\ttest: 39.6744750\tbest: 39.6725264 (3941)\ttotal: 1m 49s\tremaining: 2m 44s\n",
      "4100:\tlearn: 12.4133647\ttest: 39.6622922\tbest: 39.6611813 (4096)\ttotal: 1m 52s\tremaining: 2m 42s\n",
      "4200:\tlearn: 12.2029088\ttest: 39.6590524\tbest: 39.6563277 (4184)\ttotal: 1m 55s\tremaining: 2m 39s\n",
      "4300:\tlearn: 12.0059448\ttest: 39.6526536\tbest: 39.6505469 (4275)\ttotal: 1m 58s\tremaining: 2m 37s\n",
      "4400:\tlearn: 11.7885460\ttest: 39.6552348\tbest: 39.6505469 (4275)\ttotal: 2m 1s\tremaining: 2m 35s\n",
      "4500:\tlearn: 11.6014619\ttest: 39.6542092\tbest: 39.6505469 (4275)\ttotal: 2m 4s\tremaining: 2m 32s\n",
      "4600:\tlearn: 11.3989429\ttest: 39.6461028\tbest: 39.6456500 (4593)\ttotal: 2m 7s\tremaining: 2m 29s\n",
      "4700:\tlearn: 11.2129675\ttest: 39.6494034\tbest: 39.6445683 (4626)\ttotal: 2m 10s\tremaining: 2m 27s\n",
      "4800:\tlearn: 11.0318461\ttest: 39.6427409\tbest: 39.6426023 (4793)\ttotal: 2m 13s\tremaining: 2m 24s\n",
      "4900:\tlearn: 10.8342096\ttest: 39.6436776\tbest: 39.6405618 (4803)\ttotal: 2m 16s\tremaining: 2m 22s\n",
      "5000:\tlearn: 10.6779123\ttest: 39.6457981\tbest: 39.6405618 (4803)\ttotal: 2m 19s\tremaining: 2m 19s\n",
      "5100:\tlearn: 10.5086008\ttest: 39.6446829\tbest: 39.6405618 (4803)\ttotal: 2m 22s\tremaining: 2m 16s\n",
      "5200:\tlearn: 10.3306552\ttest: 39.6453277\tbest: 39.6405618 (4803)\ttotal: 2m 25s\tremaining: 2m 14s\n",
      "5300:\tlearn: 10.1377380\ttest: 39.6412938\tbest: 39.6404302 (5271)\ttotal: 2m 28s\tremaining: 2m 11s\n",
      "5400:\tlearn: 9.9834261\ttest: 39.6363184\tbest: 39.6363184 (5400)\ttotal: 2m 31s\tremaining: 2m 8s\n",
      "5500:\tlearn: 9.8277770\ttest: 39.6387807\tbest: 39.6363184 (5400)\ttotal: 2m 34s\tremaining: 2m 6s\n",
      "5600:\tlearn: 9.6584606\ttest: 39.6362685\tbest: 39.6358662 (5592)\ttotal: 2m 37s\tremaining: 2m 3s\n",
      "5700:\tlearn: 9.5231861\ttest: 39.6357733\tbest: 39.6343127 (5618)\ttotal: 2m 39s\tremaining: 2m\n",
      "5800:\tlearn: 9.3695144\ttest: 39.6288895\tbest: 39.6288895 (5800)\ttotal: 2m 42s\tremaining: 1m 57s\n",
      "5900:\tlearn: 9.2382029\ttest: 39.6238189\tbest: 39.6238189 (5900)\ttotal: 2m 45s\tremaining: 1m 55s\n",
      "6000:\tlearn: 9.0849740\ttest: 39.6207696\tbest: 39.6206550 (5933)\ttotal: 2m 48s\tremaining: 1m 52s\n",
      "6100:\tlearn: 8.9605752\ttest: 39.6231118\tbest: 39.6196165 (6022)\ttotal: 2m 51s\tremaining: 1m 49s\n",
      "6200:\tlearn: 8.8241690\ttest: 39.6218785\tbest: 39.6196165 (6022)\ttotal: 2m 54s\tremaining: 1m 47s\n",
      "6300:\tlearn: 8.7003834\ttest: 39.6222985\tbest: 39.6196165 (6022)\ttotal: 2m 57s\tremaining: 1m 44s\n",
      "6400:\tlearn: 8.5735723\ttest: 39.6244888\tbest: 39.6196165 (6022)\ttotal: 3m\tremaining: 1m 41s\n",
      "6500:\tlearn: 8.4597126\ttest: 39.6250689\tbest: 39.6196165 (6022)\ttotal: 3m 3s\tremaining: 1m 38s\n",
      "Stopped by overfitting detector  (500 iterations wait)\n",
      "\n",
      "bestTest = 39.61961651\n",
      "bestIteration = 6022\n",
      "\n",
      "Shrink model to first 6023 iterations.\n",
      "[0]\tvalidation_0-rmse:60.80225\tvalidation_0-root_mean_squared_error:60.80225\n",
      "[100]\tvalidation_0-rmse:48.19120\tvalidation_0-root_mean_squared_error:48.19120\n",
      "[200]\tvalidation_0-rmse:44.82936\tvalidation_0-root_mean_squared_error:44.82936\n",
      "[300]\tvalidation_0-rmse:43.40490\tvalidation_0-root_mean_squared_error:43.40490\n",
      "[400]\tvalidation_0-rmse:42.74744\tvalidation_0-root_mean_squared_error:42.74744\n",
      "[500]\tvalidation_0-rmse:42.41459\tvalidation_0-root_mean_squared_error:42.41459\n",
      "[600]\tvalidation_0-rmse:42.21728\tvalidation_0-root_mean_squared_error:42.21727\n",
      "[700]\tvalidation_0-rmse:42.11218\tvalidation_0-root_mean_squared_error:42.11218\n",
      "[800]\tvalidation_0-rmse:42.08901\tvalidation_0-root_mean_squared_error:42.08901\n",
      "[900]\tvalidation_0-rmse:42.03635\tvalidation_0-root_mean_squared_error:42.03635\n",
      "[1000]\tvalidation_0-rmse:41.97617\tvalidation_0-root_mean_squared_error:41.97617\n",
      "[1100]\tvalidation_0-rmse:41.95642\tvalidation_0-root_mean_squared_error:41.95641\n",
      "[1200]\tvalidation_0-rmse:41.93374\tvalidation_0-root_mean_squared_error:41.93375\n",
      "[1300]\tvalidation_0-rmse:41.89462\tvalidation_0-root_mean_squared_error:41.89462\n",
      "[1400]\tvalidation_0-rmse:41.87301\tvalidation_0-root_mean_squared_error:41.87301\n",
      "[1500]\tvalidation_0-rmse:41.84167\tvalidation_0-root_mean_squared_error:41.84167\n",
      "[1600]\tvalidation_0-rmse:41.82574\tvalidation_0-root_mean_squared_error:41.82574\n",
      "[1700]\tvalidation_0-rmse:41.79693\tvalidation_0-root_mean_squared_error:41.79694\n",
      "[1800]\tvalidation_0-rmse:41.75140\tvalidation_0-root_mean_squared_error:41.75140\n",
      "[1900]\tvalidation_0-rmse:41.70383\tvalidation_0-root_mean_squared_error:41.70383\n",
      "[2000]\tvalidation_0-rmse:41.65924\tvalidation_0-root_mean_squared_error:41.65924\n",
      "[2100]\tvalidation_0-rmse:41.61516\tvalidation_0-root_mean_squared_error:41.61516\n",
      "[2200]\tvalidation_0-rmse:41.59281\tvalidation_0-root_mean_squared_error:41.59281\n",
      "[2300]\tvalidation_0-rmse:41.55270\tvalidation_0-root_mean_squared_error:41.55270\n",
      "[2400]\tvalidation_0-rmse:41.51079\tvalidation_0-root_mean_squared_error:41.51079\n",
      "[2500]\tvalidation_0-rmse:41.48190\tvalidation_0-root_mean_squared_error:41.48190\n",
      "[2600]\tvalidation_0-rmse:41.46074\tvalidation_0-root_mean_squared_error:41.46075\n",
      "[2700]\tvalidation_0-rmse:41.43582\tvalidation_0-root_mean_squared_error:41.43582\n",
      "[2800]\tvalidation_0-rmse:41.42350\tvalidation_0-root_mean_squared_error:41.42350\n",
      "[2900]\tvalidation_0-rmse:41.42211\tvalidation_0-root_mean_squared_error:41.42211\n",
      "[3000]\tvalidation_0-rmse:41.43085\tvalidation_0-root_mean_squared_error:41.43085\n",
      "[3100]\tvalidation_0-rmse:41.43078\tvalidation_0-root_mean_squared_error:41.43078\n",
      "[3200]\tvalidation_0-rmse:41.41491\tvalidation_0-root_mean_squared_error:41.41491\n",
      "[3300]\tvalidation_0-rmse:41.41324\tvalidation_0-root_mean_squared_error:41.41324\n",
      "[3400]\tvalidation_0-rmse:41.40218\tvalidation_0-root_mean_squared_error:41.40218\n",
      "[3500]\tvalidation_0-rmse:41.39616\tvalidation_0-root_mean_squared_error:41.39616\n",
      "[3600]\tvalidation_0-rmse:41.38490\tvalidation_0-root_mean_squared_error:41.38491\n",
      "[3700]\tvalidation_0-rmse:41.38598\tvalidation_0-root_mean_squared_error:41.38598\n",
      "[3800]\tvalidation_0-rmse:41.39252\tvalidation_0-root_mean_squared_error:41.39252\n",
      "[3900]\tvalidation_0-rmse:41.39685\tvalidation_0-root_mean_squared_error:41.39685\n",
      "[4000]\tvalidation_0-rmse:41.40507\tvalidation_0-root_mean_squared_error:41.40507\n",
      "[4100]\tvalidation_0-rmse:41.40846\tvalidation_0-root_mean_squared_error:41.40845\n",
      "[4110]\tvalidation_0-rmse:41.40576\tvalidation_0-root_mean_squared_error:41.40576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [30:41, 368.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest rmse:  40.68354456865029\n",
      "Average MSE across folds: 36.93934050449245\n",
      "Optimized weights per fold: [array([0.19339243, 0.35164645, 0.12194966, 0.16176169, 0.17124977]), array([0.34754495, 0.22780456, 0.06915199, 0.24460673, 0.11089177]), array([0.        , 0.5267678 , 0.20173902, 0.07152367, 0.19996951]), array([0.38598819, 0.08396596, 0.14886422, 0.22545719, 0.15572469]), array([1.56450200e-01, 6.60906685e-01, 4.69712770e-13, 1.45516450e-01,\n",
      "       3.71266647e-02])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the number of folds\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Placeholder for storing results\n",
    "fold_results = []\n",
    "optimized_weights_list = []\n",
    "unique_uids = merged_df['uid'].unique()  # Extract unique uids\n",
    "\n",
    "for train_index, val_index in tqdm(kf.split(merged_df)):\n",
    "    # Split the data\n",
    "    train_X, val_X = merged_df.iloc[train_index], merged_df.iloc[val_index]\n",
    "    train_y, val_y = y.iloc[train_index], y.iloc[val_index]\n",
    "    train_X.drop(columns=['uid'], inplace=True)\n",
    "    val_X.drop(columns=['uid'], inplace=True)\n",
    "    cats = train_X.select_dtypes(include=['object', 'category']).columns.to_list()\n",
    "    \n",
    "    train_X_processed, val_X_processed = merged_df_processed.iloc[train_index], merged_df_processed.iloc[val_index]\n",
    "    train_X_processed.drop(columns=['uid'], inplace=True)\n",
    "    val_X_processed.drop(columns=['uid'], inplace=True)\n",
    "    \n",
    "    train_X_tabnet, val_X_tabnet = merged_df_tabnet.iloc[train_index], merged_df_tabnet.iloc[val_index]\n",
    "\n",
    "    # TabNet Addition: Train TabNet\n",
    "    model5 =  TabNetRegressor(\n",
    "                seed=42,\n",
    "            )\n",
    "    # TabNet expects numpy arrays\n",
    "    model5.fit(\n",
    "        train_X_tabnet.values, train_y.values.reshape(-1, 1),\n",
    "        eval_set=[(val_X_tabnet.values, val_y.values.reshape(-1, 1))],\n",
    "        max_epochs=100,\n",
    "        patience=20,\n",
    "        batch_size=128,\n",
    "        virtual_batch_size=32,\n",
    "        eval_metric=['rmse']\n",
    "    )\n",
    "    pred5 = model5.predict(val_X_tabnet.values).ravel()\n",
    "    print(\"TabNet rmse: \", root_mean_squared_error(val_y, pred5))\n",
    "    \n",
    "\n",
    "    # Train LightGBM\n",
    "    train_data = lgb.Dataset(train_X, label=train_y, categorical_feature='auto')\n",
    "    val_data = lgb.Dataset(val_X, label=val_y, categorical_feature='auto')\n",
    "\n",
    "    # Define parameters\n",
    "    params = {\n",
    "        'objective': 'regression',  # default for regression\n",
    "        'metric': 'rmse',\n",
    "        'learning_rate': 0.01,\n",
    "        'n_estimators': 10000,\n",
    "        'random_seed': 42\n",
    "    }\n",
    "\n",
    "    # Train the model with early stopping\n",
    "    model1 = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        valid_sets=[val_data],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=500, verbose=True),\n",
    "        ]\n",
    "    )\n",
    "    pred1 = model1.predict(val_X, num_iteration=model1.best_iteration)\n",
    "\n",
    "    # Train CatBoost\n",
    "    model2 = CatBoostRegressor(\n",
    "        iterations=10000, learning_rate=0.01, depth=10, loss_function='RMSE',\n",
    "        cat_features=cats,\n",
    "        verbose=100, early_stopping_rounds=500\n",
    "    )\n",
    "    model2.fit(train_X, train_y, eval_set=(val_X, val_y))\n",
    "    pred2 = model2.predict(val_X)\n",
    "    \n",
    "    # Train XGBoost\n",
    "    model3 = XGBRegressor(n_estimators=10000, learning_rate=0.01,\n",
    "        max_depth=3, random_state=42, \n",
    "        enable_categorical=True,\n",
    "        eval_metric=root_mean_squared_error,\n",
    "        early_stopping_rounds=500)\n",
    "    model3.fit(train_X, train_y, eval_set=[(val_X, val_y)], verbose=100)\n",
    "    pred3 = model3.predict(val_X)\n",
    "    \n",
    "    # Train Random Forest\n",
    "    model4 = RandomForestRegressor(n_estimators=1000, random_state=42)\n",
    "    model4.fit(train_X_processed, train_y)\n",
    "    pred4 = model4.predict(val_X_processed)\n",
    "    print(\"RandomForest rmse: \", root_mean_squared_error(val_y, pred4))\n",
    "\n",
    "    # Define loss function for weight optimization\n",
    "    def loss_function(weights):\n",
    "        w1, w2, w3, w4, w5 = weights\n",
    "        combined_predictions = w1 * pred1 + w2 * pred2 + w3 * pred3 + w4 * pred4 + w5 * pred5\n",
    "        mse = np.mean((combined_predictions - val_y) ** 2)\n",
    "        return mse\n",
    "\n",
    "    # Initial weights (now 1/5 each)\n",
    "    initial_weights = [1/5, 1/5, 1/5, 1/5, 1/5]\n",
    "\n",
    "    # Constraints: weights must sum to 1\n",
    "    constraints = {'type': 'eq', 'fun': lambda w: sum(w) - 1}\n",
    "\n",
    "    # Bounds: weights must be between 0 and 1\n",
    "    bounds = [(0, 1)] * 5\n",
    "\n",
    "    # Optimize weights\n",
    "    result = minimize(loss_function, initial_weights, constraints=constraints, bounds=bounds)\n",
    "    optimized_weights = result.x\n",
    "\n",
    "    # Combine predictions using optimized weights\n",
    "    final_predictions = (\n",
    "        optimized_weights[0] * pred1 +\n",
    "        optimized_weights[1] * pred2 +\n",
    "        optimized_weights[2] * pred3 +\n",
    "        optimized_weights[3] * pred4 + \n",
    "        optimized_weights[4] * pred5\n",
    "    )\n",
    "\n",
    "    # Store results\n",
    "    fold_mse = root_mean_squared_error(val_y, final_predictions)  # RMSE\n",
    "    fold_results.append(fold_mse)\n",
    "    optimized_weights_list.append(optimized_weights)\n",
    "\n",
    "# Display results\n",
    "print(f\"Average MSE across folds: {np.mean(fold_results)}\")\n",
    "print(f\"Optimized weights per fold: {optimized_weights_list}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MSE across folds: 40.04082018188872\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average MSE across folds: {np.mean(fold_results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MSE across folds: 36.93934050449245\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average MSE across folds: {np.mean(fold_results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_943508/2509482186.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  merged_df.drop(columns=['uid'], inplace=True)\n",
      "/tmp/ipykernel_943508/2509482186.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  merged_test.drop(columns=['uid'], inplace=True)\n",
      "/tmp/ipykernel_943508/2509482186.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  merged_df_processed.drop(columns=['uid'], inplace=True)\n",
      "/tmp/ipykernel_943508/2509482186.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  merged_test_processed.drop(columns=['uid'], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "merged_df.drop(columns=['uid'], inplace=True)\n",
    "merged_test.drop(columns=['uid'], inplace=True)\n",
    "merged_df_processed.drop(columns=['uid'], inplace=True)\n",
    "merged_test_processed.drop(columns=['uid'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.19339243, 0.35164645, 0.12194966, 0.16176169, 0.17124977]),\n",
       " array([0.34754495, 0.22780456, 0.06915199, 0.24460673, 0.11089177]),\n",
       " array([0.        , 0.5267678 , 0.20173902, 0.07152367, 0.19996951]),\n",
       " array([0.38598819, 0.08396596, 0.14886422, 0.22545719, 0.15572469]),\n",
       " array([1.56450200e-01, 6.60906685e-01, 4.69712770e-13, 1.45516450e-01,\n",
       "        3.71266647e-02])]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimized_weights_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.21667516, 0.37021829, 0.10834098, 0.16977315, 0.13499248])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(optimized_weights_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/spa-d4/grad/mfe261/Projects/PREPARE/envs/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/u/spa-d4/grad/mfe261/Projects/PREPARE/envs/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 26377.85819|  0:00:00s\n",
      "epoch 1  | loss: 17215.63258|  0:00:01s\n",
      "epoch 2  | loss: 4386.34172|  0:00:02s\n",
      "epoch 3  | loss: 2397.21258|  0:00:02s\n",
      "epoch 4  | loss: 2252.42459|  0:00:03s\n",
      "epoch 5  | loss: 2216.85756|  0:00:04s\n",
      "epoch 6  | loss: 2155.13417|  0:00:04s\n",
      "epoch 7  | loss: 2110.09249|  0:00:05s\n",
      "epoch 8  | loss: 2068.50956|  0:00:06s\n",
      "epoch 9  | loss: 1990.38272|  0:00:06s\n",
      "epoch 10 | loss: 1883.04275|  0:00:07s\n",
      "epoch 11 | loss: 1827.89516|  0:00:08s\n",
      "epoch 12 | loss: 1797.80285|  0:00:08s\n",
      "epoch 13 | loss: 1834.34994|  0:00:09s\n",
      "epoch 14 | loss: 1837.47727|  0:00:10s\n",
      "epoch 15 | loss: 1770.52994|  0:00:10s\n",
      "epoch 16 | loss: 1772.50751|  0:00:11s\n",
      "epoch 17 | loss: 1769.15787|  0:00:12s\n",
      "epoch 18 | loss: 1736.00471|  0:00:12s\n",
      "epoch 19 | loss: 1727.82227|  0:00:13s\n",
      "epoch 20 | loss: 1698.94354|  0:00:14s\n",
      "epoch 21 | loss: 1710.66203|  0:00:14s\n",
      "epoch 22 | loss: 1709.96817|  0:00:15s\n",
      "epoch 23 | loss: 1728.67289|  0:00:16s\n",
      "epoch 24 | loss: 1668.71088|  0:00:16s\n",
      "epoch 25 | loss: 1620.36938|  0:00:17s\n",
      "epoch 26 | loss: 1606.08848|  0:00:18s\n",
      "epoch 27 | loss: 1590.07803|  0:00:18s\n",
      "epoch 28 | loss: 1560.21242|  0:00:19s\n",
      "epoch 29 | loss: 1574.05592|  0:00:19s\n",
      "epoch 30 | loss: 1550.71251|  0:00:20s\n",
      "epoch 31 | loss: 1499.41818|  0:00:21s\n",
      "epoch 32 | loss: 1515.29651|  0:00:21s\n",
      "epoch 33 | loss: 1471.72729|  0:00:22s\n",
      "epoch 34 | loss: 1539.53183|  0:00:23s\n",
      "epoch 35 | loss: 1478.7494|  0:00:23s\n",
      "epoch 36 | loss: 1488.88569|  0:00:24s\n",
      "epoch 37 | loss: 1434.50418|  0:00:25s\n",
      "epoch 38 | loss: 1420.19966|  0:00:25s\n",
      "epoch 39 | loss: 1415.93802|  0:00:26s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/spa-d4/grad/mfe261/Projects/PREPARE/envs/lib/python3.9/site-packages/lightgbm/engine.py:204: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001058 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2030\n",
      "[LightGBM] [Info] Number of data points in the train set: 4343, number of used features: 195\n",
      "[LightGBM] [Info] Start training from score 157.016809\n",
      "0:\tlearn: 60.6706359\ttotal: 5.68ms\tremaining: 56.8s\n",
      "100:\tlearn: 45.1469297\ttotal: 2.13s\tremaining: 3m 28s\n",
      "200:\tlearn: 39.1364111\ttotal: 4.58s\tremaining: 3m 43s\n",
      "300:\tlearn: 35.8111622\ttotal: 7.1s\tremaining: 3m 48s\n",
      "400:\tlearn: 33.7047091\ttotal: 9.55s\tremaining: 3m 48s\n",
      "500:\tlearn: 32.1122361\ttotal: 12.1s\tremaining: 3m 48s\n",
      "600:\tlearn: 30.7861504\ttotal: 14.5s\tremaining: 3m 46s\n",
      "700:\tlearn: 29.6481136\ttotal: 17s\tremaining: 3m 45s\n",
      "800:\tlearn: 28.6747853\ttotal: 19.5s\tremaining: 3m 44s\n",
      "900:\tlearn: 27.7920805\ttotal: 22s\tremaining: 3m 42s\n",
      "1000:\tlearn: 26.9744507\ttotal: 24.8s\tremaining: 3m 42s\n",
      "1100:\tlearn: 26.2333420\ttotal: 27.6s\tremaining: 3m 43s\n",
      "1200:\tlearn: 25.5219533\ttotal: 30.6s\tremaining: 3m 44s\n",
      "1300:\tlearn: 24.9012612\ttotal: 33.5s\tremaining: 3m 43s\n",
      "1400:\tlearn: 24.2192386\ttotal: 36.3s\tremaining: 3m 42s\n",
      "1500:\tlearn: 23.5690407\ttotal: 39.1s\tremaining: 3m 41s\n",
      "1600:\tlearn: 23.0208701\ttotal: 42s\tremaining: 3m 40s\n",
      "1700:\tlearn: 22.4183496\ttotal: 44.9s\tremaining: 3m 39s\n",
      "1800:\tlearn: 21.8250388\ttotal: 47.9s\tremaining: 3m 37s\n",
      "1900:\tlearn: 21.2548819\ttotal: 50.8s\tremaining: 3m 36s\n",
      "2000:\tlearn: 20.7353817\ttotal: 53.8s\tremaining: 3m 35s\n",
      "2100:\tlearn: 20.2018202\ttotal: 56.7s\tremaining: 3m 33s\n",
      "2200:\tlearn: 19.7175311\ttotal: 59.6s\tremaining: 3m 31s\n",
      "2300:\tlearn: 19.2933891\ttotal: 1m 2s\tremaining: 3m 29s\n",
      "2400:\tlearn: 18.8834159\ttotal: 1m 5s\tremaining: 3m 27s\n",
      "2500:\tlearn: 18.4826053\ttotal: 1m 8s\tremaining: 3m 25s\n",
      "2600:\tlearn: 18.1223336\ttotal: 1m 11s\tremaining: 3m 23s\n",
      "2700:\tlearn: 17.8230903\ttotal: 1m 14s\tremaining: 3m 20s\n",
      "2800:\tlearn: 17.4829108\ttotal: 1m 17s\tremaining: 3m 18s\n",
      "2900:\tlearn: 17.1565442\ttotal: 1m 20s\tremaining: 3m 16s\n",
      "3000:\tlearn: 16.8167499\ttotal: 1m 23s\tremaining: 3m 14s\n",
      "3100:\tlearn: 16.4934565\ttotal: 1m 26s\tremaining: 3m 11s\n",
      "3200:\tlearn: 16.1946755\ttotal: 1m 29s\tremaining: 3m 9s\n",
      "3300:\tlearn: 15.8919306\ttotal: 1m 32s\tremaining: 3m 6s\n",
      "3400:\tlearn: 15.5935752\ttotal: 1m 35s\tremaining: 3m 4s\n",
      "3500:\tlearn: 15.3398997\ttotal: 1m 37s\tremaining: 3m 1s\n",
      "3600:\tlearn: 15.0667628\ttotal: 1m 40s\tremaining: 2m 59s\n",
      "3700:\tlearn: 14.8179294\ttotal: 1m 43s\tremaining: 2m 56s\n",
      "3800:\tlearn: 14.5927804\ttotal: 1m 46s\tremaining: 2m 54s\n",
      "3900:\tlearn: 14.3565150\ttotal: 1m 49s\tremaining: 2m 51s\n",
      "4000:\tlearn: 14.1295224\ttotal: 1m 52s\tremaining: 2m 49s\n",
      "4100:\tlearn: 13.8956108\ttotal: 1m 55s\tremaining: 2m 46s\n",
      "4200:\tlearn: 13.6791863\ttotal: 1m 58s\tremaining: 2m 44s\n",
      "4300:\tlearn: 13.4319041\ttotal: 2m 1s\tremaining: 2m 41s\n",
      "4400:\tlearn: 13.2479084\ttotal: 2m 5s\tremaining: 2m 39s\n",
      "4500:\tlearn: 13.0314234\ttotal: 2m 8s\tremaining: 2m 36s\n",
      "4600:\tlearn: 12.8403881\ttotal: 2m 11s\tremaining: 2m 33s\n",
      "4700:\tlearn: 12.6260784\ttotal: 2m 14s\tremaining: 2m 31s\n",
      "4800:\tlearn: 12.4249301\ttotal: 2m 17s\tremaining: 2m 28s\n",
      "4900:\tlearn: 12.1974478\ttotal: 2m 20s\tremaining: 2m 25s\n",
      "5000:\tlearn: 11.9820822\ttotal: 2m 23s\tremaining: 2m 23s\n",
      "5100:\tlearn: 11.7708254\ttotal: 2m 26s\tremaining: 2m 20s\n",
      "5200:\tlearn: 11.5620095\ttotal: 2m 29s\tremaining: 2m 17s\n",
      "5300:\tlearn: 11.3949467\ttotal: 2m 32s\tremaining: 2m 15s\n",
      "5400:\tlearn: 11.2067626\ttotal: 2m 35s\tremaining: 2m 12s\n",
      "5500:\tlearn: 11.0063516\ttotal: 2m 38s\tremaining: 2m 9s\n",
      "5600:\tlearn: 10.8343975\ttotal: 2m 41s\tremaining: 2m 6s\n",
      "5700:\tlearn: 10.6738791\ttotal: 2m 44s\tremaining: 2m 4s\n",
      "5800:\tlearn: 10.5216598\ttotal: 2m 47s\tremaining: 2m 1s\n",
      "5900:\tlearn: 10.3404130\ttotal: 2m 50s\tremaining: 1m 58s\n",
      "6000:\tlearn: 10.1929364\ttotal: 2m 53s\tremaining: 1m 55s\n",
      "6100:\tlearn: 10.0316053\ttotal: 2m 56s\tremaining: 1m 52s\n",
      "6200:\tlearn: 9.8862555\ttotal: 2m 59s\tremaining: 1m 50s\n",
      "6300:\tlearn: 9.7282211\ttotal: 3m 2s\tremaining: 1m 47s\n",
      "6400:\tlearn: 9.5857856\ttotal: 3m 5s\tremaining: 1m 44s\n",
      "6500:\tlearn: 9.4332055\ttotal: 3m 8s\tremaining: 1m 41s\n",
      "6600:\tlearn: 9.3185877\ttotal: 3m 11s\tremaining: 1m 38s\n",
      "6700:\tlearn: 9.1995161\ttotal: 3m 14s\tremaining: 1m 35s\n",
      "6800:\tlearn: 9.0612500\ttotal: 3m 17s\tremaining: 1m 33s\n",
      "6900:\tlearn: 8.9414655\ttotal: 3m 20s\tremaining: 1m 30s\n",
      "7000:\tlearn: 8.8002136\ttotal: 3m 23s\tremaining: 1m 27s\n",
      "7100:\tlearn: 8.6881536\ttotal: 3m 26s\tremaining: 1m 24s\n",
      "7200:\tlearn: 8.5860575\ttotal: 3m 30s\tremaining: 1m 21s\n",
      "7300:\tlearn: 8.4729600\ttotal: 3m 33s\tremaining: 1m 18s\n",
      "7400:\tlearn: 8.3443196\ttotal: 3m 36s\tremaining: 1m 15s\n",
      "7500:\tlearn: 8.2262028\ttotal: 3m 39s\tremaining: 1m 13s\n",
      "7600:\tlearn: 8.1275808\ttotal: 3m 42s\tremaining: 1m 10s\n",
      "7700:\tlearn: 8.0103429\ttotal: 3m 45s\tremaining: 1m 7s\n",
      "7800:\tlearn: 7.9118349\ttotal: 3m 48s\tremaining: 1m 4s\n",
      "7900:\tlearn: 7.8076070\ttotal: 3m 51s\tremaining: 1m 1s\n",
      "8000:\tlearn: 7.7033755\ttotal: 3m 54s\tremaining: 58.6s\n",
      "8100:\tlearn: 7.6126451\ttotal: 3m 57s\tremaining: 55.7s\n",
      "8200:\tlearn: 7.5105898\ttotal: 4m\tremaining: 52.8s\n",
      "8300:\tlearn: 7.4036451\ttotal: 4m 3s\tremaining: 49.9s\n",
      "8400:\tlearn: 7.3147367\ttotal: 4m 6s\tremaining: 46.9s\n",
      "8500:\tlearn: 7.2267071\ttotal: 4m 9s\tremaining: 44s\n",
      "8600:\tlearn: 7.1303000\ttotal: 4m 12s\tremaining: 41.1s\n",
      "8700:\tlearn: 7.0332901\ttotal: 4m 15s\tremaining: 38.2s\n",
      "8800:\tlearn: 6.9415078\ttotal: 4m 18s\tremaining: 35.3s\n",
      "8900:\tlearn: 6.8573814\ttotal: 4m 21s\tremaining: 32.3s\n",
      "9000:\tlearn: 6.7682558\ttotal: 4m 24s\tremaining: 29.4s\n",
      "9100:\tlearn: 6.6880863\ttotal: 4m 28s\tremaining: 26.5s\n",
      "9200:\tlearn: 6.5966540\ttotal: 4m 31s\tremaining: 23.5s\n",
      "9300:\tlearn: 6.5259766\ttotal: 4m 33s\tremaining: 20.6s\n",
      "9400:\tlearn: 6.4256777\ttotal: 4m 36s\tremaining: 17.6s\n",
      "9500:\tlearn: 6.3494734\ttotal: 4m 39s\tremaining: 14.7s\n",
      "9600:\tlearn: 6.2831951\ttotal: 4m 42s\tremaining: 11.8s\n",
      "9700:\tlearn: 6.1957445\ttotal: 4m 45s\tremaining: 8.81s\n",
      "9800:\tlearn: 6.1135583\ttotal: 4m 49s\tremaining: 5.87s\n",
      "9900:\tlearn: 6.0433805\ttotal: 4m 52s\tremaining: 2.92s\n",
      "9999:\tlearn: 5.9743884\ttotal: 4m 55s\tremaining: 0us\n",
      "Final blended predictions for the test dataset:\n",
      "[188 206 207 ... 197 178 153]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average weights from cross-validation\n",
    "average_weights = np.mean(optimized_weights_list, axis=0)\n",
    "\n",
    "\n",
    "# TabNet Addition: Train TabNet\n",
    "final_model5 =  TabNetRegressor(\n",
    "            seed=42\n",
    "        )\n",
    "\n",
    "final_model5.fit(\n",
    "    merged_df_tabnet.values, y.values.reshape(-1, 1),\n",
    "    max_epochs=40,\n",
    "    batch_size=128,\n",
    "    virtual_batch_size=32,\n",
    "    eval_metric=['rmse']\n",
    ")\n",
    "\n",
    "# Train models on the entire training dataset\n",
    "train_data = lgb.Dataset(merged_df, label=y, categorical_feature='auto')\n",
    "\n",
    "params = {\n",
    "    'objective': 'regression',  # default for regression\n",
    "    'metric': 'rmse',\n",
    "    'learning_rate': 0.01,\n",
    "    'n_estimators': 10000,\n",
    "    'random_seed': 42\n",
    "}\n",
    "final_model1 = lgb.train(\n",
    "    params,\n",
    "    train_data\n",
    ")\n",
    "\n",
    "final_model2 = CatBoostRegressor(\n",
    "    iterations=10000, learning_rate=0.01, depth=10, loss_function='RMSE',\n",
    "    cat_features=merged_df.select_dtypes(include=['object', 'category']).columns.to_list(),\n",
    "    verbose=100\n",
    ")\n",
    "final_model2.fit(merged_df, y)\n",
    "\n",
    "final_model3 = XGBRegressor(n_estimators=10000, learning_rate=0.01,\n",
    "        max_depth=3, random_state=42, \n",
    "        enable_categorical=True,\n",
    "        eval_metric=root_mean_squared_error)\n",
    "final_model3.fit(merged_df, y)\n",
    "\n",
    "# Train Random Forest\n",
    "final_model4 = RandomForestRegressor(n_estimators=1000, random_state=42)\n",
    "final_model4.fit(merged_df_processed, y)\n",
    "\n",
    "# Generate predictions on the test dataset\n",
    "test_pred1 = final_model1.predict(merged_test)\n",
    "test_pred2 = final_model2.predict(merged_test)\n",
    "test_pred3 = final_model3.predict(merged_test)\n",
    "test_pred4 = final_model4.predict(merged_test_processed)\n",
    "test_pred5 = final_model5.predict(merged_test_tabnet.values).ravel()\n",
    "\n",
    "# Combine the predictions using the average weights\n",
    "final_test_predictions = (\n",
    "    average_weights[0] * test_pred1 + \n",
    "    average_weights[1] * test_pred2 + \n",
    "    average_weights[2] * test_pred3 +\n",
    "    average_weights[3] * test_pred4 +\n",
    "    average_weights[4] * test_pred5\n",
    ")\n",
    "\n",
    "# Optionally round predictions if required (e.g., for classification tasks)\n",
    "final_test_predictions = np.round(final_test_predictions).astype(int)\n",
    "\n",
    "# Display final predictions\n",
    "print(\"Final blended predictions for the test dataset:\")\n",
    "print(final_test_predictions)\n",
    "\n",
    "ss['composite_score']=final_test_predictions\n",
    "#generate submission\n",
    "ss.to_csv('dataset/LGBM_and_CatBoost_and_XGBoost_RandomForest_TabNet_engineering_3.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
