{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# load libraries  45.03423447058859 42.58356449833813\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.model_selection import StratifiedGroupKFold,StratifiedShuffleSplit,KFold,train_test_split\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "from scipy.optimize import minimize\n",
    "from tqdm import tqdm\n",
    "from xgboost import XGBRegressor\n",
    "import copy\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data\n",
    "train=pd.read_csv(\"dataset/train_features.csv\")\n",
    "y=pd.read_csv(\"dataset/train_labels.csv\")\n",
    "test=pd.read_csv(\"dataset/test_features.csv\")\n",
    "ss=pd.read_csv(\"dataset/submission_format.csv\")\n",
    "test_ground_truth=pd.read_csv(\"dataset/sdoh_test_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>age_03</th>\n",
       "      <th>urban_03</th>\n",
       "      <th>married_03</th>\n",
       "      <th>n_mar_03</th>\n",
       "      <th>edu_gru_03</th>\n",
       "      <th>n_living_child_03</th>\n",
       "      <th>migration_03</th>\n",
       "      <th>glob_hlth_03</th>\n",
       "      <th>adl_dress_03</th>\n",
       "      <th>...</th>\n",
       "      <th>rrelgimp_12</th>\n",
       "      <th>rrfcntx_m_12</th>\n",
       "      <th>rsocact_m_12</th>\n",
       "      <th>rrelgwk_12</th>\n",
       "      <th>a16a_12</th>\n",
       "      <th>a21_12</th>\n",
       "      <th>a22_12</th>\n",
       "      <th>a33b_12</th>\n",
       "      <th>a34_12</th>\n",
       "      <th>j11_12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aace</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.somewhat important</td>\n",
       "      <td>9.Never</td>\n",
       "      <td>9.Never</td>\n",
       "      <td>0.No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Concrete 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aanz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.very important</td>\n",
       "      <td>9.Never</td>\n",
       "      <td>1.Almost every day</td>\n",
       "      <td>0.No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Concrete 2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 184 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    uid age_03 urban_03 married_03  n_mar_03 edu_gru_03 n_living_child_03  \\\n",
       "0  aace    NaN      NaN        NaN       NaN        NaN               NaN   \n",
       "1  aanz    NaN      NaN        NaN       NaN        NaN               NaN   \n",
       "\n",
       "   migration_03 glob_hlth_03  adl_dress_03  ...           rrelgimp_12  \\\n",
       "0           NaN          NaN           NaN  ...  2.somewhat important   \n",
       "1           NaN          NaN           NaN  ...      1.very important   \n",
       "\n",
       "   rrfcntx_m_12        rsocact_m_12  rrelgwk_12  a16a_12  a21_12  a22_12  \\\n",
       "0       9.Never             9.Never        0.No      NaN     NaN     NaN   \n",
       "1       9.Never  1.Almost every day        0.No      NaN     NaN     NaN   \n",
       "\n",
       "   a33b_12  a34_12      j11_12  \n",
       "0      NaN     NaN  Concrete 2  \n",
       "1      NaN     NaN  Concrete 2  \n",
       "\n",
       "[2 rows x 184 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>age_03</th>\n",
       "      <th>urban_03</th>\n",
       "      <th>married_03</th>\n",
       "      <th>n_mar_03</th>\n",
       "      <th>edu_gru_03</th>\n",
       "      <th>n_living_child_03</th>\n",
       "      <th>migration_03</th>\n",
       "      <th>glob_hlth_03</th>\n",
       "      <th>adl_dress_03</th>\n",
       "      <th>...</th>\n",
       "      <th>rrelgimp_12</th>\n",
       "      <th>rrfcntx_m_12</th>\n",
       "      <th>rsocact_m_12</th>\n",
       "      <th>rrelgwk_12</th>\n",
       "      <th>a16a_12</th>\n",
       "      <th>a21_12</th>\n",
       "      <th>a22_12</th>\n",
       "      <th>a33b_12</th>\n",
       "      <th>a34_12</th>\n",
       "      <th>j11_12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abxu</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wood, mosaic, or other covering 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aeol</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.very important</td>\n",
       "      <td>9.Never</td>\n",
       "      <td>9.Never</td>\n",
       "      <td>1.Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Concrete 2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 184 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    uid age_03 urban_03 married_03  n_mar_03 edu_gru_03 n_living_child_03  \\\n",
       "0  abxu    NaN      NaN        NaN       NaN        NaN               NaN   \n",
       "1  aeol    NaN      NaN        NaN       NaN        NaN               NaN   \n",
       "\n",
       "   migration_03 glob_hlth_03  adl_dress_03  ...       rrelgimp_12  \\\n",
       "0           NaN          NaN           NaN  ...               NaN   \n",
       "1           NaN          NaN           NaN  ...  1.very important   \n",
       "\n",
       "   rrfcntx_m_12  rsocact_m_12  rrelgwk_12  a16a_12  a21_12  a22_12  a33b_12  \\\n",
       "0           NaN           NaN         NaN      NaN     NaN     NaN      NaN   \n",
       "1       9.Never       9.Never       1.Yes      NaN     NaN     NaN      NaN   \n",
       "\n",
       "   a34_12                             j11_12  \n",
       "0     NaN  Wood, mosaic, or other covering 1  \n",
       "1     NaN                         Concrete 2  \n",
       "\n",
       "[2 rows x 184 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3276 entries, 0 to 3275\n",
      "Columns: 184 entries, uid to j11_12\n",
      "dtypes: float64(140), object(44)\n",
      "memory usage: 4.6+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let merge train and label\n",
    "merged_df = pd.merge(train, y, on='uid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       2021\n",
       "1       2021\n",
       "2       2016\n",
       "3       2021\n",
       "4       2021\n",
       "        ... \n",
       "4338    2021\n",
       "4339    2016\n",
       "4340    2021\n",
       "4341    2021\n",
       "4342    2021\n",
       "Name: year, Length: 4343, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df[\"year\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "uid                   0\n",
       "age_03             1456\n",
       "urban_03           1454\n",
       "married_03         1454\n",
       "n_mar_03           1482\n",
       "                   ... \n",
       "a33b_12            4288\n",
       "a34_12             1601\n",
       "j11_12               89\n",
       "year                  0\n",
       "composite_score       0\n",
       "Length: 186, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#so many missing values\n",
    "merged_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets merged test AND sample submission\n",
    "merged_test = pd.merge(test, ss, on='uid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       2016\n",
       "1       2016\n",
       "2       2021\n",
       "3       2016\n",
       "4       2021\n",
       "        ... \n",
       "1100    2016\n",
       "1101    2021\n",
       "1102    2016\n",
       "1103    2021\n",
       "1104    2021\n",
       "Name: year, Length: 1105, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_test[\"year\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: title={'center': 'Target Distribution'}, ylabel='Frequency'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "merged_df['composite_score'].plot(kind='hist',title='Target Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new features by combining 2003 and 2012 scores and numbering ordinal variables\n",
    "def feature_engineering(data):\n",
    "    data['rjob_hrswk_change'] = (data['rjob_hrswk_12'] - data['rjob_hrswk_03']).astype(float)\n",
    "    data['max_work_year']=data[['rjob_end_12','rjob_end_03']].max(axis=1).astype(float)\n",
    "    data['years_since_work']=(data['year']-data['max_work_year']).astype(float)\n",
    "    data['hincome_change']=(data['hincome_12']-data['hincome_03']).astype(float)\n",
    "    data['niadl_change']=(data['n_iadl_12']-data['n_iadl_03']).astype(float)\n",
    "    data['adl_change']=(data['n_adl_12']-data['n_adl_03']).astype(float)\n",
    "    data['depr_change']=(data['n_depr_12']-data['n_depr_03']).astype(float)\n",
    "    data['glob_hlth_03']=data['glob_hlth_03'].replace({'5. Poor':0, '4. Fair':1, '3. Good':2, '2. Very good': 3, '1. Excellent':4}).astype(float)\n",
    "    data['glob_hlth_12']=data['glob_hlth_12'].replace({'5. Poor':0, '4. Fair':1, '3. Good':2, '2. Very good': 3, '1. Excellent':4}).astype(float)\n",
    "    data['glob_hlth_change']=(data['glob_hlth_12']-data['glob_hlth_03']).astype(float)\n",
    "    data['bmi_03']=data['bmi_03'].replace({'1. Underweight': 1, '2. Normal weight': 2, '3. Overweight':3, '4. Obese':4, '5. Morbidly obese':5}).astype(float)\n",
    "    data['bmi_12']=data['bmi_12'].replace({'1. Underweight': 1, '2. Normal weight': 2, '3. Overweight':3, '4. Obese':4, '5. Morbidly obese':5}).astype(float)\n",
    "    data['bmi_change']=(data['bmi_12']-data['bmi_03']).astype(float)\n",
    "    data['employment_03']=data['employment_03'].replace({'1. Currently Working': 'Working', '2. Currently looking for work':'Looking for work', '3. Dedicated to household chores': 'House', '4. Retired, incapacitated, or does not work': 'No work'})\n",
    "    data['employment_12']=data['employment_12'].replace({'1. Currently Working': 'Working', '2. Currently looking for work':'Looking for work', '3. Dedicated to household chores': 'House', '4. Retired, incapacitated, or does not work': 'No work'})\n",
    "    data['memory_12']=data['memory_12'].replace({'5. Poor':0, '4. Fair':1, '3. Good':2, '2. Very good': 3, '1. Excellent':4}).astype(float)\n",
    "    data['edu_gru_03']=data['edu_gru_03'].replace({'0. No education':0,'1. 1–5 years':1, '2. 6 years':2, '3. 7–9 years':3,'4. 10+ years':4}).astype(float)\n",
    "    data['edu_gru_12']=data['edu_gru_12'].replace({'0. No education':0,'1. 1–5 years':1, '2. 6 years':2, '3. 7–9 years':3,'4. 10+ years':4}).astype(float)\n",
    "    data['edu_gru_change']=(data['edu_gru_12']-data['edu_gru_03']).astype(float)\n",
    "    data['illnesses_change']=(data['n_illnesses_12']-data['n_illnesses_03']).astype(float)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cat_cols(data):\n",
    "    # Get the columns with object datatype\n",
    "    cat_columns=[]\n",
    "    dummies=[]\n",
    "    for col in data.columns:\n",
    "        if data[col].dtype=='object' and 'uid' not in col:\n",
    "            cat_columns.append(col)\n",
    "            dummies.append(col)\n",
    "        elif data[col].dtype!='object' and 'uid' not in col and (data[col].max()==1.0):\n",
    "            cat_columns.append(col)\n",
    "            data[col].fillna(0, inplace=True)\n",
    "        else:\n",
    "            continue\n",
    "    return cat_columns, dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_cat_cols(train_data, cat_cols, dummy_cols):\n",
    "    enc = OneHotEncoder()\n",
    "    enc.fit(train_data[dummy_cols])\n",
    "    encoded_train_data=enc.transform(train_data[dummy_cols]).toarray()\n",
    "    feature_names = enc.get_feature_names_out(dummy_cols)\n",
    "    train_data.drop(columns=dummy_cols, inplace=True)\n",
    "    encoded_train_df = pd.DataFrame(encoded_train_data, columns=feature_names)\n",
    "    train_data[feature_names]=encoded_train_df[feature_names]\n",
    "    return train_data, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2994495/2658724754.py:10: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data['glob_hlth_03']=data['glob_hlth_03'].replace({'5. Poor':0, '4. Fair':1, '3. Good':2, '2. Very good': 3, '1. Excellent':4}).astype(float)\n",
      "/tmp/ipykernel_2994495/2658724754.py:11: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data['glob_hlth_12']=data['glob_hlth_12'].replace({'5. Poor':0, '4. Fair':1, '3. Good':2, '2. Very good': 3, '1. Excellent':4}).astype(float)\n",
      "/tmp/ipykernel_2994495/2658724754.py:13: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data['bmi_03']=data['bmi_03'].replace({'1. Underweight': 1, '2. Normal weight': 2, '3. Overweight':3, '4. Obese':4, '5. Morbidly obese':5}).astype(float)\n",
      "/tmp/ipykernel_2994495/2658724754.py:14: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data['bmi_12']=data['bmi_12'].replace({'1. Underweight': 1, '2. Normal weight': 2, '3. Overweight':3, '4. Obese':4, '5. Morbidly obese':5}).astype(float)\n",
      "/tmp/ipykernel_2994495/2658724754.py:18: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data['memory_12']=data['memory_12'].replace({'5. Poor':0, '4. Fair':1, '3. Good':2, '2. Very good': 3, '1. Excellent':4}).astype(float)\n",
      "/tmp/ipykernel_2994495/2658724754.py:19: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data['edu_gru_03']=data['edu_gru_03'].replace({'0. No education':0,'1. 1–5 years':1, '2. 6 years':2, '3. 7–9 years':3,'4. 10+ years':4}).astype(float)\n",
      "/tmp/ipykernel_2994495/2658724754.py:20: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data['edu_gru_12']=data['edu_gru_12'].replace({'0. No education':0,'1. 1–5 years':1, '2. 6 years':2, '3. 7–9 years':3,'4. 10+ years':4}).astype(float)\n",
      "/tmp/ipykernel_2994495/53275523.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[col].fillna(0, inplace=True)\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_2994495/3102208485.py:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data_processed[col].fillna(0, inplace=True)\n",
      "/tmp/ipykernel_2994495/3102208485.py:16: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data_processed[col].fillna(0, inplace=True)\n",
      "/tmp/ipykernel_2994495/3102208485.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data_processed[col].fillna(0, inplace=True)\n",
      "/tmp/ipykernel_2994495/3102208485.py:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data_processed[col].fillna(0, inplace=True)\n",
      "/tmp/ipykernel_2994495/3102208485.py:16: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data_processed[col].fillna(0, inplace=True)\n",
      "/tmp/ipykernel_2994495/3102208485.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data_processed[col].fillna(0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_mar_03 33.83 % missing\n",
      "edu_gru_03 33.55 % missing\n",
      "glob_hlth_03 36.99 % missing\n",
      "n_adl_03 33.5 % missing\n",
      "n_iadl_03 36.99 % missing\n",
      "n_depr_03 37.1 % missing\n",
      "n_illnesses_03 33.31 % missing\n",
      "decis_personal_03 37.17 % missing\n",
      "glob_hlth_12 5.8 % missing\n",
      "n_iadl_12 5.84 % missing\n",
      "n_depr_12 6.52 % missing\n",
      "bmi_12 12.48 % missing\n",
      "memory_12 6.72 % missing\n",
      "rearnings_03 33.37 % missing\n",
      "searnings_03 49.98 % missing\n",
      "hincome_03 34.01 % missing\n",
      "hinc_business_03 32.98 % missing\n",
      "hinc_rent_03 32.98 % missing\n",
      "hinc_assets_03 32.98 % missing\n",
      "hinc_cap_03 32.98 % missing\n",
      "rinc_pension_03 33.37 % missing\n",
      "sinc_pension_03 49.98 % missing\n",
      "searnings_12 35.19 % missing\n",
      "sinc_pension_12 35.19 % missing\n",
      "hincome_change 37.28 % missing\n",
      "niadl_change 41.13 % missing\n",
      "adl_change 36.4 % missing\n",
      "depr_change 41.65 % missing\n",
      "glob_hlth_change 41.1 % missing\n",
      "edu_gru_change 36.09 % missing\n",
      "illnesses_change 36.29 % missing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2994495/3102208485.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data_processed[col].fillna(0, inplace=True)\n",
      "/tmp/ipykernel_2994495/3102208485.py:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data_processed[col].fillna(0, inplace=True)\n",
      "/tmp/ipykernel_2994495/3102208485.py:16: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data_processed[col].fillna(0, inplace=True)\n",
      "/tmp/ipykernel_2994495/3102208485.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data_processed[col].fillna(0, inplace=True)\n",
      "/tmp/ipykernel_2994495/3102208485.py:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data_processed[col].fillna(0, inplace=True)\n",
      "/tmp/ipykernel_2994495/3102208485.py:16: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data_processed[col].fillna(0, inplace=True)\n",
      "/tmp/ipykernel_2994495/3102208485.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data_processed[col].fillna(0, inplace=True)\n",
      "/tmp/ipykernel_2994495/3102208485.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data_processed[col].fillna(0, inplace=True)\n",
      "/tmp/ipykernel_2994495/3102208485.py:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data_processed[col].fillna(0, inplace=True)\n",
      "/tmp/ipykernel_2994495/3102208485.py:16: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data_processed[col].fillna(0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "y=merged_df['composite_score']\n",
    "data_processed=pd.concat((merged_df,merged_test)).reset_index(drop=True).copy()\n",
    "data_processed = feature_engineering(data_processed)\n",
    "cat_cols, dummy_cols = get_cat_cols(data_processed)\n",
    "data_processed, dummy_feature_names=encode_cat_cols(data_processed, cat_cols, dummy_cols)\n",
    "data_processed=data_processed.drop(columns=['composite_score'],axis=1)\n",
    "\n",
    "for col in data_processed.columns: \n",
    "    if round((data_processed[col].isna().sum() /len(data_processed)*100), 2)>50:\n",
    "        data_processed.drop(columns=col, inplace=True)\n",
    "    elif round((data_processed[col].isna().sum() /len(data_processed)*100), 2)>5:\n",
    "        print(col,round((data_processed[col].isna().sum() /len(data_processed)*100), 2), '% missing')\n",
    "        data_processed[col].fillna(0, inplace=True)\n",
    "    else: \n",
    "        data_processed[col].fillna(0, inplace=True)\n",
    "        data_processed[col].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2994495/2658724754.py:10: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data['glob_hlth_03']=data['glob_hlth_03'].replace({'5. Poor':0, '4. Fair':1, '3. Good':2, '2. Very good': 3, '1. Excellent':4}).astype(float)\n",
      "/tmp/ipykernel_2994495/2658724754.py:11: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data['glob_hlth_12']=data['glob_hlth_12'].replace({'5. Poor':0, '4. Fair':1, '3. Good':2, '2. Very good': 3, '1. Excellent':4}).astype(float)\n",
      "/tmp/ipykernel_2994495/2658724754.py:13: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data['bmi_03']=data['bmi_03'].replace({'1. Underweight': 1, '2. Normal weight': 2, '3. Overweight':3, '4. Obese':4, '5. Morbidly obese':5}).astype(float)\n",
      "/tmp/ipykernel_2994495/2658724754.py:14: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data['bmi_12']=data['bmi_12'].replace({'1. Underweight': 1, '2. Normal weight': 2, '3. Overweight':3, '4. Obese':4, '5. Morbidly obese':5}).astype(float)\n",
      "/tmp/ipykernel_2994495/2658724754.py:18: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data['memory_12']=data['memory_12'].replace({'5. Poor':0, '4. Fair':1, '3. Good':2, '2. Very good': 3, '1. Excellent':4}).astype(float)\n",
      "/tmp/ipykernel_2994495/2658724754.py:19: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data['edu_gru_03']=data['edu_gru_03'].replace({'0. No education':0,'1. 1–5 years':1, '2. 6 years':2, '3. 7–9 years':3,'4. 10+ years':4}).astype(float)\n",
      "/tmp/ipykernel_2994495/2658724754.py:20: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data['edu_gru_12']=data['edu_gru_12'].replace({'0. No education':0,'1. 1–5 years':1, '2. 6 years':2, '3. 7–9 years':3,'4. 10+ years':4}).astype(float)\n"
     ]
    }
   ],
   "source": [
    "y=merged_df['composite_score']\n",
    "data=pd.concat((merged_df,merged_test)).reset_index(drop=True).copy()\n",
    "data = feature_engineering(data)\n",
    "data=data.drop(columns=['composite_score'],axis=1)\n",
    "\n",
    "# Get the columns with object datatype\n",
    "object_cols = data.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Convert the object columns to category dtype\n",
    "for col in object_cols:\n",
    "    #data[col] = data[col].astype('category').fillna(\"Missing\")\n",
    "    data[col] = pd.Categorical(data[col].fillna(\"Missing\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate train and test\n",
    "merged_df_processed=data_processed[:len(merged_df)]\n",
    "merged_test_processed=data_processed[len(merged_df):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate train and test\n",
    "merged_df=data[:len(merged_df)]\n",
    "merged_test=data[len(merged_df):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4343, 196)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4343, 349)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df_processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running this code cell takes a long time. I already ran it and saved the best parameters for each model in the next cell\n",
    "\n",
    "\"\"\"# Define an objective function for Optuna for each model\n",
    "def objective_lightgbm(trial, train_X, train_y, val_X, val_y):\n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        \"random_state\": 42,\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 5e-1),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 10, 200),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 20),\n",
    "        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.3, 1.0),\n",
    "        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.3, 1.0),\n",
    "        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-3, 50.0),\n",
    "        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-3, 50.0),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 1, 200),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 5000)\n",
    "    }\n",
    "\n",
    "    train_data = lgb.Dataset(train_X, label=train_y)\n",
    "    val_data = lgb.Dataset(val_X, label=val_y, reference=train_data)\n",
    "\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        valid_sets=[val_data],\n",
    "        num_boost_round=params['n_estimators'],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=500, verbose=False),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    preds = model.predict(val_X, num_iteration=model.best_iteration)\n",
    "    rmse = root_mean_squared_error(val_y, preds)\n",
    "    return rmse\n",
    "\n",
    "def objective_catboost(trial, train_X, train_y, val_X, val_y):\n",
    "    params = {\n",
    "        'depth': trial.suggest_int('depth', 3, 12),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 5e-1),\n",
    "        'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-4, 50.0),\n",
    "        'random_strength': trial.suggest_uniform('random_strength', 0.0, 5.0),\n",
    "        'bagging_temperature': trial.suggest_uniform('bagging_temperature', 0.0, 1.0),\n",
    "        'iterations': trial.suggest_int('iterations', 50, 5000)\n",
    "    }\n",
    "\n",
    "    model = CatBoostRegressor(\n",
    "        **params,\n",
    "        loss_function='RMSE',\n",
    "        cat_features=train_X.select_dtypes(include=['object', 'category']).columns.to_list(),\n",
    "        verbose=0, early_stopping_rounds = 500,\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(train_X, train_y, eval_set=(val_X, val_y))\n",
    "\n",
    "    preds = model.predict(val_X)\n",
    "    rmse = root_mean_squared_error(val_y, preds)\n",
    "    return rmse\n",
    "\n",
    "def objective_xgboost(trial, train_X, train_y, val_X, val_y):\n",
    "    params = {\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 5e-1),\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 20),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 20),\n",
    "        'subsample': trial.suggest_uniform('subsample', 0.4, 1.0),\n",
    "        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.4, 1.0),\n",
    "        'gamma': trial.suggest_uniform('gamma', 0, 10),\n",
    "        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-3, 50.0),\n",
    "        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 50.0),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 5000)\n",
    "    }\n",
    "\n",
    "    model = XGBRegressor(**params,\n",
    "                         enable_categorical=True,\n",
    "                         eval_metric=root_mean_squared_error,\n",
    "                         early_stopping_rounds=500,\n",
    "                         random_state=42)\n",
    "    model.fit(\n",
    "        train_X, train_y,\n",
    "        eval_set=[(val_X, val_y)],\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    preds = model.predict(val_X)\n",
    "    rmse = root_mean_squared_error(val_y, preds)\n",
    "    return rmse\n",
    "\n",
    "def objective_randomforest(trial, train_X, train_y, val_X, val_y):\n",
    "    params = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 50, log=True),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 50),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 50),\n",
    "        'max_features': trial.suggest_uniform('max_features', 0.1, 1.0),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 1000)\n",
    "    }\n",
    "\n",
    "    model = RandomForestRegressor(**params, random_state=42)\n",
    "    model.fit(train_X, train_y)\n",
    "\n",
    "    preds = model.predict(val_X)\n",
    "    rmse = root_mean_squared_error(val_y, preds)\n",
    "    return rmse\n",
    "\n",
    "# Run Optuna for each model and store the best parameters\n",
    "best_params = {}\n",
    "for model_name, objective in zip(\n",
    "    ['lightgbm', 'RandomForest', 'XGBoost', 'CatBoost'],\n",
    "    [objective_lightgbm, objective_randomforest, objective_xgboost, objective_catboost]\n",
    "):\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    if model_name == 'RandomForest':\n",
    "        unique_uids = merged_df_processed['uid'].unique()\n",
    "        train_ids, val_ids = train_test_split(unique_uids, test_size=0.2, random_state=42)\n",
    "        keep_train = merged_df_processed['uid'].isin(train_ids)\n",
    "        keep_val = merged_df_processed['uid'].isin(val_ids)\n",
    "        train_X, val_X = merged_df_processed[keep_train],merged_df_processed[keep_val]\n",
    "        train_X.drop(columns=['uid'], inplace=True)\n",
    "        val_X.drop(columns=['uid'], inplace=True)\n",
    "        train_y, val_y= y[keep_train], y[keep_val]\n",
    "        # train_X, val_X, train_y, val_y = train_test_split(merged_df_processed, y, test_size=0.2, random_state=42)\n",
    "    else:\n",
    "        unique_uids = merged_df['uid'].unique()\n",
    "        train_ids, val_ids = train_test_split(unique_uids, test_size=0.2, random_state=42)\n",
    "        keep_train = merged_df['uid'].isin(train_ids)\n",
    "        keep_val = merged_df['uid'].isin(val_ids)\n",
    "        train_X, val_X = merged_df[keep_train],merged_df[keep_val]\n",
    "        train_X.drop(columns=['uid'], inplace=True)\n",
    "        val_X.drop(columns=['uid'], inplace=True)\n",
    "        train_y, val_y= y[keep_train], y[keep_val]\n",
    "        # train_X, val_X, train_y, val_y = train_test_split(merged_df, y, test_size=0.2, random_state=42)\n",
    "    study.optimize(lambda trial: objective(trial, train_X, train_y, val_X, val_y), n_trials=100, n_jobs=-1, show_progress_bar=True)\n",
    "    best_params[model_name] = study.best_params\n",
    "    print(f\"Best params for {model_name}: {study.best_params}\")\n",
    "    print(f\"Best RMSE for {model_name}: {study.best_value}\")\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lightgbm': {'learning_rate': 0.07089438962267576,\n",
       "  'num_leaves': 49,\n",
       "  'max_depth': 4,\n",
       "  'feature_fraction': 0.4839642718097913,\n",
       "  'bagging_fraction': 0.5497799156909652,\n",
       "  'lambda_l1': 0.058207015936292725,\n",
       "  'lambda_l2': 0.08350139961819975,\n",
       "  'min_child_samples': 85,\n",
       "  'n_estimators': 4077},\n",
       " 'RandomForest': {'max_depth': 30,\n",
       "  'min_samples_split': 22,\n",
       "  'min_samples_leaf': 6,\n",
       "  'max_features': 0.8191960808589205,\n",
       "  'n_estimators': 97},\n",
       " 'XGBoost': {'learning_rate': 0.033763045458285304,\n",
       "  'max_depth': 3,\n",
       "  'min_child_weight': 14,\n",
       "  'subsample': 0.9500263286639197,\n",
       "  'colsample_bytree': 0.7747635615654157,\n",
       "  'gamma': 7.558507636378349,\n",
       "  'reg_alpha': 0.4378783168336932,\n",
       "  'reg_lambda': 2.8408676886824322,\n",
       "  'n_estimators': 2804},\n",
       " 'CatBoost': {'depth': 6,\n",
       "  'learning_rate': 0.025804617832891515,\n",
       "  'l2_leaf_reg': 0.22567510948252656,\n",
       "  'random_strength': 1.8064759944089426,\n",
       "  'bagging_temperature': 0.4675254776654324,\n",
       "  'iterations': 1224}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {'lightgbm': {'learning_rate': 0.07089438962267576,\n",
    "  'num_leaves': 49,\n",
    "  'max_depth': 4,\n",
    "  'feature_fraction': 0.4839642718097913,\n",
    "  'bagging_fraction': 0.5497799156909652,\n",
    "  'lambda_l1': 0.058207015936292725,\n",
    "  'lambda_l2': 0.08350139961819975,\n",
    "  'min_child_samples': 85,\n",
    "  'n_estimators': 4077},\n",
    " 'RandomForest': {'max_depth': 30,\n",
    "  'min_samples_split': 22,\n",
    "  'min_samples_leaf': 6,\n",
    "  'max_features': 0.8191960808589205,\n",
    "  'n_estimators': 97},\n",
    " 'XGBoost': {'learning_rate': 0.033763045458285304,\n",
    "  'max_depth': 3,\n",
    "  'min_child_weight': 14,\n",
    "  'subsample': 0.9500263286639197,\n",
    "  'colsample_bytree': 0.7747635615654157,\n",
    "  'gamma': 7.558507636378349,\n",
    "  'reg_alpha': 0.4378783168336932,\n",
    "  'reg_lambda': 2.8408676886824322,\n",
    "  'n_estimators': 2804},\n",
    " 'CatBoost': {'depth': 6,\n",
    "  'learning_rate': 0.025804617832891515,\n",
    "  'l2_leaf_reg': 0.22567510948252656,\n",
    "  'random_strength': 1.8064759944089426,\n",
    "  'bagging_temperature': 0.4675254776654324,\n",
    "  'iterations': 1224}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of folds\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Placeholder for storing results\n",
    "fold_results = []\n",
    "optimized_weights_list = []\n",
    "unique_uids = merged_df['uid'].unique()  # Extract unique uids\n",
    "\n",
    "for train_ids, val_ids in tqdm(KFold(n_splits=5, shuffle=True, random_state=42).split(unique_uids)):\n",
    "    # Split the data\n",
    "    train_uids, val_uids = unique_uids[train_ids], unique_uids[val_ids]\n",
    "    keep_train = merged_df['uid'].isin(train_uids)\n",
    "    keep_val = merged_df['uid'].isin(val_uids)\n",
    "    \n",
    "    train_X, val_X = merged_df[keep_train], merged_df[keep_val]\n",
    "    train_X.drop(columns=['uid'], inplace=True)\n",
    "    val_X.drop(columns=['uid'], inplace=True)\n",
    "    train_y, val_y = y[keep_train], y[keep_val]\n",
    "    \n",
    "    train_X_processed, val_X_processed = merged_df_processed[keep_train], merged_df_processed[keep_val]\n",
    "    train_X_processed.drop(columns=['uid'], inplace=True)\n",
    "    val_X_processed.drop(columns=['uid'], inplace=True)\n",
    "\n",
    "    # Train LightGBM\n",
    "    train_data = lgb.Dataset(train_X, label=train_y, categorical_feature='auto')\n",
    "    val_data = lgb.Dataset(val_X, label=val_y, categorical_feature='auto')\n",
    "        \n",
    "    best_params['lightgbm']['objective'] = 'regression'\n",
    "    best_params['lightgbm']['metric'] = 'rmse'\n",
    "    best_params['lightgbm']['random_state'] = 42\n",
    "\n",
    "    # Train the model with early stopping\n",
    "    model1 = lgb.train(\n",
    "        best_params['lightgbm'],\n",
    "        train_data,\n",
    "        valid_sets=[val_data],\n",
    "        num_boost_round=best_params['lightgbm']['n_estimators'],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=500, verbose=True),\n",
    "        ]\n",
    "    )\n",
    "    pred1 = model1.predict(val_X, num_iteration=model1.best_iteration)\n",
    "\n",
    "    # Train CatBoost\n",
    "    model2 = CatBoostRegressor(\n",
    "        **best_params['CatBoost'],\n",
    "        loss_function='RMSE',\n",
    "        cat_features=train_X.select_dtypes(include=['object', 'category']).columns.to_list(),\n",
    "        verbose=100, early_stopping_rounds=500,\n",
    "        random_state=42\n",
    "    )\n",
    "    model2.fit(train_X, train_y, eval_set=(val_X, val_y))\n",
    "    pred2 = model2.predict(val_X)\n",
    "    \n",
    "    # Train XGBoost\n",
    "    model3 = XGBRegressor(\n",
    "        **best_params['XGBoost'],\n",
    "        enable_categorical=True,\n",
    "        eval_metric=root_mean_squared_error,\n",
    "        early_stopping_rounds=500,\n",
    "        random_state=42)\n",
    "    model3.fit(train_X, train_y, eval_set=[(val_X, val_y)], verbose=100)\n",
    "    pred3 = model3.predict(val_X)\n",
    "    \n",
    "    # Train Random Forest\n",
    "    model4 = RandomForestRegressor(**best_params['RandomForest'], random_state=42)\n",
    "    model4.fit(train_X_processed, train_y)\n",
    "    pred4 = model4.predict(val_X_processed)\n",
    "    print(\"RandomForest rmse: \", root_mean_squared_error(val_y, pred4))\n",
    "\n",
    "    # Define loss function for weight optimization\n",
    "    def loss_function(weights):\n",
    "        w1, w2, w3, w4 = weights\n",
    "        combined_predictions = w1 * pred1 + w2 * pred2 + w3 * pred3 + w4 * pred4\n",
    "        mse = np.mean((combined_predictions - val_y) ** 2)\n",
    "        return mse\n",
    "\n",
    "    # Initial weights\n",
    "    initial_weights = [1/4, 1/4, 1/4, 1/4]\n",
    "\n",
    "    # Constraints: weights must sum to 1\n",
    "    constraints = {'type': 'eq', 'fun': lambda w: w[0] + w[1] + w[2] + w[3] - 1}\n",
    "\n",
    "    # Bounds: weights must be between 0 and 1\n",
    "    bounds = [(0, 1), (0, 1), (0, 1), (0, 1)]\n",
    "\n",
    "    # Optimize weights\n",
    "    result = minimize(loss_function, initial_weights, constraints=constraints, bounds=bounds)\n",
    "    optimized_weights = result.x\n",
    "\n",
    "    # Combine predictions using optimized weights\n",
    "    final_predictions = (\n",
    "        optimized_weights[0] * pred1 +\n",
    "        optimized_weights[1] * pred2 +\n",
    "        optimized_weights[2] * pred3 +\n",
    "        optimized_weights[3] * pred4\n",
    "    )\n",
    "\n",
    "    # Store results\n",
    "    fold_mse = root_mean_squared_error(val_y, final_predictions)  # RMSE\n",
    "    fold_results.append(fold_mse)\n",
    "    optimized_weights_list.append(optimized_weights)\n",
    "\n",
    "# Display results\n",
    "print(f\"Average MSE across folds: {np.mean(fold_results)}\")\n",
    "print(f\"Optimized weights per fold: {optimized_weights_list}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MSE across folds: 40.157203856294316\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average MSE across folds: {np.mean(fold_results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25985829, 0.38508924, 0.21887303, 0.13617944])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(optimized_weights_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average weights from cross-validation\n",
    "average_weights = np.mean(optimized_weights_list, axis=0)\n",
    "\n",
    "# Train models on the entire training dataset\n",
    "merged_df.drop(columns=['uid'], inplace=True)\n",
    "merged_test.drop(columns=['uid'], inplace=True)\n",
    "merged_df_processed.drop(columns=['uid'], inplace=True)\n",
    "merged_test_processed.drop(columns=['uid'], inplace=True)\n",
    "train_data = lgb.Dataset(merged_df, label=y, categorical_feature='auto')\n",
    "\n",
    "best_params['lightgbm']['objective'] = 'regression'\n",
    "best_params['lightgbm']['metric'] = 'rmse'\n",
    "best_params['lightgbm']['random_state'] = 42\n",
    "\n",
    "final_model1 = lgb.train(\n",
    "    best_params['lightgbm'],\n",
    "    train_data,\n",
    "    num_boost_round=best_params['lightgbm']['n_estimators']\n",
    ")\n",
    "\n",
    "final_model2 = CatBoostRegressor(\n",
    "    **best_params['CatBoost'],\n",
    "    loss_function='RMSE',\n",
    "    cat_features=merged_df.select_dtypes(include=['object', 'category']).columns.to_list(),\n",
    "    random_state=42\n",
    ")\n",
    "final_model2.fit(merged_df, y)\n",
    "\n",
    "final_model3 = XGBRegressor(\n",
    "    **best_params['XGBoost'],\n",
    "    enable_categorical=True,\n",
    "    eval_metric=root_mean_squared_error,\n",
    "    random_state=42)\n",
    "final_model3.fit(merged_df, y)\n",
    "\n",
    "# Train Random Forest\n",
    "final_model4 = RandomForestRegressor(**best_params['RandomForest'], random_state=42)\n",
    "final_model4.fit(merged_df_processed, y)\n",
    "\n",
    "# Generate predictions on the test dataset\n",
    "test_pred1 = final_model1.predict(merged_test)\n",
    "test_pred2 = final_model2.predict(merged_test)\n",
    "test_pred3 = final_model3.predict(merged_test)\n",
    "test_pred4 = final_model4.predict(merged_test_processed)\n",
    "\n",
    "# Combine the predictions using the average weights\n",
    "final_test_predictions = (\n",
    "    average_weights[0] * test_pred1 + average_weights[1] * test_pred2 + average_weights[2] * test_pred3 + average_weights[3] * test_pred4\n",
    ")\n",
    "\n",
    "# Optionally round predictions if required (e.g., for classification tasks)\n",
    "final_test_predictions = np.round(final_test_predictions).astype(int)\n",
    "\n",
    "# Display final predictions\n",
    "print(\"Final blended predictions for the test dataset:\")\n",
    "print(final_test_predictions)\n",
    "\n",
    "ss['composite_score']=final_test_predictions\n",
    "#generate submission\n",
    "ss.to_csv('dataset/Final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39.23915534535873"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean squared error on test set\n",
    "root_mean_squared_error(test_ground_truth['composite_score'], ss['composite_score'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
