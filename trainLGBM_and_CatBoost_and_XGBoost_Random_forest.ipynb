{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# load libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.model_selection import StratifiedGroupKFold,StratifiedShuffleSplit,KFold,train_test_split\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "from scipy.optimize import minimize\n",
    "from tqdm import tqdm\n",
    "from xgboost import XGBRegressor\n",
    "import copy\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data\n",
    "train=pd.read_csv(\"dataset/train_features.csv\")\n",
    "y=pd.read_csv(\"dataset/train_labels.csv\")\n",
    "test=pd.read_csv(\"dataset/test_features.csv\")\n",
    "ss=pd.read_csv(\"dataset/submission_format.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>age_03</th>\n",
       "      <th>urban_03</th>\n",
       "      <th>married_03</th>\n",
       "      <th>n_mar_03</th>\n",
       "      <th>edu_gru_03</th>\n",
       "      <th>n_living_child_03</th>\n",
       "      <th>migration_03</th>\n",
       "      <th>glob_hlth_03</th>\n",
       "      <th>adl_dress_03</th>\n",
       "      <th>...</th>\n",
       "      <th>rrelgimp_12</th>\n",
       "      <th>rrfcntx_m_12</th>\n",
       "      <th>rsocact_m_12</th>\n",
       "      <th>rrelgwk_12</th>\n",
       "      <th>a16a_12</th>\n",
       "      <th>a21_12</th>\n",
       "      <th>a22_12</th>\n",
       "      <th>a33b_12</th>\n",
       "      <th>a34_12</th>\n",
       "      <th>j11_12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aace</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.somewhat important</td>\n",
       "      <td>9.Never</td>\n",
       "      <td>9.Never</td>\n",
       "      <td>0.No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Concrete 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aanz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.very important</td>\n",
       "      <td>9.Never</td>\n",
       "      <td>1.Almost every day</td>\n",
       "      <td>0.No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Concrete 2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 184 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    uid age_03 urban_03 married_03  n_mar_03 edu_gru_03 n_living_child_03  \\\n",
       "0  aace    NaN      NaN        NaN       NaN        NaN               NaN   \n",
       "1  aanz    NaN      NaN        NaN       NaN        NaN               NaN   \n",
       "\n",
       "   migration_03 glob_hlth_03  adl_dress_03  ...           rrelgimp_12  \\\n",
       "0           NaN          NaN           NaN  ...  2.somewhat important   \n",
       "1           NaN          NaN           NaN  ...      1.very important   \n",
       "\n",
       "   rrfcntx_m_12        rsocact_m_12  rrelgwk_12  a16a_12  a21_12  a22_12  \\\n",
       "0       9.Never             9.Never        0.No      NaN     NaN     NaN   \n",
       "1       9.Never  1.Almost every day        0.No      NaN     NaN     NaN   \n",
       "\n",
       "   a33b_12  a34_12      j11_12  \n",
       "0      NaN     NaN  Concrete 2  \n",
       "1      NaN     NaN  Concrete 2  \n",
       "\n",
       "[2 rows x 184 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>age_03</th>\n",
       "      <th>urban_03</th>\n",
       "      <th>married_03</th>\n",
       "      <th>n_mar_03</th>\n",
       "      <th>edu_gru_03</th>\n",
       "      <th>n_living_child_03</th>\n",
       "      <th>migration_03</th>\n",
       "      <th>glob_hlth_03</th>\n",
       "      <th>adl_dress_03</th>\n",
       "      <th>...</th>\n",
       "      <th>rrelgimp_12</th>\n",
       "      <th>rrfcntx_m_12</th>\n",
       "      <th>rsocact_m_12</th>\n",
       "      <th>rrelgwk_12</th>\n",
       "      <th>a16a_12</th>\n",
       "      <th>a21_12</th>\n",
       "      <th>a22_12</th>\n",
       "      <th>a33b_12</th>\n",
       "      <th>a34_12</th>\n",
       "      <th>j11_12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abxu</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wood, mosaic, or other covering 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aeol</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.very important</td>\n",
       "      <td>9.Never</td>\n",
       "      <td>9.Never</td>\n",
       "      <td>1.Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Concrete 2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 184 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    uid age_03 urban_03 married_03  n_mar_03 edu_gru_03 n_living_child_03  \\\n",
       "0  abxu    NaN      NaN        NaN       NaN        NaN               NaN   \n",
       "1  aeol    NaN      NaN        NaN       NaN        NaN               NaN   \n",
       "\n",
       "   migration_03 glob_hlth_03  adl_dress_03  ...       rrelgimp_12  \\\n",
       "0           NaN          NaN           NaN  ...               NaN   \n",
       "1           NaN          NaN           NaN  ...  1.very important   \n",
       "\n",
       "   rrfcntx_m_12  rsocact_m_12  rrelgwk_12  a16a_12  a21_12  a22_12  a33b_12  \\\n",
       "0           NaN           NaN         NaN      NaN     NaN     NaN      NaN   \n",
       "1       9.Never       9.Never       1.Yes      NaN     NaN     NaN      NaN   \n",
       "\n",
       "   a34_12                             j11_12  \n",
       "0     NaN  Wood, mosaic, or other covering 1  \n",
       "1     NaN                         Concrete 2  \n",
       "\n",
       "[2 rows x 184 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3276 entries, 0 to 3275\n",
      "Columns: 184 entries, uid to j11_12\n",
      "dtypes: float64(140), object(44)\n",
      "memory usage: 4.6+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let merge train and label\n",
    "merged_df = pd.merge(train, y, on='uid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       2021\n",
       "1       2021\n",
       "2       2016\n",
       "3       2021\n",
       "4       2021\n",
       "        ... \n",
       "4338    2021\n",
       "4339    2016\n",
       "4340    2021\n",
       "4341    2021\n",
       "4342    2021\n",
       "Name: year, Length: 4343, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df[\"year\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "uid                   0\n",
       "age_03             1456\n",
       "urban_03           1454\n",
       "married_03         1454\n",
       "n_mar_03           1482\n",
       "                   ... \n",
       "a33b_12            4288\n",
       "a34_12             1601\n",
       "j11_12               89\n",
       "year                  0\n",
       "composite_score       0\n",
       "Length: 186, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#so many missing values\n",
    "merged_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets merged test AND sample submission\n",
    "merged_test = pd.merge(test, ss, on='uid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       2016\n",
       "1       2016\n",
       "2       2021\n",
       "3       2016\n",
       "4       2021\n",
       "        ... \n",
       "1100    2016\n",
       "1101    2021\n",
       "1102    2016\n",
       "1103    2021\n",
       "1104    2021\n",
       "Name: year, Length: 1105, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_test[\"year\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: title={'center': 'Target Distribution'}, ylabel='Frequency'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "merged_df['composite_score'].plot(kind='hist',title='Target Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cat_cols(data):\n",
    "    # Get the columns with object datatype\n",
    "    cat_columns=[]\n",
    "    dummies=[]\n",
    "    for col in data.columns:\n",
    "        if data[col].dtype=='object' and 'uid' not in col:\n",
    "            cat_columns.append(col)\n",
    "            dummies.append(col)\n",
    "        elif data[col].dtype!='object' and 'uid' not in col and (data[col].max()==1.0):\n",
    "            cat_columns.append(col)\n",
    "            data[col].fillna(0, inplace=True)\n",
    "        else:\n",
    "            continue\n",
    "    return cat_columns, dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_cat_cols(train_data, cat_cols, dummy_cols):\n",
    "    enc = OneHotEncoder()\n",
    "    enc.fit(train_data[dummy_cols])\n",
    "    encoded_train_data=enc.transform(train_data[dummy_cols]).toarray()\n",
    "    feature_names = enc.get_feature_names_out(dummy_cols)\n",
    "    train_data.drop(columns=dummy_cols, inplace=True)\n",
    "    encoded_train_df = pd.DataFrame(encoded_train_data, columns=feature_names)\n",
    "    train_data[feature_names]=encoded_train_df[feature_names]\n",
    "    return train_data, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_955414/53275523.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[col].fillna(0, inplace=True)\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_955414/4256840405.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data_processed[col].fillna(0, inplace=True)\n",
      "/tmp/ipykernel_955414/4256840405.py:14: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data_processed[col].fillna(0, inplace=True)\n",
      "/tmp/ipykernel_955414/4256840405.py:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data_processed[col].fillna(0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_mar_03 33.83 % missing\n",
      "n_adl_03 33.5 % missing\n",
      "n_iadl_03 36.99 % missing\n",
      "n_depr_03 37.1 % missing\n",
      "n_illnesses_03 33.31 % missing\n",
      "decis_personal_03 37.17 % missing\n",
      "n_iadl_12 5.84 % missing\n",
      "n_depr_12 6.52 % missing\n",
      "rearnings_03 33.37 % missing\n",
      "searnings_03 49.98 % missing\n",
      "hincome_03 34.01 % missing\n",
      "hinc_business_03 32.98 % missing\n",
      "hinc_rent_03 32.98 % missing\n",
      "hinc_assets_03 32.98 % missing\n",
      "hinc_cap_03 32.98 % missing\n",
      "rinc_pension_03 33.37 % missing\n",
      "sinc_pension_03 49.98 % missing\n",
      "searnings_12 35.19 % missing\n",
      "sinc_pension_12 35.19 % missing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_955414/4256840405.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data_processed[col].fillna(0, inplace=True)\n",
      "/tmp/ipykernel_955414/4256840405.py:14: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data_processed[col].fillna(0, inplace=True)\n",
      "/tmp/ipykernel_955414/4256840405.py:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data_processed[col].fillna(0, inplace=True)\n",
      "/tmp/ipykernel_955414/4256840405.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data_processed[col].fillna(0, inplace=True)\n",
      "/tmp/ipykernel_955414/4256840405.py:14: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data_processed[col].fillna(0, inplace=True)\n",
      "/tmp/ipykernel_955414/4256840405.py:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data_processed[col].fillna(0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "y=merged_df['composite_score']\n",
    "data_processed=pd.concat((merged_df,merged_test)).reset_index(drop=True).copy()\n",
    "cat_cols, dummy_cols = get_cat_cols(data_processed)\n",
    "data_processed, dummy_feature_names=encode_cat_cols(data_processed, cat_cols, dummy_cols)\n",
    "data_processed=data_processed.drop(columns=['uid', 'composite_score'],axis=1)\n",
    "\n",
    "for col in data_processed.columns: \n",
    "    if round((data_processed[col].isna().sum() /len(data_processed)*100), 2)>50:\n",
    "        data_processed.drop(columns=col, inplace=True)\n",
    "    elif round((data_processed[col].isna().sum() /len(data_processed)*100), 2)>5:\n",
    "        print(col,round((data_processed[col].isna().sum() /len(data_processed)*100), 2), '% missing')\n",
    "        data_processed[col].fillna(0, inplace=True)\n",
    "    else: \n",
    "        data_processed[col].fillna(0, inplace=True)\n",
    "        data_processed[col].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=merged_df['composite_score']\n",
    "data=pd.concat((merged_df,merged_test)).reset_index(drop=True).copy()\n",
    "data=data.drop(columns=['uid','composite_score'],axis=1)\n",
    "\n",
    "# Get the columns with object datatype\n",
    "object_cols = data.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Convert the object columns to category dtype\n",
    "for col in object_cols:\n",
    "    #data[col] = data[col].astype('category').fillna(\"Missing\")\n",
    "    data[col] = pd.Categorical(data[col].fillna(\"Missing\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate train and test\n",
    "merged_df_processed=data_processed[:len(merged_df)]\n",
    "merged_test_processed=data_processed[len(merged_df):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate train and test\n",
    "merged_df=data[:len(merged_df)]\n",
    "merged_test=data[len(merged_df):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4343, 184)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4343, 377)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df_processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/u/spa-d4/grad/mfe261/Projects/PREPARE/envs/lib/python3.9/site-packages/lightgbm/engine.py:204: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000929 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1565\n",
      "[LightGBM] [Info] Number of data points in the train set: 3474, number of used features: 184\n",
      "[LightGBM] [Info] Start training from score 157.136730\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1492]\tvalid_0's rmse: 37.0653\n",
      "0:\tlearn: 61.0968399\ttest: 58.8058395\tbest: 58.8058395 (0)\ttotal: 68.9ms\tremaining: 11m 29s\n",
      "100:\tlearn: 45.4410279\ttest: 45.6028066\tbest: 45.6028066 (100)\ttotal: 2.62s\tremaining: 4m 17s\n",
      "200:\tlearn: 39.0878675\ttest: 41.5456612\tbest: 41.5456612 (200)\ttotal: 5.98s\tremaining: 4m 51s\n",
      "300:\tlearn: 35.5986901\ttest: 39.9273411\tbest: 39.9273411 (300)\ttotal: 9.37s\tremaining: 5m 2s\n",
      "400:\tlearn: 33.2094891\ttest: 39.1314163\tbest: 39.1314163 (400)\ttotal: 12.4s\tremaining: 4m 57s\n",
      "500:\tlearn: 31.4176779\ttest: 38.6480547\tbest: 38.6480547 (500)\ttotal: 15.5s\tremaining: 4m 53s\n",
      "600:\tlearn: 30.1218652\ttest: 38.3368350\tbest: 38.3368350 (600)\ttotal: 18.6s\tremaining: 4m 50s\n",
      "700:\tlearn: 29.0396136\ttest: 38.1174048\tbest: 38.1174048 (700)\ttotal: 21.3s\tremaining: 4m 42s\n",
      "800:\tlearn: 28.1765249\ttest: 37.9571419\tbest: 37.9571419 (800)\ttotal: 24.1s\tremaining: 4m 36s\n",
      "900:\tlearn: 27.3631189\ttest: 37.8359519\tbest: 37.8359519 (900)\ttotal: 26.9s\tremaining: 4m 31s\n",
      "1000:\tlearn: 26.6854148\ttest: 37.7060396\tbest: 37.7059171 (999)\ttotal: 30s\tremaining: 4m 29s\n",
      "1100:\tlearn: 25.9836598\ttest: 37.5950522\tbest: 37.5950522 (1100)\ttotal: 33.1s\tremaining: 4m 27s\n",
      "1200:\tlearn: 25.2755455\ttest: 37.4891538\tbest: 37.4891538 (1200)\ttotal: 36.3s\tremaining: 4m 25s\n",
      "1300:\tlearn: 24.6105168\ttest: 37.3926022\tbest: 37.3924316 (1298)\ttotal: 39.5s\tremaining: 4m 24s\n",
      "1400:\tlearn: 23.9258090\ttest: 37.3240111\tbest: 37.3239032 (1399)\ttotal: 42.9s\tremaining: 4m 23s\n",
      "1500:\tlearn: 23.1768420\ttest: 37.2432973\tbest: 37.2432973 (1500)\ttotal: 46.2s\tremaining: 4m 21s\n",
      "1600:\tlearn: 22.4494470\ttest: 37.1927855\tbest: 37.1910263 (1591)\ttotal: 49.5s\tremaining: 4m 19s\n",
      "1700:\tlearn: 21.7832670\ttest: 37.1244423\tbest: 37.1240429 (1696)\ttotal: 52.9s\tremaining: 4m 18s\n",
      "1800:\tlearn: 21.2058446\ttest: 37.0873275\tbest: 37.0873275 (1800)\ttotal: 56.4s\tremaining: 4m 16s\n",
      "1900:\tlearn: 20.6398615\ttest: 37.0298037\tbest: 37.0298037 (1900)\ttotal: 59.8s\tremaining: 4m 14s\n",
      "2000:\tlearn: 20.0176130\ttest: 36.9825443\tbest: 36.9825443 (2000)\ttotal: 1m 3s\tremaining: 4m 12s\n",
      "2100:\tlearn: 19.4370441\ttest: 36.9295941\tbest: 36.9295941 (2100)\ttotal: 1m 6s\tremaining: 4m 9s\n",
      "2200:\tlearn: 18.8953118\ttest: 36.8912659\tbest: 36.8912659 (2200)\ttotal: 1m 10s\tremaining: 4m 8s\n",
      "2300:\tlearn: 18.3999280\ttest: 36.8524221\tbest: 36.8524097 (2299)\ttotal: 1m 13s\tremaining: 4m 5s\n",
      "2400:\tlearn: 17.9799441\ttest: 36.8176085\tbest: 36.8173416 (2399)\ttotal: 1m 16s\tremaining: 4m 3s\n",
      "2500:\tlearn: 17.5083622\ttest: 36.7888098\tbest: 36.7863951 (2494)\ttotal: 1m 20s\tremaining: 4m 1s\n",
      "2600:\tlearn: 17.0996144\ttest: 36.7612197\tbest: 36.7608820 (2599)\ttotal: 1m 23s\tremaining: 3m 58s\n",
      "2700:\tlearn: 16.6814348\ttest: 36.7391370\tbest: 36.7391370 (2700)\ttotal: 1m 27s\tremaining: 3m 56s\n",
      "2800:\tlearn: 16.2917057\ttest: 36.7084424\tbest: 36.7076249 (2795)\ttotal: 1m 31s\tremaining: 3m 55s\n",
      "2900:\tlearn: 15.9314976\ttest: 36.6941992\tbest: 36.6933951 (2899)\ttotal: 1m 35s\tremaining: 3m 52s\n",
      "3000:\tlearn: 15.5698204\ttest: 36.6867289\tbest: 36.6866828 (2968)\ttotal: 1m 38s\tremaining: 3m 50s\n",
      "3100:\tlearn: 15.1945936\ttest: 36.6655089\tbest: 36.6653550 (3099)\ttotal: 1m 42s\tremaining: 3m 47s\n",
      "3200:\tlearn: 14.8691131\ttest: 36.6411608\tbest: 36.6409042 (3199)\ttotal: 1m 45s\tremaining: 3m 44s\n",
      "3300:\tlearn: 14.5703165\ttest: 36.6247168\tbest: 36.6235915 (3296)\ttotal: 1m 49s\tremaining: 3m 41s\n",
      "3400:\tlearn: 14.2799347\ttest: 36.6045406\tbest: 36.6045406 (3400)\ttotal: 1m 52s\tremaining: 3m 38s\n",
      "3500:\tlearn: 14.0235511\ttest: 36.5888776\tbest: 36.5885848 (3499)\ttotal: 1m 56s\tremaining: 3m 35s\n",
      "3600:\tlearn: 13.7498341\ttest: 36.5898048\tbest: 36.5858308 (3543)\ttotal: 1m 59s\tremaining: 3m 32s\n",
      "3700:\tlearn: 13.5143747\ttest: 36.5794085\tbest: 36.5790383 (3697)\ttotal: 2m 2s\tremaining: 3m 28s\n",
      "3800:\tlearn: 13.2829866\ttest: 36.5735306\tbest: 36.5708231 (3776)\ttotal: 2m 5s\tremaining: 3m 25s\n",
      "3900:\tlearn: 13.0679218\ttest: 36.5641869\tbest: 36.5641869 (3900)\ttotal: 2m 9s\tremaining: 3m 22s\n",
      "4000:\tlearn: 12.8320951\ttest: 36.5494418\tbest: 36.5485848 (3983)\ttotal: 2m 12s\tremaining: 3m 19s\n",
      "4100:\tlearn: 12.6152979\ttest: 36.5421505\tbest: 36.5416001 (4091)\ttotal: 2m 16s\tremaining: 3m 15s\n",
      "4200:\tlearn: 12.4105339\ttest: 36.5292587\tbest: 36.5292587 (4200)\ttotal: 2m 19s\tremaining: 3m 12s\n",
      "4300:\tlearn: 12.1846440\ttest: 36.5244883\tbest: 36.5238771 (4298)\ttotal: 2m 22s\tremaining: 3m 9s\n",
      "4400:\tlearn: 11.9951569\ttest: 36.5196004\tbest: 36.5173076 (4354)\ttotal: 2m 26s\tremaining: 3m 5s\n",
      "4500:\tlearn: 11.7815892\ttest: 36.5221598\tbest: 36.5169857 (4444)\ttotal: 2m 29s\tremaining: 3m 2s\n",
      "4600:\tlearn: 11.6140948\ttest: 36.5188373\tbest: 36.5169857 (4444)\ttotal: 2m 32s\tremaining: 2m 59s\n",
      "4700:\tlearn: 11.4349415\ttest: 36.5153531\tbest: 36.5153531 (4700)\ttotal: 2m 36s\tremaining: 2m 56s\n",
      "4800:\tlearn: 11.2563215\ttest: 36.5088976\tbest: 36.5088976 (4800)\ttotal: 2m 39s\tremaining: 2m 52s\n",
      "4900:\tlearn: 11.0653295\ttest: 36.4970832\tbest: 36.4967211 (4895)\ttotal: 2m 42s\tremaining: 2m 49s\n",
      "5000:\tlearn: 10.8695620\ttest: 36.4904353\tbest: 36.4903101 (4999)\ttotal: 2m 46s\tremaining: 2m 46s\n",
      "5100:\tlearn: 10.7188535\ttest: 36.4793738\tbest: 36.4786537 (5098)\ttotal: 2m 50s\tremaining: 2m 43s\n",
      "5200:\tlearn: 10.5643047\ttest: 36.4800573\tbest: 36.4786537 (5098)\ttotal: 2m 53s\tremaining: 2m 40s\n",
      "5300:\tlearn: 10.4024245\ttest: 36.4751029\tbest: 36.4737995 (5277)\ttotal: 2m 57s\tremaining: 2m 37s\n",
      "5400:\tlearn: 10.2537598\ttest: 36.4704155\tbest: 36.4689785 (5359)\ttotal: 3m 1s\tremaining: 2m 34s\n",
      "5500:\tlearn: 10.0892639\ttest: 36.4667787\tbest: 36.4654975 (5478)\ttotal: 3m 4s\tremaining: 2m 31s\n",
      "5600:\tlearn: 9.9391060\ttest: 36.4604487\tbest: 36.4594691 (5592)\ttotal: 3m 8s\tremaining: 2m 28s\n",
      "5700:\tlearn: 9.7638891\ttest: 36.4567215\tbest: 36.4567215 (5700)\ttotal: 3m 12s\tremaining: 2m 24s\n",
      "5800:\tlearn: 9.6296542\ttest: 36.4553932\tbest: 36.4553932 (5800)\ttotal: 3m 15s\tremaining: 2m 21s\n",
      "5900:\tlearn: 9.4853367\ttest: 36.4477291\tbest: 36.4477291 (5900)\ttotal: 3m 19s\tremaining: 2m 18s\n",
      "6000:\tlearn: 9.3542688\ttest: 36.4411590\tbest: 36.4411590 (6000)\ttotal: 3m 22s\tremaining: 2m 15s\n",
      "6100:\tlearn: 9.2140346\ttest: 36.4413088\tbest: 36.4387112 (6038)\ttotal: 3m 26s\tremaining: 2m 11s\n",
      "6200:\tlearn: 9.0822900\ttest: 36.4418505\tbest: 36.4387112 (6038)\ttotal: 3m 29s\tremaining: 2m 8s\n",
      "6300:\tlearn: 8.9475066\ttest: 36.4388100\tbest: 36.4387112 (6038)\ttotal: 3m 33s\tremaining: 2m 5s\n",
      "6400:\tlearn: 8.8092712\ttest: 36.4445999\tbest: 36.4385390 (6305)\ttotal: 3m 36s\tremaining: 2m 1s\n",
      "6500:\tlearn: 8.6750206\ttest: 36.4421406\tbest: 36.4385390 (6305)\ttotal: 3m 40s\tremaining: 1m 58s\n",
      "6600:\tlearn: 8.5550143\ttest: 36.4367300\tbest: 36.4367300 (6600)\ttotal: 3m 44s\tremaining: 1m 55s\n",
      "6700:\tlearn: 8.4524290\ttest: 36.4344232\tbest: 36.4343184 (6688)\ttotal: 3m 47s\tremaining: 1m 52s\n",
      "6800:\tlearn: 8.3637324\ttest: 36.4349691\tbest: 36.4340056 (6753)\ttotal: 3m 51s\tremaining: 1m 48s\n",
      "6900:\tlearn: 8.2472591\ttest: 36.4297468\tbest: 36.4284168 (6859)\ttotal: 3m 54s\tremaining: 1m 45s\n",
      "7000:\tlearn: 8.1407266\ttest: 36.4275655\tbest: 36.4270412 (6988)\ttotal: 3m 58s\tremaining: 1m 41s\n",
      "7100:\tlearn: 8.0303315\ttest: 36.4251227\tbest: 36.4248142 (7097)\ttotal: 4m 1s\tremaining: 1m 38s\n",
      "7200:\tlearn: 7.9171127\ttest: 36.4233965\tbest: 36.4210436 (7143)\ttotal: 4m 5s\tremaining: 1m 35s\n",
      "7300:\tlearn: 7.8058411\ttest: 36.4187669\tbest: 36.4187669 (7300)\ttotal: 4m 8s\tremaining: 1m 31s\n",
      "7400:\tlearn: 7.6804825\ttest: 36.4123071\tbest: 36.4119659 (7395)\ttotal: 4m 12s\tremaining: 1m 28s\n",
      "7500:\tlearn: 7.5694407\ttest: 36.4141152\tbest: 36.4119659 (7395)\ttotal: 4m 15s\tremaining: 1m 25s\n",
      "7600:\tlearn: 7.4509310\ttest: 36.4117020\tbest: 36.4109704 (7574)\ttotal: 4m 19s\tremaining: 1m 21s\n",
      "7700:\tlearn: 7.3461658\ttest: 36.4075210\tbest: 36.4058096 (7684)\ttotal: 4m 22s\tremaining: 1m 18s\n",
      "7800:\tlearn: 7.2199802\ttest: 36.4095842\tbest: 36.4058096 (7684)\ttotal: 4m 25s\tremaining: 1m 14s\n",
      "7900:\tlearn: 7.1069357\ttest: 36.4068844\tbest: 36.4054124 (7884)\ttotal: 4m 29s\tremaining: 1m 11s\n",
      "8000:\tlearn: 6.9823419\ttest: 36.4056810\tbest: 36.4019511 (7968)\ttotal: 4m 32s\tremaining: 1m 8s\n",
      "8100:\tlearn: 6.8887243\ttest: 36.4034192\tbest: 36.4019511 (7968)\ttotal: 4m 36s\tremaining: 1m 4s\n",
      "8200:\tlearn: 6.7803534\ttest: 36.4027831\tbest: 36.4016074 (8122)\ttotal: 4m 39s\tremaining: 1m 1s\n",
      "8300:\tlearn: 6.6760177\ttest: 36.4045502\tbest: 36.4016074 (8122)\ttotal: 4m 43s\tremaining: 58s\n",
      "8400:\tlearn: 6.5772731\ttest: 36.4073496\tbest: 36.4016074 (8122)\ttotal: 4m 47s\tremaining: 54.6s\n",
      "8500:\tlearn: 6.4686491\ttest: 36.4040017\tbest: 36.4016074 (8122)\ttotal: 4m 50s\tremaining: 51.3s\n",
      "8600:\tlearn: 6.3742026\ttest: 36.4027607\tbest: 36.4016074 (8122)\ttotal: 4m 54s\tremaining: 47.9s\n",
      "Stopped by overfitting detector  (500 iterations wait)\n",
      "\n",
      "bestTest = 36.40160744\n",
      "bestIteration = 8122\n",
      "\n",
      "Shrink model to first 8123 iterations.\n",
      "[0]\tvalidation_0-rmse:58.77700\tvalidation_0-root_mean_squared_error:58.77700\n",
      "[100]\tvalidation_0-rmse:45.87150\tvalidation_0-root_mean_squared_error:45.87149\n",
      "[200]\tvalidation_0-rmse:42.37062\tvalidation_0-root_mean_squared_error:42.37062\n",
      "[300]\tvalidation_0-rmse:40.80716\tvalidation_0-root_mean_squared_error:40.80716\n",
      "[400]\tvalidation_0-rmse:40.00850\tvalidation_0-root_mean_squared_error:40.00850\n",
      "[500]\tvalidation_0-rmse:39.63225\tvalidation_0-root_mean_squared_error:39.63225\n",
      "[600]\tvalidation_0-rmse:39.40642\tvalidation_0-root_mean_squared_error:39.40642\n",
      "[700]\tvalidation_0-rmse:39.24872\tvalidation_0-root_mean_squared_error:39.24872\n",
      "[800]\tvalidation_0-rmse:39.13328\tvalidation_0-root_mean_squared_error:39.13328\n",
      "[900]\tvalidation_0-rmse:39.04278\tvalidation_0-root_mean_squared_error:39.04278\n",
      "[1000]\tvalidation_0-rmse:38.97120\tvalidation_0-root_mean_squared_error:38.97120\n",
      "[1100]\tvalidation_0-rmse:38.87067\tvalidation_0-root_mean_squared_error:38.87067\n",
      "[1200]\tvalidation_0-rmse:38.80425\tvalidation_0-root_mean_squared_error:38.80425\n",
      "[1300]\tvalidation_0-rmse:38.75112\tvalidation_0-root_mean_squared_error:38.75112\n",
      "[1400]\tvalidation_0-rmse:38.71132\tvalidation_0-root_mean_squared_error:38.71131\n",
      "[1500]\tvalidation_0-rmse:38.64264\tvalidation_0-root_mean_squared_error:38.64263\n",
      "[1600]\tvalidation_0-rmse:38.58799\tvalidation_0-root_mean_squared_error:38.58799\n",
      "[1700]\tvalidation_0-rmse:38.55403\tvalidation_0-root_mean_squared_error:38.55403\n",
      "[1800]\tvalidation_0-rmse:38.51003\tvalidation_0-root_mean_squared_error:38.51003\n",
      "[1900]\tvalidation_0-rmse:38.46614\tvalidation_0-root_mean_squared_error:38.46614\n",
      "[2000]\tvalidation_0-rmse:38.41804\tvalidation_0-root_mean_squared_error:38.41804\n",
      "[2100]\tvalidation_0-rmse:38.37496\tvalidation_0-root_mean_squared_error:38.37496\n",
      "[2200]\tvalidation_0-rmse:38.33310\tvalidation_0-root_mean_squared_error:38.33310\n",
      "[2300]\tvalidation_0-rmse:38.28201\tvalidation_0-root_mean_squared_error:38.28201\n",
      "[2400]\tvalidation_0-rmse:38.26208\tvalidation_0-root_mean_squared_error:38.26208\n",
      "[2500]\tvalidation_0-rmse:38.22938\tvalidation_0-root_mean_squared_error:38.22938\n",
      "[2600]\tvalidation_0-rmse:38.20272\tvalidation_0-root_mean_squared_error:38.20272\n",
      "[2700]\tvalidation_0-rmse:38.19138\tvalidation_0-root_mean_squared_error:38.19138\n",
      "[2800]\tvalidation_0-rmse:38.17518\tvalidation_0-root_mean_squared_error:38.17519\n",
      "[2900]\tvalidation_0-rmse:38.16945\tvalidation_0-root_mean_squared_error:38.16945\n",
      "[3000]\tvalidation_0-rmse:38.15760\tvalidation_0-root_mean_squared_error:38.15760\n",
      "[3100]\tvalidation_0-rmse:38.16990\tvalidation_0-root_mean_squared_error:38.16989\n",
      "[3200]\tvalidation_0-rmse:38.18416\tvalidation_0-root_mean_squared_error:38.18416\n",
      "[3300]\tvalidation_0-rmse:38.18645\tvalidation_0-root_mean_squared_error:38.18645\n",
      "[3400]\tvalidation_0-rmse:38.19213\tvalidation_0-root_mean_squared_error:38.19214\n",
      "[3495]\tvalidation_0-rmse:38.19632\tvalidation_0-root_mean_squared_error:38.19632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [06:49, 409.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest rmse:  37.167595311963325\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013938 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1558\n",
      "[LightGBM] [Info] Number of data points in the train set: 3474, number of used features: 184\n",
      "[LightGBM] [Info] Start training from score 157.271445\n",
      "Training until validation scores don't improve for 500 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/spa-d4/grad/mfe261/Projects/PREPARE/envs/lib/python3.9/site-packages/lightgbm/engine.py:204: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1278]\tvalid_0's rmse: 36.2766\n",
      "0:\tlearn: 59.9979103\ttest: 63.2095608\tbest: 63.2095608 (0)\ttotal: 92.2ms\tremaining: 15m 21s\n",
      "100:\tlearn: 45.1486532\ttest: 47.5175717\tbest: 47.5175717 (100)\ttotal: 2.57s\tremaining: 4m 11s\n",
      "200:\tlearn: 39.0906320\ttest: 42.2019668\tbest: 42.2019668 (200)\ttotal: 5.49s\tremaining: 4m 27s\n",
      "300:\tlearn: 35.7810579\ttest: 40.0875359\tbest: 40.0875359 (300)\ttotal: 8.41s\tremaining: 4m 30s\n",
      "400:\tlearn: 33.4582687\ttest: 39.0410680\tbest: 39.0410680 (400)\ttotal: 11.3s\tremaining: 4m 31s\n",
      "500:\tlearn: 31.8339698\ttest: 38.4222289\tbest: 38.4222289 (500)\ttotal: 14.3s\tremaining: 4m 30s\n",
      "600:\tlearn: 30.4534694\ttest: 38.0403618\tbest: 38.0403618 (600)\ttotal: 17.1s\tremaining: 4m 27s\n",
      "700:\tlearn: 29.4400671\ttest: 37.8087585\tbest: 37.8087585 (700)\ttotal: 20.1s\tremaining: 4m 25s\n",
      "800:\tlearn: 28.4904281\ttest: 37.6239716\tbest: 37.6239716 (800)\ttotal: 22.8s\tremaining: 4m 21s\n",
      "900:\tlearn: 27.5077689\ttest: 37.4530028\tbest: 37.4530028 (900)\ttotal: 25.9s\tremaining: 4m 21s\n",
      "1000:\tlearn: 26.5939412\ttest: 37.3155630\tbest: 37.3155630 (1000)\ttotal: 29.3s\tremaining: 4m 23s\n",
      "1100:\tlearn: 25.8106802\ttest: 37.1859183\tbest: 37.1847637 (1098)\ttotal: 32.6s\tremaining: 4m 23s\n",
      "1200:\tlearn: 25.0415003\ttest: 37.0820230\tbest: 37.0820230 (1200)\ttotal: 36s\tremaining: 4m 23s\n",
      "1300:\tlearn: 24.2060608\ttest: 36.9783396\tbest: 36.9783396 (1300)\ttotal: 39.3s\tremaining: 4m 22s\n",
      "1400:\tlearn: 23.4547949\ttest: 36.8871159\tbest: 36.8871159 (1400)\ttotal: 42.7s\tremaining: 4m 22s\n",
      "1500:\tlearn: 22.8335505\ttest: 36.8061968\tbest: 36.8061968 (1500)\ttotal: 46.1s\tremaining: 4m 21s\n",
      "1600:\tlearn: 22.1702881\ttest: 36.7387082\tbest: 36.7387082 (1600)\ttotal: 49.5s\tremaining: 4m 19s\n",
      "1700:\tlearn: 21.5394578\ttest: 36.6807695\tbest: 36.6799396 (1699)\ttotal: 52.8s\tremaining: 4m 17s\n",
      "1800:\tlearn: 20.8794814\ttest: 36.6257672\tbest: 36.6241547 (1799)\ttotal: 56.3s\tremaining: 4m 16s\n",
      "1900:\tlearn: 20.3232951\ttest: 36.5688325\tbest: 36.5674900 (1897)\ttotal: 59.7s\tremaining: 4m 14s\n",
      "2000:\tlearn: 19.7700467\ttest: 36.5294288\tbest: 36.5287194 (1998)\ttotal: 1m 3s\tremaining: 4m 13s\n",
      "2100:\tlearn: 19.2386819\ttest: 36.4915784\tbest: 36.4915784 (2100)\ttotal: 1m 6s\tremaining: 4m 11s\n",
      "2200:\tlearn: 18.6950135\ttest: 36.4454020\tbest: 36.4435800 (2197)\ttotal: 1m 10s\tremaining: 4m 9s\n",
      "2300:\tlearn: 18.2222011\ttest: 36.4038527\tbest: 36.4038527 (2300)\ttotal: 1m 13s\tremaining: 4m 7s\n",
      "2400:\tlearn: 17.7558342\ttest: 36.3790723\tbest: 36.3790723 (2400)\ttotal: 1m 17s\tremaining: 4m 4s\n",
      "2500:\tlearn: 17.3588071\ttest: 36.3546361\tbest: 36.3531604 (2485)\ttotal: 1m 20s\tremaining: 4m 2s\n",
      "2600:\tlearn: 16.9131853\ttest: 36.3308881\tbest: 36.3303358 (2599)\ttotal: 1m 24s\tremaining: 3m 59s\n",
      "2700:\tlearn: 16.5167733\ttest: 36.3140649\tbest: 36.3140649 (2700)\ttotal: 1m 27s\tremaining: 3m 56s\n",
      "2800:\tlearn: 16.0965423\ttest: 36.2980620\tbest: 36.2949674 (2789)\ttotal: 1m 31s\tremaining: 3m 54s\n",
      "2900:\tlearn: 15.7139517\ttest: 36.2739113\tbest: 36.2732278 (2898)\ttotal: 1m 34s\tremaining: 3m 51s\n",
      "3000:\tlearn: 15.3384300\ttest: 36.2494106\tbest: 36.2493901 (2999)\ttotal: 1m 38s\tremaining: 3m 48s\n",
      "3100:\tlearn: 15.0260185\ttest: 36.2298649\tbest: 36.2298649 (3100)\ttotal: 1m 41s\tremaining: 3m 45s\n",
      "3200:\tlearn: 14.7038017\ttest: 36.2122879\tbest: 36.2122879 (3200)\ttotal: 1m 44s\tremaining: 3m 42s\n",
      "3300:\tlearn: 14.3721927\ttest: 36.2033348\tbest: 36.2019346 (3272)\ttotal: 1m 48s\tremaining: 3m 40s\n",
      "3400:\tlearn: 14.0792748\ttest: 36.1838382\tbest: 36.1836044 (3399)\ttotal: 1m 52s\tremaining: 3m 37s\n",
      "3500:\tlearn: 13.7990357\ttest: 36.1780921\tbest: 36.1774945 (3471)\ttotal: 1m 55s\tremaining: 3m 34s\n",
      "3600:\tlearn: 13.5282200\ttest: 36.1621248\tbest: 36.1621248 (3600)\ttotal: 1m 59s\tremaining: 3m 31s\n",
      "3700:\tlearn: 13.2594563\ttest: 36.1514836\tbest: 36.1492632 (3689)\ttotal: 2m 2s\tremaining: 3m 28s\n",
      "3800:\tlearn: 12.9753451\ttest: 36.1432513\tbest: 36.1424212 (3798)\ttotal: 2m 6s\tremaining: 3m 25s\n",
      "3900:\tlearn: 12.6721028\ttest: 36.1385806\tbest: 36.1363182 (3884)\ttotal: 2m 9s\tremaining: 3m 22s\n",
      "4000:\tlearn: 12.4341785\ttest: 36.1336836\tbest: 36.1333138 (3998)\ttotal: 2m 13s\tremaining: 3m 19s\n",
      "4100:\tlearn: 12.1722127\ttest: 36.1222212\tbest: 36.1211739 (4077)\ttotal: 2m 16s\tremaining: 3m 16s\n",
      "4200:\tlearn: 11.9132106\ttest: 36.1087948\tbest: 36.1087948 (4200)\ttotal: 2m 20s\tremaining: 3m 13s\n",
      "4300:\tlearn: 11.6875102\ttest: 36.1004300\tbest: 36.0993088 (4245)\ttotal: 2m 23s\tremaining: 3m 10s\n",
      "4400:\tlearn: 11.4462962\ttest: 36.0935277\tbest: 36.0925060 (4393)\ttotal: 2m 26s\tremaining: 3m 6s\n",
      "4500:\tlearn: 11.2428315\ttest: 36.0936694\tbest: 36.0908807 (4420)\ttotal: 2m 30s\tremaining: 3m 3s\n",
      "4600:\tlearn: 11.0268941\ttest: 36.0963873\tbest: 36.0898709 (4511)\ttotal: 2m 33s\tremaining: 3m\n",
      "4700:\tlearn: 10.7801380\ttest: 36.0918238\tbest: 36.0898709 (4511)\ttotal: 2m 37s\tremaining: 2m 57s\n",
      "4800:\tlearn: 10.5980470\ttest: 36.0843832\tbest: 36.0842584 (4799)\ttotal: 2m 41s\tremaining: 2m 54s\n",
      "4900:\tlearn: 10.3938145\ttest: 36.0766310\tbest: 36.0756729 (4898)\ttotal: 2m 44s\tremaining: 2m 51s\n",
      "5000:\tlearn: 10.2024905\ttest: 36.0686313\tbest: 36.0671188 (4991)\ttotal: 2m 48s\tremaining: 2m 48s\n",
      "5100:\tlearn: 10.0031946\ttest: 36.0672790\tbest: 36.0663767 (5067)\ttotal: 2m 51s\tremaining: 2m 44s\n",
      "5200:\tlearn: 9.8254114\ttest: 36.0644655\tbest: 36.0636098 (5195)\ttotal: 2m 55s\tremaining: 2m 41s\n",
      "5300:\tlearn: 9.6612663\ttest: 36.0593095\tbest: 36.0593095 (5300)\ttotal: 2m 59s\tremaining: 2m 38s\n",
      "5400:\tlearn: 9.4987080\ttest: 36.0596317\tbest: 36.0569621 (5337)\ttotal: 3m 2s\tremaining: 2m 35s\n",
      "5500:\tlearn: 9.3255172\ttest: 36.0624607\tbest: 36.0569621 (5337)\ttotal: 3m 6s\tremaining: 2m 32s\n",
      "5600:\tlearn: 9.1691211\ttest: 36.0636531\tbest: 36.0569621 (5337)\ttotal: 3m 9s\tremaining: 2m 28s\n",
      "5700:\tlearn: 9.0143938\ttest: 36.0662827\tbest: 36.0569621 (5337)\ttotal: 3m 13s\tremaining: 2m 25s\n",
      "5800:\tlearn: 8.8652529\ttest: 36.0684153\tbest: 36.0569621 (5337)\ttotal: 3m 16s\tremaining: 2m 22s\n",
      "Stopped by overfitting detector  (500 iterations wait)\n",
      "\n",
      "bestTest = 36.05696207\n",
      "bestIteration = 5337\n",
      "\n",
      "Shrink model to first 5338 iterations.\n",
      "[0]\tvalidation_0-rmse:63.17007\tvalidation_0-root_mean_squared_error:63.17006\n",
      "[100]\tvalidation_0-rmse:47.39228\tvalidation_0-root_mean_squared_error:47.39228\n",
      "[200]\tvalidation_0-rmse:42.62799\tvalidation_0-root_mean_squared_error:42.62799\n",
      "[300]\tvalidation_0-rmse:40.77468\tvalidation_0-root_mean_squared_error:40.77468\n",
      "[400]\tvalidation_0-rmse:39.83632\tvalidation_0-root_mean_squared_error:39.83632\n",
      "[500]\tvalidation_0-rmse:39.31773\tvalidation_0-root_mean_squared_error:39.31773\n",
      "[600]\tvalidation_0-rmse:39.00753\tvalidation_0-root_mean_squared_error:39.00753\n",
      "[700]\tvalidation_0-rmse:38.80582\tvalidation_0-root_mean_squared_error:38.80582\n",
      "[800]\tvalidation_0-rmse:38.63751\tvalidation_0-root_mean_squared_error:38.63751\n",
      "[900]\tvalidation_0-rmse:38.49718\tvalidation_0-root_mean_squared_error:38.49718\n",
      "[1000]\tvalidation_0-rmse:38.39796\tvalidation_0-root_mean_squared_error:38.39796\n",
      "[1100]\tvalidation_0-rmse:38.32680\tvalidation_0-root_mean_squared_error:38.32681\n",
      "[1200]\tvalidation_0-rmse:38.25359\tvalidation_0-root_mean_squared_error:38.25359\n",
      "[1300]\tvalidation_0-rmse:38.18207\tvalidation_0-root_mean_squared_error:38.18208\n",
      "[1400]\tvalidation_0-rmse:38.12642\tvalidation_0-root_mean_squared_error:38.12642\n",
      "[1500]\tvalidation_0-rmse:38.07561\tvalidation_0-root_mean_squared_error:38.07561\n",
      "[1600]\tvalidation_0-rmse:38.03486\tvalidation_0-root_mean_squared_error:38.03486\n",
      "[1700]\tvalidation_0-rmse:37.99129\tvalidation_0-root_mean_squared_error:37.99130\n",
      "[1800]\tvalidation_0-rmse:37.95093\tvalidation_0-root_mean_squared_error:37.95093\n",
      "[1900]\tvalidation_0-rmse:37.92233\tvalidation_0-root_mean_squared_error:37.92233\n",
      "[2000]\tvalidation_0-rmse:37.85929\tvalidation_0-root_mean_squared_error:37.85929\n",
      "[2100]\tvalidation_0-rmse:37.81474\tvalidation_0-root_mean_squared_error:37.81474\n",
      "[2200]\tvalidation_0-rmse:37.79602\tvalidation_0-root_mean_squared_error:37.79602\n",
      "[2300]\tvalidation_0-rmse:37.76290\tvalidation_0-root_mean_squared_error:37.76290\n",
      "[2400]\tvalidation_0-rmse:37.71910\tvalidation_0-root_mean_squared_error:37.71909\n",
      "[2500]\tvalidation_0-rmse:37.67392\tvalidation_0-root_mean_squared_error:37.67392\n",
      "[2600]\tvalidation_0-rmse:37.65547\tvalidation_0-root_mean_squared_error:37.65547\n",
      "[2700]\tvalidation_0-rmse:37.63159\tvalidation_0-root_mean_squared_error:37.63159\n",
      "[2800]\tvalidation_0-rmse:37.62763\tvalidation_0-root_mean_squared_error:37.62763\n",
      "[2900]\tvalidation_0-rmse:37.62054\tvalidation_0-root_mean_squared_error:37.62054\n",
      "[3000]\tvalidation_0-rmse:37.60133\tvalidation_0-root_mean_squared_error:37.60133\n",
      "[3100]\tvalidation_0-rmse:37.58648\tvalidation_0-root_mean_squared_error:37.58648\n",
      "[3200]\tvalidation_0-rmse:37.56693\tvalidation_0-root_mean_squared_error:37.56693\n",
      "[3300]\tvalidation_0-rmse:37.55697\tvalidation_0-root_mean_squared_error:37.55697\n",
      "[3400]\tvalidation_0-rmse:37.53713\tvalidation_0-root_mean_squared_error:37.53713\n",
      "[3500]\tvalidation_0-rmse:37.53162\tvalidation_0-root_mean_squared_error:37.53162\n",
      "[3600]\tvalidation_0-rmse:37.52625\tvalidation_0-root_mean_squared_error:37.52625\n",
      "[3700]\tvalidation_0-rmse:37.52653\tvalidation_0-root_mean_squared_error:37.52653\n",
      "[3800]\tvalidation_0-rmse:37.53527\tvalidation_0-root_mean_squared_error:37.53527\n",
      "[3900]\tvalidation_0-rmse:37.53299\tvalidation_0-root_mean_squared_error:37.53299\n",
      "[4000]\tvalidation_0-rmse:37.53233\tvalidation_0-root_mean_squared_error:37.53233\n",
      "[4100]\tvalidation_0-rmse:37.51143\tvalidation_0-root_mean_squared_error:37.51143\n",
      "[4200]\tvalidation_0-rmse:37.51188\tvalidation_0-root_mean_squared_error:37.51188\n",
      "[4300]\tvalidation_0-rmse:37.50891\tvalidation_0-root_mean_squared_error:37.50891\n",
      "[4400]\tvalidation_0-rmse:37.50781\tvalidation_0-root_mean_squared_error:37.50781\n",
      "[4500]\tvalidation_0-rmse:37.49228\tvalidation_0-root_mean_squared_error:37.49228\n",
      "[4600]\tvalidation_0-rmse:37.47020\tvalidation_0-root_mean_squared_error:37.47020\n",
      "[4700]\tvalidation_0-rmse:37.46580\tvalidation_0-root_mean_squared_error:37.46580\n",
      "[4800]\tvalidation_0-rmse:37.47249\tvalidation_0-root_mean_squared_error:37.47248\n",
      "[4900]\tvalidation_0-rmse:37.48039\tvalidation_0-root_mean_squared_error:37.48039\n",
      "[5000]\tvalidation_0-rmse:37.49710\tvalidation_0-root_mean_squared_error:37.49710\n",
      "[5100]\tvalidation_0-rmse:37.50735\tvalidation_0-root_mean_squared_error:37.50735\n",
      "[5200]\tvalidation_0-rmse:37.50806\tvalidation_0-root_mean_squared_error:37.50806\n",
      "[5228]\tvalidation_0-rmse:37.50314\tvalidation_0-root_mean_squared_error:37.50315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [12:12, 358.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest rmse:  37.0849290685524\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001331 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1574\n",
      "[LightGBM] [Info] Number of data points in the train set: 3474, number of used features: 184\n",
      "[LightGBM] [Info] Start training from score 156.375360\n",
      "Training until validation scores don't improve for 500 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/spa-d4/grad/mfe261/Projects/PREPARE/envs/lib/python3.9/site-packages/lightgbm/engine.py:204: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2365]\tvalid_0's rmse: 38.977\n",
      "0:\tlearn: 60.9011270\ttest: 59.6063548\tbest: 59.6063548 (0)\ttotal: 23.2ms\tremaining: 3m 51s\n",
      "100:\tlearn: 45.0866310\ttest: 46.6020960\tbest: 46.6020960 (100)\ttotal: 2.51s\tremaining: 4m 6s\n",
      "200:\tlearn: 38.8123756\ttest: 42.7566805\tbest: 42.7566805 (200)\ttotal: 5.39s\tremaining: 4m 22s\n",
      "300:\tlearn: 35.1670499\ttest: 41.0863368\tbest: 41.0863368 (300)\ttotal: 8.45s\tremaining: 4m 32s\n",
      "400:\tlearn: 32.7953003\ttest: 40.3183533\tbest: 40.3183533 (400)\ttotal: 11.4s\tremaining: 4m 32s\n",
      "500:\tlearn: 31.0434299\ttest: 39.8397078\tbest: 39.8397078 (500)\ttotal: 14.4s\tremaining: 4m 33s\n",
      "600:\tlearn: 29.7852070\ttest: 39.5566473\tbest: 39.5566473 (600)\ttotal: 17.4s\tremaining: 4m 31s\n",
      "700:\tlearn: 28.7523588\ttest: 39.3672862\tbest: 39.3672862 (700)\ttotal: 20.2s\tremaining: 4m 28s\n",
      "800:\tlearn: 27.7153668\ttest: 39.1789621\tbest: 39.1789621 (800)\ttotal: 23.2s\tremaining: 4m 25s\n",
      "900:\tlearn: 26.8477939\ttest: 39.0294404\tbest: 39.0284130 (898)\ttotal: 26.3s\tremaining: 4m 25s\n",
      "1000:\tlearn: 26.0117468\ttest: 38.9229916\tbest: 38.9229916 (1000)\ttotal: 29.7s\tremaining: 4m 27s\n",
      "1100:\tlearn: 25.1412216\ttest: 38.8023264\tbest: 38.8022895 (1099)\ttotal: 33.1s\tremaining: 4m 27s\n",
      "1200:\tlearn: 24.3124954\ttest: 38.6969107\tbest: 38.6969107 (1200)\ttotal: 36.4s\tremaining: 4m 26s\n",
      "1300:\tlearn: 23.5796471\ttest: 38.6056001\tbest: 38.6056001 (1300)\ttotal: 39.7s\tremaining: 4m 25s\n",
      "1400:\tlearn: 22.8754128\ttest: 38.5227921\tbest: 38.5227921 (1400)\ttotal: 43s\tremaining: 4m 24s\n",
      "1500:\tlearn: 22.2461574\ttest: 38.4730581\tbest: 38.4726585 (1496)\ttotal: 46.4s\tremaining: 4m 23s\n",
      "1600:\tlearn: 21.6341953\ttest: 38.3919327\tbest: 38.3874315 (1592)\ttotal: 49.7s\tremaining: 4m 20s\n",
      "1700:\tlearn: 20.9524254\ttest: 38.3445498\tbest: 38.3393222 (1687)\ttotal: 53.2s\tremaining: 4m 19s\n",
      "1800:\tlearn: 20.4169421\ttest: 38.3086127\tbest: 38.3086127 (1800)\ttotal: 56.6s\tremaining: 4m 17s\n",
      "1900:\tlearn: 19.9165695\ttest: 38.2688423\tbest: 38.2688423 (1900)\ttotal: 60s\tremaining: 4m 15s\n",
      "2000:\tlearn: 19.3684638\ttest: 38.2403728\tbest: 38.2403728 (2000)\ttotal: 1m 3s\tremaining: 4m 13s\n",
      "2100:\tlearn: 18.8605487\ttest: 38.1832520\tbest: 38.1828060 (2099)\ttotal: 1m 6s\tremaining: 4m 11s\n",
      "2200:\tlearn: 18.3568176\ttest: 38.1500586\tbest: 38.1500586 (2200)\ttotal: 1m 10s\tremaining: 4m 8s\n",
      "2300:\tlearn: 17.9146606\ttest: 38.1178445\tbest: 38.1178445 (2300)\ttotal: 1m 13s\tremaining: 4m 6s\n",
      "2400:\tlearn: 17.4933766\ttest: 38.0875947\tbest: 38.0862034 (2394)\ttotal: 1m 17s\tremaining: 4m 4s\n",
      "2500:\tlearn: 17.0456942\ttest: 38.0689714\tbest: 38.0658154 (2492)\ttotal: 1m 20s\tremaining: 4m 1s\n",
      "2600:\tlearn: 16.6630474\ttest: 38.0478217\tbest: 38.0473473 (2598)\ttotal: 1m 23s\tremaining: 3m 58s\n",
      "2700:\tlearn: 16.2372086\ttest: 38.0150772\tbest: 38.0150772 (2700)\ttotal: 1m 27s\tremaining: 3m 55s\n",
      "2800:\tlearn: 15.8209723\ttest: 37.9833750\tbest: 37.9833750 (2800)\ttotal: 1m 30s\tremaining: 3m 53s\n",
      "2900:\tlearn: 15.4599834\ttest: 37.9597023\tbest: 37.9595530 (2899)\ttotal: 1m 34s\tremaining: 3m 50s\n",
      "3000:\tlearn: 15.1227124\ttest: 37.9407108\tbest: 37.9400501 (2999)\ttotal: 1m 37s\tremaining: 3m 47s\n",
      "3100:\tlearn: 14.7767706\ttest: 37.9247651\tbest: 37.9242502 (3095)\ttotal: 1m 41s\tremaining: 3m 45s\n",
      "3200:\tlearn: 14.4553050\ttest: 37.9018157\tbest: 37.9018157 (3200)\ttotal: 1m 44s\tremaining: 3m 42s\n",
      "3300:\tlearn: 14.0999993\ttest: 37.8762824\tbest: 37.8758722 (3298)\ttotal: 1m 48s\tremaining: 3m 39s\n",
      "3400:\tlearn: 13.8017384\ttest: 37.8602580\tbest: 37.8590199 (3380)\ttotal: 1m 51s\tremaining: 3m 36s\n",
      "3500:\tlearn: 13.5117328\ttest: 37.8472732\tbest: 37.8469034 (3495)\ttotal: 1m 55s\tremaining: 3m 33s\n",
      "3600:\tlearn: 13.2426024\ttest: 37.8335630\tbest: 37.8332420 (3593)\ttotal: 1m 58s\tremaining: 3m 30s\n",
      "3700:\tlearn: 12.9595998\ttest: 37.8223007\tbest: 37.8199686 (3680)\ttotal: 2m 2s\tremaining: 3m 27s\n",
      "3800:\tlearn: 12.7019068\ttest: 37.8181810\tbest: 37.8148286 (3770)\ttotal: 2m 5s\tremaining: 3m 24s\n",
      "3900:\tlearn: 12.4618588\ttest: 37.8068958\tbest: 37.8060119 (3890)\ttotal: 2m 8s\tremaining: 3m 21s\n",
      "4000:\tlearn: 12.2032938\ttest: 37.7962754\tbest: 37.7962754 (4000)\ttotal: 2m 12s\tremaining: 3m 18s\n",
      "4100:\tlearn: 11.9220021\ttest: 37.7828329\tbest: 37.7819692 (4077)\ttotal: 2m 15s\tremaining: 3m 15s\n",
      "4200:\tlearn: 11.6767652\ttest: 37.7711709\tbest: 37.7707408 (4147)\ttotal: 2m 19s\tremaining: 3m 12s\n",
      "4300:\tlearn: 11.4156869\ttest: 37.7567637\tbest: 37.7549568 (4289)\ttotal: 2m 22s\tremaining: 3m 9s\n",
      "4400:\tlearn: 11.1737964\ttest: 37.7411988\tbest: 37.7411988 (4400)\ttotal: 2m 26s\tremaining: 3m 6s\n",
      "4500:\tlearn: 10.9511187\ttest: 37.7403713\tbest: 37.7384113 (4469)\ttotal: 2m 29s\tremaining: 3m 3s\n",
      "4600:\tlearn: 10.7616043\ttest: 37.7394338\tbest: 37.7384113 (4469)\ttotal: 2m 33s\tremaining: 3m\n",
      "4700:\tlearn: 10.5913173\ttest: 37.7362767\tbest: 37.7340532 (4682)\ttotal: 2m 37s\tremaining: 2m 57s\n",
      "4800:\tlearn: 10.3800510\ttest: 37.7312292\tbest: 37.7302589 (4792)\ttotal: 2m 40s\tremaining: 2m 53s\n",
      "4900:\tlearn: 10.1841125\ttest: 37.7235028\tbest: 37.7231500 (4898)\ttotal: 2m 44s\tremaining: 2m 50s\n",
      "5000:\tlearn: 10.0074671\ttest: 37.7153314\tbest: 37.7153314 (5000)\ttotal: 2m 48s\tremaining: 2m 48s\n",
      "5100:\tlearn: 9.8127645\ttest: 37.7048075\tbest: 37.7037462 (5086)\ttotal: 2m 51s\tremaining: 2m 44s\n",
      "5200:\tlearn: 9.6182264\ttest: 37.7121722\tbest: 37.7037462 (5086)\ttotal: 2m 55s\tremaining: 2m 41s\n",
      "5300:\tlearn: 9.4349116\ttest: 37.7172704\tbest: 37.7037462 (5086)\ttotal: 2m 58s\tremaining: 2m 38s\n",
      "5400:\tlearn: 9.2539739\ttest: 37.7145230\tbest: 37.7037462 (5086)\ttotal: 3m 2s\tremaining: 2m 35s\n",
      "5500:\tlearn: 9.0921935\ttest: 37.7107864\tbest: 37.7037462 (5086)\ttotal: 3m 5s\tremaining: 2m 32s\n",
      "Stopped by overfitting detector  (500 iterations wait)\n",
      "\n",
      "bestTest = 37.70374621\n",
      "bestIteration = 5086\n",
      "\n",
      "Shrink model to first 5087 iterations.\n",
      "[0]\tvalidation_0-rmse:59.59052\tvalidation_0-root_mean_squared_error:59.59052\n",
      "[100]\tvalidation_0-rmse:46.84483\tvalidation_0-root_mean_squared_error:46.84483\n",
      "[200]\tvalidation_0-rmse:43.32111\tvalidation_0-root_mean_squared_error:43.32111\n",
      "[300]\tvalidation_0-rmse:41.85541\tvalidation_0-root_mean_squared_error:41.85541\n",
      "[400]\tvalidation_0-rmse:41.14360\tvalidation_0-root_mean_squared_error:41.14360\n",
      "[500]\tvalidation_0-rmse:40.73692\tvalidation_0-root_mean_squared_error:40.73692\n",
      "[600]\tvalidation_0-rmse:40.48219\tvalidation_0-root_mean_squared_error:40.48219\n",
      "[700]\tvalidation_0-rmse:40.33168\tvalidation_0-root_mean_squared_error:40.33168\n",
      "[800]\tvalidation_0-rmse:40.19536\tvalidation_0-root_mean_squared_error:40.19536\n",
      "[900]\tvalidation_0-rmse:40.10266\tvalidation_0-root_mean_squared_error:40.10265\n",
      "[1000]\tvalidation_0-rmse:40.03159\tvalidation_0-root_mean_squared_error:40.03159\n",
      "[1100]\tvalidation_0-rmse:39.97316\tvalidation_0-root_mean_squared_error:39.97316\n",
      "[1200]\tvalidation_0-rmse:39.91156\tvalidation_0-root_mean_squared_error:39.91156\n",
      "[1300]\tvalidation_0-rmse:39.86706\tvalidation_0-root_mean_squared_error:39.86706\n",
      "[1400]\tvalidation_0-rmse:39.82785\tvalidation_0-root_mean_squared_error:39.82785\n",
      "[1500]\tvalidation_0-rmse:39.78978\tvalidation_0-root_mean_squared_error:39.78978\n",
      "[1600]\tvalidation_0-rmse:39.74252\tvalidation_0-root_mean_squared_error:39.74252\n",
      "[1700]\tvalidation_0-rmse:39.71850\tvalidation_0-root_mean_squared_error:39.71850\n",
      "[1800]\tvalidation_0-rmse:39.68936\tvalidation_0-root_mean_squared_error:39.68936\n",
      "[1900]\tvalidation_0-rmse:39.65974\tvalidation_0-root_mean_squared_error:39.65974\n",
      "[2000]\tvalidation_0-rmse:39.62421\tvalidation_0-root_mean_squared_error:39.62421\n",
      "[2100]\tvalidation_0-rmse:39.60185\tvalidation_0-root_mean_squared_error:39.60185\n",
      "[2200]\tvalidation_0-rmse:39.57988\tvalidation_0-root_mean_squared_error:39.57988\n",
      "[2300]\tvalidation_0-rmse:39.56201\tvalidation_0-root_mean_squared_error:39.56201\n",
      "[2400]\tvalidation_0-rmse:39.56097\tvalidation_0-root_mean_squared_error:39.56097\n",
      "[2500]\tvalidation_0-rmse:39.55311\tvalidation_0-root_mean_squared_error:39.55312\n",
      "[2600]\tvalidation_0-rmse:39.54382\tvalidation_0-root_mean_squared_error:39.54382\n",
      "[2700]\tvalidation_0-rmse:39.54075\tvalidation_0-root_mean_squared_error:39.54075\n",
      "[2800]\tvalidation_0-rmse:39.53334\tvalidation_0-root_mean_squared_error:39.53334\n",
      "[2900]\tvalidation_0-rmse:39.52310\tvalidation_0-root_mean_squared_error:39.52310\n",
      "[3000]\tvalidation_0-rmse:39.50969\tvalidation_0-root_mean_squared_error:39.50969\n",
      "[3100]\tvalidation_0-rmse:39.49135\tvalidation_0-root_mean_squared_error:39.49135\n",
      "[3200]\tvalidation_0-rmse:39.47893\tvalidation_0-root_mean_squared_error:39.47893\n",
      "[3300]\tvalidation_0-rmse:39.47546\tvalidation_0-root_mean_squared_error:39.47546\n",
      "[3400]\tvalidation_0-rmse:39.46824\tvalidation_0-root_mean_squared_error:39.46824\n",
      "[3500]\tvalidation_0-rmse:39.45496\tvalidation_0-root_mean_squared_error:39.45496\n",
      "[3600]\tvalidation_0-rmse:39.44522\tvalidation_0-root_mean_squared_error:39.44522\n",
      "[3700]\tvalidation_0-rmse:39.43554\tvalidation_0-root_mean_squared_error:39.43554\n",
      "[3800]\tvalidation_0-rmse:39.41364\tvalidation_0-root_mean_squared_error:39.41363\n",
      "[3900]\tvalidation_0-rmse:39.39158\tvalidation_0-root_mean_squared_error:39.39158\n",
      "[4000]\tvalidation_0-rmse:39.38294\tvalidation_0-root_mean_squared_error:39.38295\n",
      "[4100]\tvalidation_0-rmse:39.36906\tvalidation_0-root_mean_squared_error:39.36906\n",
      "[4200]\tvalidation_0-rmse:39.35844\tvalidation_0-root_mean_squared_error:39.35844\n",
      "[4300]\tvalidation_0-rmse:39.34246\tvalidation_0-root_mean_squared_error:39.34246\n",
      "[4400]\tvalidation_0-rmse:39.32275\tvalidation_0-root_mean_squared_error:39.32275\n",
      "[4500]\tvalidation_0-rmse:39.32744\tvalidation_0-root_mean_squared_error:39.32744\n",
      "[4600]\tvalidation_0-rmse:39.32737\tvalidation_0-root_mean_squared_error:39.32737\n",
      "[4700]\tvalidation_0-rmse:39.33073\tvalidation_0-root_mean_squared_error:39.33073\n",
      "[4800]\tvalidation_0-rmse:39.32244\tvalidation_0-root_mean_squared_error:39.32243\n",
      "[4900]\tvalidation_0-rmse:39.32736\tvalidation_0-root_mean_squared_error:39.32737\n",
      "[4967]\tvalidation_0-rmse:39.33121\tvalidation_0-root_mean_squared_error:39.33122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [17:26, 338.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest rmse:  39.2325107880024\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000911 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1571\n",
      "[LightGBM] [Info] Number of data points in the train set: 3475, number of used features: 184\n",
      "[LightGBM] [Info] Start training from score 157.237122\n",
      "Training until validation scores don't improve for 500 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/spa-d4/grad/mfe261/Projects/PREPARE/envs/lib/python3.9/site-packages/lightgbm/engine.py:204: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[3242]\tvalid_0's rmse: 36.4052\n",
      "0:\tlearn: 60.6014303\ttest: 60.7946485\tbest: 60.7946485 (0)\ttotal: 26.2ms\tremaining: 4m 22s\n",
      "100:\tlearn: 45.1270455\ttest: 46.8539846\tbest: 46.8539846 (100)\ttotal: 2.43s\tremaining: 3m 58s\n",
      "200:\tlearn: 38.8983723\ttest: 42.3812053\tbest: 42.3812053 (200)\ttotal: 5.45s\tremaining: 4m 25s\n",
      "300:\tlearn: 35.4510649\ttest: 40.6666022\tbest: 40.6666022 (300)\ttotal: 8.46s\tremaining: 4m 32s\n",
      "400:\tlearn: 33.2082143\ttest: 39.8262530\tbest: 39.8262530 (400)\ttotal: 11.4s\tremaining: 4m 32s\n",
      "500:\tlearn: 31.5916940\ttest: 39.3422105\tbest: 39.3422105 (500)\ttotal: 14.3s\tremaining: 4m 30s\n",
      "600:\tlearn: 30.3353372\ttest: 39.0297971\tbest: 39.0297971 (600)\ttotal: 17.3s\tremaining: 4m 30s\n",
      "700:\tlearn: 29.1521855\ttest: 38.7589983\tbest: 38.7589983 (700)\ttotal: 20.3s\tremaining: 4m 28s\n",
      "800:\tlearn: 28.0347992\ttest: 38.5510221\tbest: 38.5510221 (800)\ttotal: 23.2s\tremaining: 4m 26s\n",
      "900:\tlearn: 27.1562471\ttest: 38.3687485\tbest: 38.3687485 (900)\ttotal: 26.4s\tremaining: 4m 26s\n",
      "1000:\tlearn: 26.4581270\ttest: 38.2381800\tbest: 38.2381243 (999)\ttotal: 29.8s\tremaining: 4m 28s\n",
      "1100:\tlearn: 25.5854458\ttest: 38.1137031\tbest: 38.1137031 (1100)\ttotal: 33.3s\tremaining: 4m 28s\n",
      "1200:\tlearn: 24.7689243\ttest: 37.9843686\tbest: 37.9832513 (1199)\ttotal: 36.9s\tremaining: 4m 30s\n",
      "1300:\tlearn: 23.9321596\ttest: 37.8767000\tbest: 37.8767000 (1300)\ttotal: 40.3s\tremaining: 4m 29s\n",
      "1400:\tlearn: 23.1616032\ttest: 37.7823674\tbest: 37.7804230 (1397)\ttotal: 43.7s\tremaining: 4m 28s\n",
      "1500:\tlearn: 22.4709566\ttest: 37.7061162\tbest: 37.7061162 (1500)\ttotal: 47.1s\tremaining: 4m 26s\n",
      "1600:\tlearn: 21.8346453\ttest: 37.6415297\tbest: 37.6415297 (1600)\ttotal: 50.4s\tremaining: 4m 24s\n",
      "1700:\tlearn: 21.2163242\ttest: 37.5639627\tbest: 37.5608589 (1691)\ttotal: 53.8s\tremaining: 4m 22s\n",
      "1800:\tlearn: 20.6386361\ttest: 37.4895171\tbest: 37.4893970 (1794)\ttotal: 57.3s\tremaining: 4m 20s\n",
      "1900:\tlearn: 20.1191764\ttest: 37.4305222\tbest: 37.4304886 (1898)\ttotal: 1m\tremaining: 4m 18s\n",
      "2000:\tlearn: 19.5755088\ttest: 37.3771629\tbest: 37.3770653 (1999)\ttotal: 1m 4s\tremaining: 4m 16s\n",
      "2100:\tlearn: 19.1370312\ttest: 37.3111661\tbest: 37.3111661 (2100)\ttotal: 1m 7s\tremaining: 4m 14s\n",
      "2200:\tlearn: 18.6629781\ttest: 37.2623255\tbest: 37.2616874 (2199)\ttotal: 1m 11s\tremaining: 4m 12s\n",
      "2300:\tlearn: 18.1769420\ttest: 37.2325255\tbest: 37.2325255 (2300)\ttotal: 1m 14s\tremaining: 4m 10s\n",
      "2400:\tlearn: 17.7596631\ttest: 37.1827632\tbest: 37.1827632 (2400)\ttotal: 1m 18s\tremaining: 4m 7s\n",
      "2500:\tlearn: 17.3478407\ttest: 37.1452628\tbest: 37.1452628 (2500)\ttotal: 1m 21s\tremaining: 4m 5s\n",
      "2600:\tlearn: 16.9706992\ttest: 37.1098731\tbest: 37.1087662 (2590)\ttotal: 1m 25s\tremaining: 4m 2s\n",
      "2700:\tlearn: 16.5905001\ttest: 37.0705512\tbest: 37.0692389 (2698)\ttotal: 1m 28s\tremaining: 4m\n",
      "2800:\tlearn: 16.2140805\ttest: 37.0445987\tbest: 37.0445987 (2800)\ttotal: 1m 32s\tremaining: 3m 57s\n",
      "2900:\tlearn: 15.8330971\ttest: 37.0122717\tbest: 37.0116863 (2896)\ttotal: 1m 35s\tremaining: 3m 54s\n",
      "3000:\tlearn: 15.5329802\ttest: 36.9897535\tbest: 36.9877548 (2995)\ttotal: 1m 39s\tremaining: 3m 51s\n",
      "3100:\tlearn: 15.1945900\ttest: 36.9671946\tbest: 36.9666859 (3098)\ttotal: 1m 42s\tremaining: 3m 48s\n",
      "3200:\tlearn: 14.8775292\ttest: 36.9442438\tbest: 36.9441853 (3199)\ttotal: 1m 46s\tremaining: 3m 46s\n",
      "3300:\tlearn: 14.5613022\ttest: 36.9241415\tbest: 36.9241415 (3300)\ttotal: 1m 50s\tremaining: 3m 43s\n",
      "3400:\tlearn: 14.2363017\ttest: 36.9021560\tbest: 36.9017708 (3398)\ttotal: 1m 53s\tremaining: 3m 40s\n",
      "3500:\tlearn: 13.9356641\ttest: 36.8777251\tbest: 36.8739158 (3487)\ttotal: 1m 57s\tremaining: 3m 37s\n",
      "3600:\tlearn: 13.6612619\ttest: 36.8573423\tbest: 36.8570430 (3597)\ttotal: 2m\tremaining: 3m 34s\n",
      "3700:\tlearn: 13.4043827\ttest: 36.8364539\tbest: 36.8356163 (3688)\ttotal: 2m 4s\tremaining: 3m 31s\n",
      "3800:\tlearn: 13.1592913\ttest: 36.8279728\tbest: 36.8259537 (3795)\ttotal: 2m 7s\tremaining: 3m 28s\n",
      "3900:\tlearn: 12.8966955\ttest: 36.8007619\tbest: 36.8006865 (3898)\ttotal: 2m 11s\tremaining: 3m 25s\n",
      "4000:\tlearn: 12.6652655\ttest: 36.7929796\tbest: 36.7929771 (3999)\ttotal: 2m 14s\tremaining: 3m 22s\n",
      "4100:\tlearn: 12.4538106\ttest: 36.7900857\tbest: 36.7891689 (4072)\ttotal: 2m 18s\tremaining: 3m 19s\n",
      "4200:\tlearn: 12.2412203\ttest: 36.7742643\tbest: 36.7742643 (4200)\ttotal: 2m 22s\tremaining: 3m 16s\n",
      "4300:\tlearn: 12.0506642\ttest: 36.7644039\tbest: 36.7644039 (4300)\ttotal: 2m 25s\tremaining: 3m 12s\n",
      "4400:\tlearn: 11.8237068\ttest: 36.7501552\tbest: 36.7501552 (4400)\ttotal: 2m 28s\tremaining: 3m 9s\n",
      "4500:\tlearn: 11.6213539\ttest: 36.7385301\tbest: 36.7385031 (4498)\ttotal: 2m 32s\tremaining: 3m 6s\n",
      "4600:\tlearn: 11.4329248\ttest: 36.7281049\tbest: 36.7280531 (4599)\ttotal: 2m 35s\tremaining: 3m 2s\n",
      "4700:\tlearn: 11.2219926\ttest: 36.7240127\tbest: 36.7240127 (4700)\ttotal: 2m 39s\tremaining: 2m 59s\n",
      "4800:\tlearn: 11.0282549\ttest: 36.7120584\tbest: 36.7119876 (4799)\ttotal: 2m 42s\tremaining: 2m 56s\n",
      "4900:\tlearn: 10.8276526\ttest: 36.7074969\tbest: 36.7063736 (4878)\ttotal: 2m 46s\tremaining: 2m 53s\n",
      "5000:\tlearn: 10.6436831\ttest: 36.7028296\tbest: 36.7028296 (5000)\ttotal: 2m 49s\tremaining: 2m 49s\n",
      "5100:\tlearn: 10.4580773\ttest: 36.7051562\tbest: 36.7002875 (5040)\ttotal: 2m 53s\tremaining: 2m 46s\n",
      "5200:\tlearn: 10.2866869\ttest: 36.6951654\tbest: 36.6947332 (5177)\ttotal: 2m 56s\tremaining: 2m 43s\n",
      "5300:\tlearn: 10.1162651\ttest: 36.6919478\tbest: 36.6910710 (5238)\ttotal: 3m\tremaining: 2m 39s\n",
      "5400:\tlearn: 9.9255600\ttest: 36.6845726\tbest: 36.6830585 (5393)\ttotal: 3m 4s\tremaining: 2m 36s\n",
      "5500:\tlearn: 9.7605600\ttest: 36.6798339\tbest: 36.6798339 (5500)\ttotal: 3m 7s\tremaining: 2m 33s\n",
      "5600:\tlearn: 9.5849614\ttest: 36.6788941\tbest: 36.6775012 (5578)\ttotal: 3m 11s\tremaining: 2m 30s\n",
      "5700:\tlearn: 9.4401822\ttest: 36.6712281\tbest: 36.6712192 (5695)\ttotal: 3m 14s\tremaining: 2m 26s\n",
      "5800:\tlearn: 9.2909468\ttest: 36.6688823\tbest: 36.6671639 (5762)\ttotal: 3m 18s\tremaining: 2m 23s\n",
      "5900:\tlearn: 9.1345595\ttest: 36.6641962\tbest: 36.6641962 (5900)\ttotal: 3m 21s\tremaining: 2m 20s\n",
      "6000:\tlearn: 8.9934418\ttest: 36.6576015\tbest: 36.6576015 (6000)\ttotal: 3m 25s\tremaining: 2m 16s\n",
      "6100:\tlearn: 8.8556960\ttest: 36.6557343\tbest: 36.6555999 (6088)\ttotal: 3m 28s\tremaining: 2m 13s\n",
      "6200:\tlearn: 8.7082846\ttest: 36.6491212\tbest: 36.6477071 (6175)\ttotal: 3m 32s\tremaining: 2m 9s\n",
      "6300:\tlearn: 8.5845584\ttest: 36.6484202\tbest: 36.6467004 (6293)\ttotal: 3m 35s\tremaining: 2m 6s\n",
      "6400:\tlearn: 8.4465218\ttest: 36.6428712\tbest: 36.6425464 (6391)\ttotal: 3m 39s\tremaining: 2m 3s\n",
      "6500:\tlearn: 8.3383799\ttest: 36.6434288\tbest: 36.6408927 (6450)\ttotal: 3m 42s\tremaining: 1m 59s\n",
      "6600:\tlearn: 8.2067574\ttest: 36.6416601\tbest: 36.6408927 (6450)\ttotal: 3m 45s\tremaining: 1m 56s\n",
      "6700:\tlearn: 8.1085612\ttest: 36.6354320\tbest: 36.6353304 (6696)\ttotal: 3m 49s\tremaining: 1m 53s\n",
      "6800:\tlearn: 7.9783983\ttest: 36.6277310\tbest: 36.6277019 (6793)\ttotal: 3m 53s\tremaining: 1m 49s\n",
      "6900:\tlearn: 7.8622867\ttest: 36.6180207\tbest: 36.6168844 (6871)\ttotal: 3m 56s\tremaining: 1m 46s\n",
      "7000:\tlearn: 7.7595229\ttest: 36.6150327\tbest: 36.6146303 (6995)\ttotal: 4m\tremaining: 1m 42s\n",
      "7100:\tlearn: 7.6438598\ttest: 36.6132803\tbest: 36.6123065 (7081)\ttotal: 4m 3s\tremaining: 1m 39s\n",
      "7200:\tlearn: 7.5437654\ttest: 36.6130693\tbest: 36.6119776 (7177)\ttotal: 4m 7s\tremaining: 1m 36s\n",
      "7300:\tlearn: 7.4424097\ttest: 36.6130795\tbest: 36.6119776 (7177)\ttotal: 4m 10s\tremaining: 1m 32s\n",
      "7400:\tlearn: 7.3409543\ttest: 36.6133176\tbest: 36.6119776 (7177)\ttotal: 4m 14s\tremaining: 1m 29s\n",
      "7500:\tlearn: 7.2486599\ttest: 36.6141752\tbest: 36.6117319 (7436)\ttotal: 4m 17s\tremaining: 1m 25s\n",
      "7600:\tlearn: 7.1411690\ttest: 36.6162373\tbest: 36.6117319 (7436)\ttotal: 4m 21s\tremaining: 1m 22s\n",
      "7700:\tlearn: 7.0611614\ttest: 36.6144467\tbest: 36.6117319 (7436)\ttotal: 4m 24s\tremaining: 1m 19s\n",
      "7800:\tlearn: 6.9782858\ttest: 36.6091764\tbest: 36.6090810 (7799)\ttotal: 4m 28s\tremaining: 1m 15s\n",
      "7900:\tlearn: 6.8817452\ttest: 36.6100062\tbest: 36.6079320 (7808)\ttotal: 4m 32s\tremaining: 1m 12s\n",
      "8000:\tlearn: 6.8060696\ttest: 36.6074395\tbest: 36.6065773 (7990)\ttotal: 4m 35s\tremaining: 1m 8s\n",
      "8100:\tlearn: 6.7174440\ttest: 36.6079275\tbest: 36.6060937 (8067)\ttotal: 4m 39s\tremaining: 1m 5s\n",
      "8200:\tlearn: 6.6493731\ttest: 36.6066963\tbest: 36.6060937 (8067)\ttotal: 4m 43s\tremaining: 1m 2s\n",
      "8300:\tlearn: 6.5426920\ttest: 36.6095680\tbest: 36.6053740 (8226)\ttotal: 4m 46s\tremaining: 58.6s\n",
      "8400:\tlearn: 6.4476049\ttest: 36.6120389\tbest: 36.6053740 (8226)\ttotal: 4m 50s\tremaining: 55.2s\n",
      "8500:\tlearn: 6.3598182\ttest: 36.6132598\tbest: 36.6053740 (8226)\ttotal: 4m 53s\tremaining: 51.7s\n",
      "8600:\tlearn: 6.2712663\ttest: 36.6117379\tbest: 36.6053740 (8226)\ttotal: 4m 56s\tremaining: 48.3s\n",
      "8700:\tlearn: 6.1709494\ttest: 36.6095297\tbest: 36.6053740 (8226)\ttotal: 5m\tremaining: 44.9s\n",
      "Stopped by overfitting detector  (500 iterations wait)\n",
      "\n",
      "bestTest = 36.60537396\n",
      "bestIteration = 8226\n",
      "\n",
      "Shrink model to first 8227 iterations.\n",
      "[0]\tvalidation_0-rmse:60.77724\tvalidation_0-root_mean_squared_error:60.77724\n",
      "[100]\tvalidation_0-rmse:46.89625\tvalidation_0-root_mean_squared_error:46.89625\n",
      "[200]\tvalidation_0-rmse:43.15537\tvalidation_0-root_mean_squared_error:43.15538\n",
      "[300]\tvalidation_0-rmse:41.64008\tvalidation_0-root_mean_squared_error:41.64008\n",
      "[400]\tvalidation_0-rmse:40.82778\tvalidation_0-root_mean_squared_error:40.82778\n",
      "[500]\tvalidation_0-rmse:40.26208\tvalidation_0-root_mean_squared_error:40.26208\n",
      "[600]\tvalidation_0-rmse:39.93237\tvalidation_0-root_mean_squared_error:39.93237\n",
      "[700]\tvalidation_0-rmse:39.73795\tvalidation_0-root_mean_squared_error:39.73795\n",
      "[800]\tvalidation_0-rmse:39.59748\tvalidation_0-root_mean_squared_error:39.59748\n",
      "[900]\tvalidation_0-rmse:39.47073\tvalidation_0-root_mean_squared_error:39.47073\n",
      "[1000]\tvalidation_0-rmse:39.39821\tvalidation_0-root_mean_squared_error:39.39820\n",
      "[1100]\tvalidation_0-rmse:39.32234\tvalidation_0-root_mean_squared_error:39.32234\n",
      "[1200]\tvalidation_0-rmse:39.25261\tvalidation_0-root_mean_squared_error:39.25261\n",
      "[1300]\tvalidation_0-rmse:39.18237\tvalidation_0-root_mean_squared_error:39.18237\n",
      "[1400]\tvalidation_0-rmse:39.09624\tvalidation_0-root_mean_squared_error:39.09624\n",
      "[1500]\tvalidation_0-rmse:39.01617\tvalidation_0-root_mean_squared_error:39.01617\n",
      "[1600]\tvalidation_0-rmse:38.95396\tvalidation_0-root_mean_squared_error:38.95396\n",
      "[1700]\tvalidation_0-rmse:38.89642\tvalidation_0-root_mean_squared_error:38.89642\n",
      "[1800]\tvalidation_0-rmse:38.85772\tvalidation_0-root_mean_squared_error:38.85772\n",
      "[1900]\tvalidation_0-rmse:38.83309\tvalidation_0-root_mean_squared_error:38.83309\n",
      "[2000]\tvalidation_0-rmse:38.78374\tvalidation_0-root_mean_squared_error:38.78375\n",
      "[2100]\tvalidation_0-rmse:38.73477\tvalidation_0-root_mean_squared_error:38.73478\n",
      "[2200]\tvalidation_0-rmse:38.66559\tvalidation_0-root_mean_squared_error:38.66559\n",
      "[2300]\tvalidation_0-rmse:38.61821\tvalidation_0-root_mean_squared_error:38.61821\n",
      "[2400]\tvalidation_0-rmse:38.55106\tvalidation_0-root_mean_squared_error:38.55106\n",
      "[2500]\tvalidation_0-rmse:38.48215\tvalidation_0-root_mean_squared_error:38.48215\n",
      "[2600]\tvalidation_0-rmse:38.42738\tvalidation_0-root_mean_squared_error:38.42738\n",
      "[2700]\tvalidation_0-rmse:38.36923\tvalidation_0-root_mean_squared_error:38.36923\n",
      "[2800]\tvalidation_0-rmse:38.31690\tvalidation_0-root_mean_squared_error:38.31690\n",
      "[2900]\tvalidation_0-rmse:38.27694\tvalidation_0-root_mean_squared_error:38.27694\n",
      "[3000]\tvalidation_0-rmse:38.25047\tvalidation_0-root_mean_squared_error:38.25047\n",
      "[3100]\tvalidation_0-rmse:38.22885\tvalidation_0-root_mean_squared_error:38.22885\n",
      "[3200]\tvalidation_0-rmse:38.20527\tvalidation_0-root_mean_squared_error:38.20527\n",
      "[3300]\tvalidation_0-rmse:38.17581\tvalidation_0-root_mean_squared_error:38.17581\n",
      "[3400]\tvalidation_0-rmse:38.14680\tvalidation_0-root_mean_squared_error:38.14680\n",
      "[3500]\tvalidation_0-rmse:38.11964\tvalidation_0-root_mean_squared_error:38.11964\n",
      "[3600]\tvalidation_0-rmse:38.09463\tvalidation_0-root_mean_squared_error:38.09463\n",
      "[3700]\tvalidation_0-rmse:38.06563\tvalidation_0-root_mean_squared_error:38.06563\n",
      "[3800]\tvalidation_0-rmse:38.03057\tvalidation_0-root_mean_squared_error:38.03057\n",
      "[3900]\tvalidation_0-rmse:38.02334\tvalidation_0-root_mean_squared_error:38.02334\n",
      "[4000]\tvalidation_0-rmse:38.00524\tvalidation_0-root_mean_squared_error:38.00524\n",
      "[4100]\tvalidation_0-rmse:37.99787\tvalidation_0-root_mean_squared_error:37.99788\n",
      "[4200]\tvalidation_0-rmse:37.98154\tvalidation_0-root_mean_squared_error:37.98154\n",
      "[4300]\tvalidation_0-rmse:37.97669\tvalidation_0-root_mean_squared_error:37.97669\n",
      "[4400]\tvalidation_0-rmse:37.97089\tvalidation_0-root_mean_squared_error:37.97089\n",
      "[4500]\tvalidation_0-rmse:37.96503\tvalidation_0-root_mean_squared_error:37.96503\n",
      "[4600]\tvalidation_0-rmse:37.93792\tvalidation_0-root_mean_squared_error:37.93792\n",
      "[4700]\tvalidation_0-rmse:37.94396\tvalidation_0-root_mean_squared_error:37.94396\n",
      "[4800]\tvalidation_0-rmse:37.93610\tvalidation_0-root_mean_squared_error:37.93610\n",
      "[4900]\tvalidation_0-rmse:37.94187\tvalidation_0-root_mean_squared_error:37.94187\n",
      "[5000]\tvalidation_0-rmse:37.93777\tvalidation_0-root_mean_squared_error:37.93777\n",
      "[5100]\tvalidation_0-rmse:37.94636\tvalidation_0-root_mean_squared_error:37.94636\n",
      "[5116]\tvalidation_0-rmse:37.94590\tvalidation_0-root_mean_squared_error:37.94590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [24:38, 375.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest rmse:  37.84536545958547\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014897 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1560\n",
      "[LightGBM] [Info] Number of data points in the train set: 3475, number of used features: 184\n",
      "[LightGBM] [Info] Start training from score 157.063309\n",
      "Training until validation scores don't improve for 500 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/spa-d4/grad/mfe261/Projects/PREPARE/envs/lib/python3.9/site-packages/lightgbm/engine.py:204: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1889]\tvalid_0's rmse: 40.4176\n",
      "0:\tlearn: 60.6063864\ttest: 60.8112299\tbest: 60.8112299 (0)\ttotal: 23.3ms\tremaining: 3m 53s\n",
      "100:\tlearn: 44.6720446\ttest: 47.5307961\tbest: 47.5307961 (100)\ttotal: 2.51s\tremaining: 4m 6s\n",
      "200:\tlearn: 38.3069288\ttest: 43.9226278\tbest: 43.9226278 (200)\ttotal: 5.49s\tremaining: 4m 27s\n",
      "300:\tlearn: 34.8000820\ttest: 42.5284960\tbest: 42.5284960 (300)\ttotal: 8.59s\tremaining: 4m 36s\n",
      "400:\tlearn: 32.6221258\ttest: 41.9003158\tbest: 41.9003158 (400)\ttotal: 11.6s\tremaining: 4m 38s\n",
      "500:\tlearn: 31.0140313\ttest: 41.4877224\tbest: 41.4877224 (500)\ttotal: 14.5s\tremaining: 4m 35s\n",
      "600:\tlearn: 29.6661111\ttest: 41.2052206\tbest: 41.2052206 (600)\ttotal: 17.4s\tremaining: 4m 31s\n",
      "700:\tlearn: 28.4412860\ttest: 40.9896129\tbest: 40.9896129 (700)\ttotal: 20.4s\tremaining: 4m 30s\n",
      "800:\tlearn: 27.3865312\ttest: 40.8010040\tbest: 40.8010040 (800)\ttotal: 23.4s\tremaining: 4m 28s\n",
      "900:\tlearn: 26.3695772\ttest: 40.6661726\tbest: 40.6661726 (900)\ttotal: 26.6s\tremaining: 4m 28s\n",
      "1000:\tlearn: 25.4944457\ttest: 40.5329138\tbest: 40.5329138 (1000)\ttotal: 30s\tremaining: 4m 29s\n",
      "1100:\tlearn: 24.6654174\ttest: 40.4208060\tbest: 40.4208060 (1100)\ttotal: 33.3s\tremaining: 4m 28s\n",
      "1200:\tlearn: 23.9210927\ttest: 40.3198583\tbest: 40.3198583 (1200)\ttotal: 36.6s\tremaining: 4m 28s\n",
      "1300:\tlearn: 23.2495474\ttest: 40.2524222\tbest: 40.2524222 (1300)\ttotal: 40s\tremaining: 4m 27s\n",
      "1400:\tlearn: 22.6657511\ttest: 40.1849786\tbest: 40.1845274 (1399)\ttotal: 43.4s\tremaining: 4m 26s\n",
      "1500:\tlearn: 22.0363863\ttest: 40.1088362\tbest: 40.1072356 (1493)\ttotal: 46.7s\tremaining: 4m 24s\n",
      "1600:\tlearn: 21.4928818\ttest: 40.0679019\tbest: 40.0668821 (1598)\ttotal: 50.1s\tremaining: 4m 23s\n",
      "1700:\tlearn: 20.8924147\ttest: 40.0326244\tbest: 40.0321520 (1699)\ttotal: 53.6s\tremaining: 4m 21s\n",
      "1800:\tlearn: 20.3589957\ttest: 40.0035112\tbest: 40.0035112 (1800)\ttotal: 57s\tremaining: 4m 19s\n",
      "1900:\tlearn: 19.8502355\ttest: 39.9703430\tbest: 39.9692959 (1897)\ttotal: 1m\tremaining: 4m 17s\n",
      "2000:\tlearn: 19.3351318\ttest: 39.9424837\tbest: 39.9412404 (1998)\ttotal: 1m 3s\tremaining: 4m 15s\n",
      "2100:\tlearn: 18.8080377\ttest: 39.9153689\tbest: 39.9145173 (2082)\ttotal: 1m 7s\tremaining: 4m 13s\n",
      "2200:\tlearn: 18.3163152\ttest: 39.8927063\tbest: 39.8921618 (2199)\ttotal: 1m 10s\tremaining: 4m 11s\n",
      "2300:\tlearn: 17.8771159\ttest: 39.8777589\tbest: 39.8764787 (2295)\ttotal: 1m 14s\tremaining: 4m 8s\n",
      "2400:\tlearn: 17.4704234\ttest: 39.8559078\tbest: 39.8559078 (2400)\ttotal: 1m 17s\tremaining: 4m 5s\n",
      "2500:\tlearn: 17.0985161\ttest: 39.8401638\tbest: 39.8389413 (2495)\ttotal: 1m 20s\tremaining: 4m 2s\n",
      "2600:\tlearn: 16.7266392\ttest: 39.8292438\tbest: 39.8238595 (2564)\ttotal: 1m 24s\tremaining: 3m 59s\n",
      "2700:\tlearn: 16.4260121\ttest: 39.8067196\tbest: 39.8064259 (2699)\ttotal: 1m 27s\tremaining: 3m 57s\n",
      "2800:\tlearn: 16.0832016\ttest: 39.8049937\tbest: 39.8036981 (2736)\ttotal: 1m 31s\tremaining: 3m 54s\n",
      "2900:\tlearn: 15.7447946\ttest: 39.7824535\tbest: 39.7824535 (2900)\ttotal: 1m 34s\tremaining: 3m 51s\n",
      "3000:\tlearn: 15.4631316\ttest: 39.7755039\tbest: 39.7727452 (2962)\ttotal: 1m 37s\tremaining: 3m 48s\n",
      "3100:\tlearn: 15.1331150\ttest: 39.7617164\tbest: 39.7584059 (3096)\ttotal: 1m 41s\tremaining: 3m 45s\n",
      "3200:\tlearn: 14.8175123\ttest: 39.7530343\tbest: 39.7512669 (3191)\ttotal: 1m 44s\tremaining: 3m 42s\n",
      "3300:\tlearn: 14.5607993\ttest: 39.7415987\tbest: 39.7395083 (3289)\ttotal: 1m 48s\tremaining: 3m 39s\n",
      "3400:\tlearn: 14.3053864\ttest: 39.7322770\tbest: 39.7299707 (3387)\ttotal: 1m 51s\tremaining: 3m 37s\n",
      "3500:\tlearn: 14.0405854\ttest: 39.7358312\tbest: 39.7299707 (3387)\ttotal: 1m 55s\tremaining: 3m 34s\n",
      "3600:\tlearn: 13.7844881\ttest: 39.7234220\tbest: 39.7232956 (3599)\ttotal: 1m 58s\tremaining: 3m 31s\n",
      "3700:\tlearn: 13.5388397\ttest: 39.7169363\tbest: 39.7163776 (3692)\ttotal: 2m 2s\tremaining: 3m 28s\n",
      "3800:\tlearn: 13.2948743\ttest: 39.7005974\tbest: 39.7005974 (3800)\ttotal: 2m 5s\tremaining: 3m 25s\n",
      "3900:\tlearn: 13.0795317\ttest: 39.6926118\tbest: 39.6915664 (3878)\ttotal: 2m 9s\tremaining: 3m 22s\n",
      "4000:\tlearn: 12.8121942\ttest: 39.6803337\tbest: 39.6789800 (3997)\ttotal: 2m 12s\tremaining: 3m 19s\n",
      "4100:\tlearn: 12.5744572\ttest: 39.6694966\tbest: 39.6671058 (4085)\ttotal: 2m 16s\tremaining: 3m 16s\n",
      "4200:\tlearn: 12.3251882\ttest: 39.6587469\tbest: 39.6587469 (4200)\ttotal: 2m 19s\tremaining: 3m 13s\n",
      "4300:\tlearn: 12.1203713\ttest: 39.6490817\tbest: 39.6488021 (4299)\ttotal: 2m 23s\tremaining: 3m 9s\n",
      "4400:\tlearn: 11.9172773\ttest: 39.6468780\tbest: 39.6432161 (4378)\ttotal: 2m 26s\tremaining: 3m 6s\n",
      "4500:\tlearn: 11.7291004\ttest: 39.6408746\tbest: 39.6392720 (4484)\ttotal: 2m 30s\tremaining: 3m 3s\n",
      "4600:\tlearn: 11.5792807\ttest: 39.6282836\tbest: 39.6272665 (4590)\ttotal: 2m 33s\tremaining: 3m\n",
      "4700:\tlearn: 11.3971385\ttest: 39.6196472\tbest: 39.6192772 (4699)\ttotal: 2m 37s\tremaining: 2m 57s\n",
      "4800:\tlearn: 11.2130762\ttest: 39.6212943\tbest: 39.6164259 (4749)\ttotal: 2m 40s\tremaining: 2m 54s\n",
      "4900:\tlearn: 11.0606021\ttest: 39.6213574\tbest: 39.6164259 (4749)\ttotal: 2m 44s\tremaining: 2m 50s\n",
      "5000:\tlearn: 10.8934379\ttest: 39.6223915\tbest: 39.6164259 (4749)\ttotal: 2m 47s\tremaining: 2m 47s\n",
      "5100:\tlearn: 10.7453762\ttest: 39.6145881\tbest: 39.6145881 (5100)\ttotal: 2m 51s\tremaining: 2m 44s\n",
      "5200:\tlearn: 10.5868590\ttest: 39.6087224\tbest: 39.6067839 (5165)\ttotal: 2m 54s\tremaining: 2m 41s\n",
      "5300:\tlearn: 10.4348238\ttest: 39.5983396\tbest: 39.5979970 (5295)\ttotal: 2m 58s\tremaining: 2m 38s\n",
      "5400:\tlearn: 10.2947938\ttest: 39.5981664\tbest: 39.5964013 (5340)\ttotal: 3m 1s\tremaining: 2m 34s\n",
      "5500:\tlearn: 10.1540803\ttest: 39.5937699\tbest: 39.5924321 (5494)\ttotal: 3m 5s\tremaining: 2m 31s\n",
      "5600:\tlearn: 9.9964629\ttest: 39.5804173\tbest: 39.5804173 (5600)\ttotal: 3m 9s\tremaining: 2m 28s\n",
      "5700:\tlearn: 9.8487088\ttest: 39.5789122\tbest: 39.5773597 (5697)\ttotal: 3m 13s\tremaining: 2m 25s\n",
      "5800:\tlearn: 9.6872942\ttest: 39.5753648\tbest: 39.5742540 (5790)\ttotal: 3m 16s\tremaining: 2m 22s\n",
      "5900:\tlearn: 9.5361191\ttest: 39.5743097\tbest: 39.5727834 (5891)\ttotal: 3m 20s\tremaining: 2m 19s\n",
      "6000:\tlearn: 9.3871212\ttest: 39.5775732\tbest: 39.5727834 (5891)\ttotal: 3m 23s\tremaining: 2m 15s\n",
      "6100:\tlearn: 9.2554984\ttest: 39.5712648\tbest: 39.5708367 (6097)\ttotal: 3m 27s\tremaining: 2m 12s\n",
      "6200:\tlearn: 9.1247337\ttest: 39.5678119\tbest: 39.5675766 (6197)\ttotal: 3m 30s\tremaining: 2m 9s\n",
      "6300:\tlearn: 8.9855996\ttest: 39.5646332\tbest: 39.5609046 (6268)\ttotal: 3m 34s\tremaining: 2m 5s\n",
      "6400:\tlearn: 8.8320171\ttest: 39.5660045\tbest: 39.5609046 (6268)\ttotal: 3m 37s\tremaining: 2m 2s\n",
      "6500:\tlearn: 8.6970113\ttest: 39.5633008\tbest: 39.5609046 (6268)\ttotal: 3m 41s\tremaining: 1m 59s\n",
      "6600:\tlearn: 8.5829360\ttest: 39.5605064\tbest: 39.5599242 (6549)\ttotal: 3m 44s\tremaining: 1m 55s\n",
      "6700:\tlearn: 8.4740671\ttest: 39.5615909\tbest: 39.5595339 (6603)\ttotal: 3m 48s\tremaining: 1m 52s\n",
      "6800:\tlearn: 8.3638932\ttest: 39.5567114\tbest: 39.5567114 (6800)\ttotal: 3m 51s\tremaining: 1m 49s\n",
      "6900:\tlearn: 8.2373865\ttest: 39.5526488\tbest: 39.5520939 (6861)\ttotal: 3m 55s\tremaining: 1m 45s\n",
      "7000:\tlearn: 8.1109364\ttest: 39.5499706\tbest: 39.5491694 (6995)\ttotal: 3m 58s\tremaining: 1m 42s\n",
      "7100:\tlearn: 7.9870671\ttest: 39.5487008\tbest: 39.5483823 (7086)\ttotal: 4m 2s\tremaining: 1m 38s\n",
      "7200:\tlearn: 7.8650595\ttest: 39.5471811\tbest: 39.5456337 (7161)\ttotal: 4m 5s\tremaining: 1m 35s\n",
      "7300:\tlearn: 7.7460955\ttest: 39.5481905\tbest: 39.5456337 (7161)\ttotal: 4m 9s\tremaining: 1m 32s\n",
      "7400:\tlearn: 7.6515868\ttest: 39.5463280\tbest: 39.5436026 (7368)\ttotal: 4m 12s\tremaining: 1m 28s\n",
      "7500:\tlearn: 7.5375922\ttest: 39.5457643\tbest: 39.5436026 (7368)\ttotal: 4m 16s\tremaining: 1m 25s\n",
      "7600:\tlearn: 7.4381846\ttest: 39.5407856\tbest: 39.5398022 (7594)\ttotal: 4m 19s\tremaining: 1m 22s\n",
      "7700:\tlearn: 7.3109353\ttest: 39.5421525\tbest: 39.5380432 (7620)\ttotal: 4m 23s\tremaining: 1m 18s\n",
      "7800:\tlearn: 7.2218021\ttest: 39.5435488\tbest: 39.5380432 (7620)\ttotal: 4m 26s\tremaining: 1m 15s\n",
      "7900:\tlearn: 7.1215532\ttest: 39.5445210\tbest: 39.5380432 (7620)\ttotal: 4m 30s\tremaining: 1m 11s\n",
      "8000:\tlearn: 7.0177164\ttest: 39.5428653\tbest: 39.5380432 (7620)\ttotal: 4m 33s\tremaining: 1m 8s\n",
      "8100:\tlearn: 6.9113297\ttest: 39.5453725\tbest: 39.5380432 (7620)\ttotal: 4m 37s\tremaining: 1m 5s\n",
      "Stopped by overfitting detector  (500 iterations wait)\n",
      "\n",
      "bestTest = 39.53804322\n",
      "bestIteration = 7620\n",
      "\n",
      "Shrink model to first 7621 iterations.\n",
      "[0]\tvalidation_0-rmse:60.80225\tvalidation_0-root_mean_squared_error:60.80225\n",
      "[100]\tvalidation_0-rmse:48.18373\tvalidation_0-root_mean_squared_error:48.18373\n",
      "[200]\tvalidation_0-rmse:44.82792\tvalidation_0-root_mean_squared_error:44.82793\n",
      "[300]\tvalidation_0-rmse:43.43305\tvalidation_0-root_mean_squared_error:43.43305\n",
      "[400]\tvalidation_0-rmse:42.75253\tvalidation_0-root_mean_squared_error:42.75253\n",
      "[500]\tvalidation_0-rmse:42.43699\tvalidation_0-root_mean_squared_error:42.43698\n",
      "[600]\tvalidation_0-rmse:42.24010\tvalidation_0-root_mean_squared_error:42.24010\n",
      "[700]\tvalidation_0-rmse:42.14702\tvalidation_0-root_mean_squared_error:42.14702\n",
      "[800]\tvalidation_0-rmse:42.09019\tvalidation_0-root_mean_squared_error:42.09019\n",
      "[900]\tvalidation_0-rmse:41.98381\tvalidation_0-root_mean_squared_error:41.98381\n",
      "[1000]\tvalidation_0-rmse:41.91189\tvalidation_0-root_mean_squared_error:41.91189\n",
      "[1100]\tvalidation_0-rmse:41.84907\tvalidation_0-root_mean_squared_error:41.84907\n",
      "[1200]\tvalidation_0-rmse:41.80767\tvalidation_0-root_mean_squared_error:41.80767\n",
      "[1300]\tvalidation_0-rmse:41.78509\tvalidation_0-root_mean_squared_error:41.78509\n",
      "[1400]\tvalidation_0-rmse:41.77698\tvalidation_0-root_mean_squared_error:41.77698\n",
      "[1500]\tvalidation_0-rmse:41.76261\tvalidation_0-root_mean_squared_error:41.76261\n",
      "[1600]\tvalidation_0-rmse:41.71880\tvalidation_0-root_mean_squared_error:41.71880\n",
      "[1700]\tvalidation_0-rmse:41.68556\tvalidation_0-root_mean_squared_error:41.68556\n",
      "[1800]\tvalidation_0-rmse:41.63887\tvalidation_0-root_mean_squared_error:41.63887\n",
      "[1900]\tvalidation_0-rmse:41.59978\tvalidation_0-root_mean_squared_error:41.59978\n",
      "[2000]\tvalidation_0-rmse:41.55931\tvalidation_0-root_mean_squared_error:41.55931\n",
      "[2100]\tvalidation_0-rmse:41.51662\tvalidation_0-root_mean_squared_error:41.51662\n",
      "[2200]\tvalidation_0-rmse:41.50349\tvalidation_0-root_mean_squared_error:41.50349\n",
      "[2300]\tvalidation_0-rmse:41.47375\tvalidation_0-root_mean_squared_error:41.47375\n",
      "[2400]\tvalidation_0-rmse:41.45125\tvalidation_0-root_mean_squared_error:41.45126\n",
      "[2500]\tvalidation_0-rmse:41.42785\tvalidation_0-root_mean_squared_error:41.42785\n",
      "[2600]\tvalidation_0-rmse:41.40575\tvalidation_0-root_mean_squared_error:41.40575\n",
      "[2700]\tvalidation_0-rmse:41.38670\tvalidation_0-root_mean_squared_error:41.38671\n",
      "[2800]\tvalidation_0-rmse:41.36584\tvalidation_0-root_mean_squared_error:41.36584\n",
      "[2900]\tvalidation_0-rmse:41.34756\tvalidation_0-root_mean_squared_error:41.34756\n",
      "[3000]\tvalidation_0-rmse:41.34200\tvalidation_0-root_mean_squared_error:41.34200\n",
      "[3100]\tvalidation_0-rmse:41.33607\tvalidation_0-root_mean_squared_error:41.33607\n",
      "[3200]\tvalidation_0-rmse:41.34438\tvalidation_0-root_mean_squared_error:41.34438\n",
      "[3300]\tvalidation_0-rmse:41.34800\tvalidation_0-root_mean_squared_error:41.34800\n",
      "[3400]\tvalidation_0-rmse:41.35660\tvalidation_0-root_mean_squared_error:41.35660\n",
      "[3500]\tvalidation_0-rmse:41.34632\tvalidation_0-root_mean_squared_error:41.34632\n",
      "[3600]\tvalidation_0-rmse:41.33095\tvalidation_0-root_mean_squared_error:41.33095\n",
      "[3700]\tvalidation_0-rmse:41.33781\tvalidation_0-root_mean_squared_error:41.33781\n",
      "[3800]\tvalidation_0-rmse:41.33543\tvalidation_0-root_mean_squared_error:41.33543\n",
      "[3900]\tvalidation_0-rmse:41.33160\tvalidation_0-root_mean_squared_error:41.33160\n",
      "[4000]\tvalidation_0-rmse:41.33372\tvalidation_0-root_mean_squared_error:41.33372\n",
      "[4100]\tvalidation_0-rmse:41.33201\tvalidation_0-root_mean_squared_error:41.33201\n",
      "[4105]\tvalidation_0-rmse:41.33162\tvalidation_0-root_mean_squared_error:41.33162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [31:18, 375.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest rmse:  40.62677968525773\n",
      "Average MSE across folds: 37.09901391322352\n",
      "Optimized weights per fold: [array([0.00970546, 0.48701967, 0.24999563, 0.25327923]), array([0.32447771, 0.47947732, 0.10986934, 0.08617563]), array([0.08375916, 0.87731506, 0.02437395, 0.01455183]), array([5.54591421e-01, 3.24937521e-01, 1.20471059e-01, 3.73565809e-13]), array([5.51754103e-18, 6.24163289e-01, 2.50116089e-01, 1.25720622e-01])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the number of folds\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Placeholder for storing results\n",
    "fold_results = []\n",
    "optimized_weights_list = []\n",
    "\n",
    "for train_index, val_index in tqdm(kf.split(merged_df)):\n",
    "    # Split the data\n",
    "    train_X, val_X = merged_df.iloc[train_index], merged_df.iloc[val_index]\n",
    "    train_y, val_y = y.iloc[train_index], y.iloc[val_index]\n",
    "    \n",
    "    train_X_processed, val_X_processed = merged_df_processed.iloc[train_index], merged_df_processed.iloc[val_index]\n",
    "\n",
    "    # Train LightGBM\n",
    "    train_data = lgb.Dataset(train_X, label=train_y, categorical_feature='auto')\n",
    "    val_data = lgb.Dataset(val_X, label=val_y, categorical_feature='auto')\n",
    "\n",
    "    # Define parameters\n",
    "    params = {\n",
    "        'objective': 'regression',  # default for regression\n",
    "        'metric': 'rmse',\n",
    "        'learning_rate': 0.01,\n",
    "        'n_estimators': 10000,\n",
    "        'random_seed': 42\n",
    "    }\n",
    "\n",
    "    # Train the model with early stopping\n",
    "    model1 = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        valid_sets=[val_data],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=500, verbose=True),\n",
    "        ]\n",
    "    )\n",
    "    pred1 = model1.predict(val_X, num_iteration=model1.best_iteration)\n",
    "\n",
    "    # Train CatBoost\n",
    "    model2 = CatBoostRegressor(\n",
    "        iterations=10000, learning_rate=0.01, depth=10, loss_function='RMSE',\n",
    "        cat_features=merged_df.select_dtypes(include=['object', 'category']).columns.to_list(),\n",
    "        verbose=100, early_stopping_rounds=500\n",
    "    )\n",
    "    model2.fit(train_X, train_y, eval_set=(val_X, val_y))\n",
    "    pred2 = model2.predict(val_X)\n",
    "    \n",
    "    # Train XGBoost\n",
    "    model3 = XGBRegressor(n_estimators=10000, learning_rate=0.01,\n",
    "        max_depth=3, random_state=42, \n",
    "        enable_categorical=True,\n",
    "        eval_metric=root_mean_squared_error,\n",
    "        early_stopping_rounds=500)\n",
    "    model3.fit(train_X, train_y, eval_set=[(val_X, val_y)], verbose=100)\n",
    "    pred3 = model3.predict(val_X)\n",
    "    \n",
    "    # Train Random Forest\n",
    "    model4 = RandomForestRegressor(n_estimators=1000, random_state=42)\n",
    "    model4.fit(train_X_processed, train_y)\n",
    "    pred4 = model4.predict(val_X_processed)\n",
    "    print(\"RandomForest rmse: \", root_mean_squared_error(val_y, pred4))\n",
    "\n",
    "    # Define loss function for weight optimization\n",
    "    def loss_function(weights):\n",
    "        w1, w2, w3, w4 = weights\n",
    "        combined_predictions = w1 * pred1 + w2 * pred2 + w3 * pred3 + w4 * pred4\n",
    "        mse = np.mean((combined_predictions - val_y) ** 2)\n",
    "        return mse\n",
    "\n",
    "    # Initial weights\n",
    "    initial_weights = [1/4, 1/4, 1/4, 1/4]\n",
    "\n",
    "    # Constraints: weights must sum to 1\n",
    "    constraints = {'type': 'eq', 'fun': lambda w: w[0] + w[1] + w[2] + w[3] - 1}\n",
    "\n",
    "    # Bounds: weights must be between 0 and 1\n",
    "    bounds = [(0, 1), (0, 1), (0, 1), (0, 1)]\n",
    "\n",
    "    # Optimize weights\n",
    "    result = minimize(loss_function, initial_weights, constraints=constraints, bounds=bounds)\n",
    "    optimized_weights = result.x\n",
    "\n",
    "    # Combine predictions using optimized weights\n",
    "    final_predictions = (\n",
    "        optimized_weights[0] * pred1 +\n",
    "        optimized_weights[1] * pred2 +\n",
    "        optimized_weights[2] * pred3 +\n",
    "        optimized_weights[3] * pred4\n",
    "    )\n",
    "\n",
    "    # Store results\n",
    "    fold_mse = root_mean_squared_error(val_y, final_predictions)  # RMSE\n",
    "    fold_results.append(fold_mse)\n",
    "    optimized_weights_list.append(optimized_weights)\n",
    "\n",
    "# Display results\n",
    "print(f\"Average MSE across folds: {np.mean(fold_results)}\")\n",
    "print(f\"Optimized weights per fold: {optimized_weights_list}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MSE across folds: 37.09901391322352\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average MSE across folds: {np.mean(fold_results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.19450675, 0.55858257, 0.15096522, 0.09594546])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(optimized_weights_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000973 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1653\n",
      "[LightGBM] [Info] Number of data points in the train set: 4343, number of used features: 184\n",
      "[LightGBM] [Info] Start training from score 157.016809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/spa-d4/grad/mfe261/Projects/PREPARE/envs/lib/python3.9/site-packages/lightgbm/engine.py:204: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 60.6445520\ttotal: 13.2ms\tremaining: 2m 11s\n",
      "100:\tlearn: 45.0332493\ttotal: 2.51s\tremaining: 4m 6s\n",
      "200:\tlearn: 38.9044928\ttotal: 5.5s\tremaining: 4m 28s\n",
      "300:\tlearn: 35.5399273\ttotal: 8.57s\tremaining: 4m 36s\n",
      "400:\tlearn: 33.3917089\ttotal: 11.5s\tremaining: 4m 35s\n",
      "500:\tlearn: 31.8994396\ttotal: 14.4s\tremaining: 4m 33s\n",
      "600:\tlearn: 30.7114954\ttotal: 17.4s\tremaining: 4m 32s\n",
      "700:\tlearn: 29.6846195\ttotal: 20.7s\tremaining: 4m 34s\n",
      "800:\tlearn: 28.8254993\ttotal: 23.7s\tremaining: 4m 32s\n",
      "900:\tlearn: 27.9642830\ttotal: 26.9s\tremaining: 4m 31s\n",
      "1000:\tlearn: 27.1063650\ttotal: 30.3s\tremaining: 4m 32s\n",
      "1100:\tlearn: 26.4289151\ttotal: 33.7s\tremaining: 4m 32s\n",
      "1200:\tlearn: 25.7500371\ttotal: 37s\tremaining: 4m 31s\n",
      "1300:\tlearn: 25.0498635\ttotal: 40.3s\tremaining: 4m 29s\n",
      "1400:\tlearn: 24.4402644\ttotal: 43.7s\tremaining: 4m 27s\n",
      "1500:\tlearn: 23.9600868\ttotal: 47.1s\tremaining: 4m 26s\n",
      "1600:\tlearn: 23.4730841\ttotal: 50.6s\tremaining: 4m 25s\n",
      "1700:\tlearn: 23.0239954\ttotal: 54.1s\tremaining: 4m 23s\n",
      "1800:\tlearn: 22.5589787\ttotal: 57.6s\tremaining: 4m 22s\n",
      "1900:\tlearn: 22.0027453\ttotal: 1m 1s\tremaining: 4m 20s\n",
      "2000:\tlearn: 21.5280748\ttotal: 1m 4s\tremaining: 4m 18s\n",
      "2100:\tlearn: 21.0758575\ttotal: 1m 8s\tremaining: 4m 16s\n",
      "2200:\tlearn: 20.6035806\ttotal: 1m 11s\tremaining: 4m 14s\n",
      "2300:\tlearn: 20.0923941\ttotal: 1m 15s\tremaining: 4m 11s\n",
      "2400:\tlearn: 19.6304760\ttotal: 1m 18s\tremaining: 4m 9s\n",
      "2500:\tlearn: 19.1690470\ttotal: 1m 22s\tremaining: 4m 6s\n",
      "2600:\tlearn: 18.7054061\ttotal: 1m 25s\tremaining: 4m 4s\n",
      "2700:\tlearn: 18.3020235\ttotal: 1m 29s\tremaining: 4m 1s\n",
      "2800:\tlearn: 17.8971683\ttotal: 1m 32s\tremaining: 3m 58s\n",
      "2900:\tlearn: 17.5014374\ttotal: 1m 36s\tremaining: 3m 55s\n",
      "3000:\tlearn: 17.1584459\ttotal: 1m 39s\tremaining: 3m 53s\n",
      "3100:\tlearn: 16.7710376\ttotal: 1m 43s\tremaining: 3m 50s\n",
      "3200:\tlearn: 16.4389023\ttotal: 1m 47s\tremaining: 3m 47s\n",
      "3300:\tlearn: 16.1492914\ttotal: 1m 50s\tremaining: 3m 44s\n",
      "3400:\tlearn: 15.8150273\ttotal: 1m 54s\tremaining: 3m 41s\n",
      "3500:\tlearn: 15.5370708\ttotal: 1m 57s\tremaining: 3m 39s\n",
      "3600:\tlearn: 15.2190514\ttotal: 2m 1s\tremaining: 3m 36s\n",
      "3700:\tlearn: 14.9355552\ttotal: 2m 5s\tremaining: 3m 33s\n",
      "3800:\tlearn: 14.6754735\ttotal: 2m 8s\tremaining: 3m 30s\n",
      "3900:\tlearn: 14.3971936\ttotal: 2m 12s\tremaining: 3m 27s\n",
      "4000:\tlearn: 14.1569598\ttotal: 2m 16s\tremaining: 3m 24s\n",
      "4100:\tlearn: 13.8984256\ttotal: 2m 19s\tremaining: 3m 21s\n",
      "4200:\tlearn: 13.6295098\ttotal: 2m 23s\tremaining: 3m 17s\n",
      "4300:\tlearn: 13.4023328\ttotal: 2m 26s\tremaining: 3m 14s\n",
      "4400:\tlearn: 13.1919329\ttotal: 2m 30s\tremaining: 3m 11s\n",
      "4500:\tlearn: 12.9748400\ttotal: 2m 34s\tremaining: 3m 8s\n",
      "4600:\tlearn: 12.7630913\ttotal: 2m 37s\tremaining: 3m 5s\n",
      "4700:\tlearn: 12.5497149\ttotal: 2m 41s\tremaining: 3m 1s\n",
      "4800:\tlearn: 12.3366443\ttotal: 2m 44s\tremaining: 2m 58s\n",
      "4900:\tlearn: 12.1615950\ttotal: 2m 48s\tremaining: 2m 55s\n",
      "5000:\tlearn: 11.9702002\ttotal: 2m 52s\tremaining: 2m 51s\n",
      "5100:\tlearn: 11.7668404\ttotal: 2m 55s\tremaining: 2m 48s\n",
      "5200:\tlearn: 11.5881455\ttotal: 2m 59s\tremaining: 2m 45s\n",
      "5300:\tlearn: 11.4196292\ttotal: 3m 2s\tremaining: 2m 41s\n",
      "5400:\tlearn: 11.2406082\ttotal: 3m 6s\tremaining: 2m 38s\n",
      "5500:\tlearn: 11.0899215\ttotal: 3m 9s\tremaining: 2m 35s\n",
      "5600:\tlearn: 10.9458999\ttotal: 3m 13s\tremaining: 2m 32s\n",
      "5700:\tlearn: 10.7971515\ttotal: 3m 17s\tremaining: 2m 28s\n",
      "5800:\tlearn: 10.6651145\ttotal: 3m 20s\tremaining: 2m 25s\n",
      "5900:\tlearn: 10.5198428\ttotal: 3m 24s\tremaining: 2m 22s\n",
      "6000:\tlearn: 10.3687621\ttotal: 3m 28s\tremaining: 2m 18s\n",
      "6100:\tlearn: 10.2291979\ttotal: 3m 31s\tremaining: 2m 15s\n",
      "6200:\tlearn: 10.0977860\ttotal: 3m 35s\tremaining: 2m 11s\n",
      "6300:\tlearn: 9.9688500\ttotal: 3m 38s\tremaining: 2m 8s\n",
      "6400:\tlearn: 9.8329615\ttotal: 3m 42s\tremaining: 2m 5s\n",
      "6500:\tlearn: 9.7010226\ttotal: 3m 46s\tremaining: 2m 1s\n",
      "6600:\tlearn: 9.5681164\ttotal: 3m 49s\tremaining: 1m 58s\n",
      "6700:\tlearn: 9.4474501\ttotal: 3m 53s\tremaining: 1m 55s\n",
      "6800:\tlearn: 9.3373909\ttotal: 3m 57s\tremaining: 1m 51s\n",
      "6900:\tlearn: 9.2316788\ttotal: 4m\tremaining: 1m 48s\n",
      "7000:\tlearn: 9.0950100\ttotal: 4m 4s\tremaining: 1m 44s\n",
      "7100:\tlearn: 8.9641019\ttotal: 4m 7s\tremaining: 1m 41s\n",
      "7200:\tlearn: 8.8330828\ttotal: 4m 11s\tremaining: 1m 37s\n",
      "7300:\tlearn: 8.7056355\ttotal: 4m 15s\tremaining: 1m 34s\n",
      "7400:\tlearn: 8.5944923\ttotal: 4m 18s\tremaining: 1m 30s\n",
      "7500:\tlearn: 8.4769652\ttotal: 4m 22s\tremaining: 1m 27s\n",
      "7600:\tlearn: 8.3787511\ttotal: 4m 25s\tremaining: 1m 23s\n",
      "7700:\tlearn: 8.2768821\ttotal: 4m 29s\tremaining: 1m 20s\n",
      "7800:\tlearn: 8.1577658\ttotal: 4m 33s\tremaining: 1m 17s\n",
      "7900:\tlearn: 8.0566503\ttotal: 4m 36s\tremaining: 1m 13s\n",
      "8000:\tlearn: 7.9530611\ttotal: 4m 40s\tremaining: 1m 10s\n",
      "8100:\tlearn: 7.8488680\ttotal: 4m 43s\tremaining: 1m 6s\n",
      "8200:\tlearn: 7.7414139\ttotal: 4m 47s\tremaining: 1m 3s\n",
      "8300:\tlearn: 7.6400841\ttotal: 4m 50s\tremaining: 59.5s\n",
      "8400:\tlearn: 7.5409817\ttotal: 4m 54s\tremaining: 56.1s\n",
      "8500:\tlearn: 7.4448480\ttotal: 4m 58s\tremaining: 52.6s\n",
      "8600:\tlearn: 7.3443907\ttotal: 5m 1s\tremaining: 49.1s\n",
      "8700:\tlearn: 7.2458450\ttotal: 5m 5s\tremaining: 45.6s\n",
      "8800:\tlearn: 7.1647105\ttotal: 5m 9s\tremaining: 42.1s\n",
      "8900:\tlearn: 7.0770649\ttotal: 5m 12s\tremaining: 38.6s\n",
      "9000:\tlearn: 6.9864746\ttotal: 5m 16s\tremaining: 35.1s\n",
      "9100:\tlearn: 6.8939751\ttotal: 5m 20s\tremaining: 31.6s\n",
      "9200:\tlearn: 6.8155701\ttotal: 5m 23s\tremaining: 28.1s\n",
      "9300:\tlearn: 6.7304956\ttotal: 5m 27s\tremaining: 24.6s\n",
      "9400:\tlearn: 6.6450581\ttotal: 5m 30s\tremaining: 21.1s\n",
      "9500:\tlearn: 6.5535993\ttotal: 5m 34s\tremaining: 17.6s\n",
      "9600:\tlearn: 6.4757394\ttotal: 5m 37s\tremaining: 14s\n",
      "9700:\tlearn: 6.4064269\ttotal: 5m 41s\tremaining: 10.5s\n",
      "9800:\tlearn: 6.3182378\ttotal: 5m 44s\tremaining: 7s\n",
      "9900:\tlearn: 6.2349518\ttotal: 5m 48s\tremaining: 3.48s\n",
      "9999:\tlearn: 6.1630591\ttotal: 5m 52s\tremaining: 0us\n",
      "Final blended predictions for the test dataset:\n",
      "[181 202 200 ... 201 180 157]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average weights from cross-validation\n",
    "average_weights = np.mean(optimized_weights_list, axis=0)\n",
    "\n",
    "# Train models on the entire training dataset\n",
    "train_data = lgb.Dataset(merged_df, label=y, categorical_feature='auto')\n",
    "\n",
    "params = {\n",
    "    'objective': 'regression',  # default for regression\n",
    "    'metric': 'rmse',\n",
    "    'learning_rate': 0.01,\n",
    "    'n_estimators': 10000,\n",
    "    'random_seed': 42\n",
    "}\n",
    "final_model1 = lgb.train(\n",
    "    params,\n",
    "    train_data\n",
    ")\n",
    "\n",
    "final_model2 = CatBoostRegressor(\n",
    "    iterations=10000, learning_rate=0.01, depth=10, loss_function='RMSE',\n",
    "    cat_features=merged_df.select_dtypes(include=['object', 'category']).columns.to_list(),\n",
    "    verbose=100\n",
    ")\n",
    "final_model2.fit(merged_df, y)\n",
    "\n",
    "final_model3 = XGBRegressor(n_estimators=10000, learning_rate=0.01,\n",
    "        max_depth=3, random_state=42, \n",
    "        enable_categorical=True,\n",
    "        eval_metric=root_mean_squared_error)\n",
    "final_model3.fit(merged_df, y)\n",
    "\n",
    "# Train Random Forest\n",
    "final_model4 = RandomForestRegressor(n_estimators=1000, random_state=42)\n",
    "final_model4.fit(merged_df_processed, y)\n",
    "\n",
    "# Generate predictions on the test dataset\n",
    "test_pred1 = final_model1.predict(merged_test)\n",
    "test_pred2 = final_model2.predict(merged_test)\n",
    "test_pred3 = final_model3.predict(merged_test)\n",
    "test_pred4 = final_model4.predict(merged_test_processed)\n",
    "\n",
    "# Combine the predictions using the average weights\n",
    "final_test_predictions = (\n",
    "    average_weights[0] * test_pred1 + average_weights[1] * test_pred2 + average_weights[2] * test_pred3 + average_weights[3] * test_pred4\n",
    ")\n",
    "\n",
    "# Optionally round predictions if required (e.g., for classification tasks)\n",
    "final_test_predictions = np.round(final_test_predictions).astype(int)\n",
    "\n",
    "# Display final predictions\n",
    "print(\"Final blended predictions for the test dataset:\")\n",
    "print(final_test_predictions)\n",
    "\n",
    "ss['composite_score']=final_test_predictions\n",
    "#generate submission\n",
    "ss.to_csv('dataset/LGBM_and_CatBoost_and_XGBoost_RandomForest.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
