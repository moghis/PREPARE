{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# load libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.model_selection import StratifiedGroupKFold,StratifiedShuffleSplit,KFold,train_test_split\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "from scipy.optimize import minimize\n",
    "from tqdm import tqdm\n",
    "from xgboost import XGBRegressor\n",
    "import copy\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data\n",
    "train=pd.read_csv(\"../dataset/train_features.csv\")\n",
    "y=pd.read_csv(\"../dataset/train_labels.csv\")\n",
    "test=pd.read_csv(\"../dataset/test_features.csv\")\n",
    "ss=pd.read_csv(\"../dataset/submission_format.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>age_03</th>\n",
       "      <th>urban_03</th>\n",
       "      <th>married_03</th>\n",
       "      <th>n_mar_03</th>\n",
       "      <th>edu_gru_03</th>\n",
       "      <th>n_living_child_03</th>\n",
       "      <th>migration_03</th>\n",
       "      <th>glob_hlth_03</th>\n",
       "      <th>adl_dress_03</th>\n",
       "      <th>...</th>\n",
       "      <th>rrelgimp_12</th>\n",
       "      <th>rrfcntx_m_12</th>\n",
       "      <th>rsocact_m_12</th>\n",
       "      <th>rrelgwk_12</th>\n",
       "      <th>a16a_12</th>\n",
       "      <th>a21_12</th>\n",
       "      <th>a22_12</th>\n",
       "      <th>a33b_12</th>\n",
       "      <th>a34_12</th>\n",
       "      <th>j11_12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aace</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.somewhat important</td>\n",
       "      <td>9.Never</td>\n",
       "      <td>9.Never</td>\n",
       "      <td>0.No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Concrete 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aanz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.very important</td>\n",
       "      <td>9.Never</td>\n",
       "      <td>1.Almost every day</td>\n",
       "      <td>0.No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Concrete 2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 184 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    uid age_03 urban_03 married_03  n_mar_03 edu_gru_03 n_living_child_03  \\\n",
       "0  aace    NaN      NaN        NaN       NaN        NaN               NaN   \n",
       "1  aanz    NaN      NaN        NaN       NaN        NaN               NaN   \n",
       "\n",
       "   migration_03 glob_hlth_03  adl_dress_03  ...           rrelgimp_12  \\\n",
       "0           NaN          NaN           NaN  ...  2.somewhat important   \n",
       "1           NaN          NaN           NaN  ...      1.very important   \n",
       "\n",
       "   rrfcntx_m_12        rsocact_m_12  rrelgwk_12  a16a_12  a21_12  a22_12  \\\n",
       "0       9.Never             9.Never        0.No      NaN     NaN     NaN   \n",
       "1       9.Never  1.Almost every day        0.No      NaN     NaN     NaN   \n",
       "\n",
       "   a33b_12  a34_12      j11_12  \n",
       "0      NaN     NaN  Concrete 2  \n",
       "1      NaN     NaN  Concrete 2  \n",
       "\n",
       "[2 rows x 184 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>age_03</th>\n",
       "      <th>urban_03</th>\n",
       "      <th>married_03</th>\n",
       "      <th>n_mar_03</th>\n",
       "      <th>edu_gru_03</th>\n",
       "      <th>n_living_child_03</th>\n",
       "      <th>migration_03</th>\n",
       "      <th>glob_hlth_03</th>\n",
       "      <th>adl_dress_03</th>\n",
       "      <th>...</th>\n",
       "      <th>rrelgimp_12</th>\n",
       "      <th>rrfcntx_m_12</th>\n",
       "      <th>rsocact_m_12</th>\n",
       "      <th>rrelgwk_12</th>\n",
       "      <th>a16a_12</th>\n",
       "      <th>a21_12</th>\n",
       "      <th>a22_12</th>\n",
       "      <th>a33b_12</th>\n",
       "      <th>a34_12</th>\n",
       "      <th>j11_12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abxu</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wood, mosaic, or other covering 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aeol</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.very important</td>\n",
       "      <td>9.Never</td>\n",
       "      <td>9.Never</td>\n",
       "      <td>1.Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Concrete 2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 184 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    uid age_03 urban_03 married_03  n_mar_03 edu_gru_03 n_living_child_03  \\\n",
       "0  abxu    NaN      NaN        NaN       NaN        NaN               NaN   \n",
       "1  aeol    NaN      NaN        NaN       NaN        NaN               NaN   \n",
       "\n",
       "   migration_03 glob_hlth_03  adl_dress_03  ...       rrelgimp_12  \\\n",
       "0           NaN          NaN           NaN  ...               NaN   \n",
       "1           NaN          NaN           NaN  ...  1.very important   \n",
       "\n",
       "   rrfcntx_m_12  rsocact_m_12  rrelgwk_12  a16a_12  a21_12  a22_12  a33b_12  \\\n",
       "0           NaN           NaN         NaN      NaN     NaN     NaN      NaN   \n",
       "1       9.Never       9.Never       1.Yes      NaN     NaN     NaN      NaN   \n",
       "\n",
       "   a34_12                             j11_12  \n",
       "0     NaN  Wood, mosaic, or other covering 1  \n",
       "1     NaN                         Concrete 2  \n",
       "\n",
       "[2 rows x 184 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3276 entries, 0 to 3275\n",
      "Columns: 184 entries, uid to j11_12\n",
      "dtypes: float64(140), object(44)\n",
      "memory usage: 4.6+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let merge train and label\n",
    "merged_df = pd.merge(train, y, on='uid', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4343, 186)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       2021\n",
       "1       2021\n",
       "2       2016\n",
       "3       2021\n",
       "4       2021\n",
       "        ... \n",
       "4338    2021\n",
       "4339    2016\n",
       "4340    2021\n",
       "4341    2021\n",
       "4342    2021\n",
       "Name: year, Length: 4343, dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df[\"year\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "uid                   0\n",
       "age_03             1456\n",
       "urban_03           1454\n",
       "married_03         1454\n",
       "n_mar_03           1482\n",
       "                   ... \n",
       "a33b_12            4288\n",
       "a34_12             1601\n",
       "j11_12               89\n",
       "year                  0\n",
       "composite_score       0\n",
       "Length: 186, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#so many missing values\n",
    "merged_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets merged test AND sample submission\n",
    "merged_test = pd.merge(test, ss, on='uid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       2016\n",
       "1       2016\n",
       "2       2021\n",
       "3       2016\n",
       "4       2021\n",
       "        ... \n",
       "1100    2016\n",
       "1101    2021\n",
       "1102    2016\n",
       "1103    2021\n",
       "1104    2021\n",
       "Name: year, Length: 1105, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_test[\"year\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: title={'center': 'Target Distribution'}, ylabel='Frequency'>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "merged_df['composite_score'].plot(kind='hist',title='Target Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new features by combining 2003 and 2012 scores and numbering ordinal variables\n",
    "def feature_engineering(data):\n",
    "    data['rjob_hrswk_change'] = (data['rjob_hrswk_12'] - data['rjob_hrswk_03']).astype(float)\n",
    "    data['max_work_year']=data[['rjob_end_12','rjob_end_03']].max(axis=1).astype(float)\n",
    "    data['years_since_work']=(data['year']-data['max_work_year']).astype(float)\n",
    "    data['hincome_change']=(data['hincome_12']-data['hincome_03']).astype(float)\n",
    "    data['niadl_change']=(data['n_iadl_12']-data['n_iadl_03']).astype(float)\n",
    "    data['adl_change']=(data['n_adl_12']-data['n_adl_03']).astype(float)\n",
    "    data['depr_change']=(data['n_depr_12']-data['n_depr_03']).astype(float)\n",
    "    data['glob_hlth_03']=data['glob_hlth_03'].replace({'5. Poor':0, '4. Fair':1, '3. Good':2, '2. Very good': 3, '1. Excellent':4}).astype(float)\n",
    "    data['glob_hlth_12']=data['glob_hlth_12'].replace({'5. Poor':0, '4. Fair':1, '3. Good':2, '2. Very good': 3, '1. Excellent':4}).astype(float)\n",
    "    data['glob_hlth_change']=(data['glob_hlth_12']-data['glob_hlth_03']).astype(float)\n",
    "    data['bmi_03']=data['bmi_03'].replace({'1. Underweight': 1, '2. Normal weight': 2, '3. Overweight':3, '4. Obese':4, '5. Morbidly obese':5}).astype(float)\n",
    "    data['bmi_12']=data['bmi_12'].replace({'1. Underweight': 1, '2. Normal weight': 2, '3. Overweight':3, '4. Obese':4, '5. Morbidly obese':5}).astype(float)\n",
    "    data['bmi_change']=(data['bmi_12']-data['bmi_03']).astype(float)\n",
    "    data['employment_03']=data['employment_03'].replace({'1. Currently Working': 'Working', '2. Currently looking for work':'Looking for work', '3. Dedicated to household chores': 'House', '4. Retired, incapacitated, or does not work': 'No work'})\n",
    "    data['employment_12']=data['employment_12'].replace({'1. Currently Working': 'Working', '2. Currently looking for work':'Looking for work', '3. Dedicated to household chores': 'House', '4. Retired, incapacitated, or does not work': 'No work'})\n",
    "    data['memory_12']=data['memory_12'].replace({'5. Poor':0, '4. Fair':1, '3. Good':2, '2. Very good': 3, '1. Excellent':4}).astype(float)\n",
    "    data['edu_gru_03']=data['edu_gru_03'].replace({'0. No education':0,'1. 1–5 years':1, '2. 6 years':2, '3. 7–9 years':3,'4. 10+ years':4}).astype(float)\n",
    "    data['edu_gru_12']=data['edu_gru_12'].replace({'0. No education':0,'1. 1–5 years':1, '2. 6 years':2, '3. 7–9 years':3,'4. 10+ years':4}).astype(float)\n",
    "    data['edu_gru_change']=(data['edu_gru_12']-data['edu_gru_03']).astype(float)\n",
    "    data['illnesses_change']=(data['n_illnesses_12']-data['n_illnesses_03']).astype(float)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(data):\n",
    "    data['rjob_hrswk_change']=data['rjob_hrswk_12'].fillna(0)-data['rjob_hrswk_03'].fillna(0).astype(float)\n",
    "    data['rjob_hrswk']=data[['rjob_hrswk_03','rjob_hrswk_12']].mean(axis=1).astype(float)\n",
    "    data['max_work_year']=data[['rjob_end_12','rjob_end_03']].max(axis=1).astype(float)\n",
    "    data['years_since_work']=(data['year']-data['max_work_year']).astype(float)\n",
    "    data['hincome_change']=(data['hincome_12'].fillna(0)-data['hincome_03'].fillna(0)).astype(float)\n",
    "    data['niadl_change']=(data['n_iadl_12'].fillna(0)-data['n_iadl_03'].fillna(0)).astype(float)\n",
    "    data['adl_max']=(data[['n_adl_12','n_adl_03']].fillna(0)).max(axis=1).astype(float)\n",
    "    data['iadl_max']=(data[['n_iadl_12','n_iadl_03']].fillna(0)).max(axis=1).astype(float)\n",
    "    data['neg_adl']=6-data['adl_max']\n",
    "    data['neg_iadl']=4-data['iadl_max']\n",
    "    data['adl_change']=(data['n_adl_12'].fillna(0)-data['n_adl_03'].fillna(0)).astype(float)\n",
    "    data['depr_change']=(data['n_depr_12'].fillna(0)-data['n_depr_03'].fillna(0)).astype(float)\n",
    "    data['glob_hlth_03']=data['glob_hlth_03'].replace({'5. Poor':0, '4. Fair':1, '3. Good':2, '2. Very good': 3, '1. Excellent':4}).astype(float)\n",
    "    data['glob_hlth_12']=data['glob_hlth_12'].replace({'5. Poor':0, '4. Fair':1, '3. Good':2, '2. Very good': 3, '1. Excellent':4}).astype(float)\n",
    "    data['glob_hlth']=data[['glob_hlth_03', 'glob_hlth_12']].sum(axis=1).astype(float)\n",
    "    data['bmi_03']=data['bmi_03'].replace({'1. Underweight': 1, '2. Normal weight': 2, '3. Overweight':3, '4. Obese':4, '5. Morbidly obese':5}).astype(float)\n",
    "    data['bmi_12']=data['bmi_12'].replace({'1. Underweight': 1, '2. Normal weight': 2, '3. Overweight':3, '4. Obese':4, '5. Morbidly obese':5}).astype(float)\n",
    "    data['bmi']=data[['bmi_03', 'bmi_12']].sum(axis=1).astype(float)\n",
    "    data['employment_03']=data['employment_03'].replace({'1. Currently Working': 'Working', '2. Currently looking for work':'Looking for work', '3. Dedicated to household chores': 'House', '4. Retired, incapacitated, or does not work': 'No work'})\n",
    "    data['employment_12']=data['employment_12'].replace({'1. Currently Working': 'Working', '2. Currently looking for work':'Looking for work', '3. Dedicated to household chores': 'House', '4. Retired, incapacitated, or does not work': 'No work'})\n",
    "    data['memory_12']=data['memory_12'].replace({'5. Poor':0, '4. Fair':1, '3. Good':2, '2. Very good': 3, '1. Excellent':4}).astype(float)\n",
    "    data['edu_gru_03']=data['edu_gru_03'].replace({'0. No education':0,'1. 1–5 years':1, '2. 6 years':2, '3. 7–9 years':3,'4. 10+ years':4}).astype(float)\n",
    "    data['edu_gru_12']=data['edu_gru_12'].replace({'0. No education':0,'1. 1–5 years':1, '2. 6 years':2, '3. 7–9 years':3,'4. 10+ years':4}).astype(float)\n",
    "    data['urban_03']=data['urban_03'].fillna(0).replace({ '1. 100,000+':2, '0. <100,000':1}).astype(float)\n",
    "    data['urban_12']=data['urban_03'].fillna(0).replace({ '1. 100,000+':2, '0. <100,000':1}).astype(float)\n",
    "    data['issste']=data[['issste_03', 'issste_12']].fillna(0).max(axis=1).astype(float)\n",
    "    data['urban']=data[['urban_03', 'urban_12']].max(axis=1).astype(float)\n",
    "    data['edu_gru']=data[['edu_gru_03', 'edu_gru_12']].max(axis=1).astype(float)\n",
    "    data['hincome']=data[['hincome_03', 'hincome_12']].max(axis=1).astype(float)\n",
    "    data['illnesses']=data[['n_illnesses_03', 'n_illnesses_12']].max(axis=1).astype(float)\n",
    "    data['alc_tob_03']=data[['alcohol_03','tobacco_03']].sum(axis=1).astype(float)\n",
    "    data['alc_tob_12']=data[['alcohol_12','tobacco_12']].sum(axis=1).astype(float)\n",
    "    data['alc_tob']=data[['alc_tob_03', 'alc_tob_12']].max(axis=1).astype(float)\n",
    "    data['rearnings']=data[['rearnings_03', 'rearnings_12']].max(axis=1).astype(float)\n",
    "    data.drop(columns=['issste_03', 'issste_12','urban_03', 'urban_12','edu_gru_03', 'edu_gru_12','bmi_03', 'bmi_12','alc_tob_03', 'alc_tob_12','alcohol_03','tobacco_03','alcohol_12','tobacco_12','hincome_03', 'hincome_12' ], inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cat_cols(data):\n",
    "    # Get the columns with object datatype\n",
    "    cat_columns=[]\n",
    "    dummies=[]\n",
    "    for col in data.columns:\n",
    "        if data[col].dtype=='object' and 'uid' not in col:\n",
    "            cat_columns.append(col)\n",
    "            dummies.append(col)\n",
    "        elif data[col].dtype!='object' and 'uid' not in col and (data[col].max()==1.0):\n",
    "            cat_columns.append(col)\n",
    "            data[col].fillna(0, inplace=True)\n",
    "        else:\n",
    "            continue\n",
    "    return cat_columns, dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_cat_cols(train_data, cat_cols, dummy_cols):\n",
    "    enc = OneHotEncoder()\n",
    "    enc.fit(train_data[dummy_cols])\n",
    "    encoded_train_data=enc.transform(train_data[dummy_cols]).toarray()\n",
    "    feature_names = enc.get_feature_names_out(dummy_cols)\n",
    "    train_data.drop(columns=dummy_cols, inplace=True)\n",
    "    encoded_train_df = pd.DataFrame(encoded_train_data, columns=feature_names)\n",
    "    train_data[feature_names]=encoded_train_df[feature_names]\n",
    "    return train_data, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4088657/3657557729.py:14: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data['glob_hlth_03']=data['glob_hlth_03'].replace({'5. Poor':0, '4. Fair':1, '3. Good':2, '2. Very good': 3, '1. Excellent':4}).astype(float)\n",
      "/tmp/ipykernel_4088657/3657557729.py:15: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data['glob_hlth_12']=data['glob_hlth_12'].replace({'5. Poor':0, '4. Fair':1, '3. Good':2, '2. Very good': 3, '1. Excellent':4}).astype(float)\n",
      "/tmp/ipykernel_4088657/3657557729.py:17: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data['bmi_03']=data['bmi_03'].replace({'1. Underweight': 1, '2. Normal weight': 2, '3. Overweight':3, '4. Obese':4, '5. Morbidly obese':5}).astype(float)\n",
      "/tmp/ipykernel_4088657/3657557729.py:18: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data['bmi_12']=data['bmi_12'].replace({'1. Underweight': 1, '2. Normal weight': 2, '3. Overweight':3, '4. Obese':4, '5. Morbidly obese':5}).astype(float)\n",
      "/tmp/ipykernel_4088657/3657557729.py:22: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data['memory_12']=data['memory_12'].replace({'5. Poor':0, '4. Fair':1, '3. Good':2, '2. Very good': 3, '1. Excellent':4}).astype(float)\n",
      "/tmp/ipykernel_4088657/3657557729.py:23: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data['edu_gru_03']=data['edu_gru_03'].replace({'0. No education':0,'1. 1–5 years':1, '2. 6 years':2, '3. 7–9 years':3,'4. 10+ years':4}).astype(float)\n",
      "/tmp/ipykernel_4088657/3657557729.py:24: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data['edu_gru_12']=data['edu_gru_12'].replace({'0. No education':0,'1. 1–5 years':1, '2. 6 years':2, '3. 7–9 years':3,'4. 10+ years':4}).astype(float)\n",
      "/tmp/ipykernel_4088657/3657557729.py:25: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data['urban_03']=data['urban_03'].fillna(0).replace({ '1. 100,000+':2, '0. <100,000':1}).astype(float)\n",
      "/tmp/ipykernel_4088657/53275523.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[col].fillna(0, inplace=True)\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/1296090801.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_data[feature_names]=encoded_train_df[feature_names]\n",
      "/tmp/ipykernel_4088657/3102208485.py:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data_processed[col].fillna(0, inplace=True)\n",
      "/tmp/ipykernel_4088657/3102208485.py:16: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data_processed[col].fillna(0, inplace=True)\n",
      "/tmp/ipykernel_4088657/3102208485.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data_processed[col].fillna(0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_mar_03 33.83 % missing\n",
      "glob_hlth_03 36.99 % missing\n",
      "n_adl_03 33.5 % missing\n",
      "n_iadl_03 36.99 % missing\n",
      "n_depr_03 37.1 % missing\n",
      "n_illnesses_03 33.31 % missing\n",
      "decis_personal_03 37.17 % missing\n",
      "glob_hlth_12 5.8 % missing\n",
      "n_iadl_12 5.84 % missing\n",
      "n_depr_12 6.52 % missing\n",
      "memory_12 6.72 % missing\n",
      "rearnings_03 33.37 % missing\n",
      "searnings_03 49.98 % missing\n",
      "hinc_business_03 32.98 % missing\n",
      "hinc_rent_03 32.98 % missing\n",
      "hinc_assets_03 32.98 % missing\n",
      "hinc_cap_03 32.98 % missing\n",
      "rinc_pension_03 33.37 % missing\n",
      "sinc_pension_03 49.98 % missing\n",
      "searnings_12 35.19 % missing\n",
      "sinc_pension_12 35.19 % missing\n",
      "rjob_hrswk 44.62 % missing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4088657/3102208485.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data_processed[col].fillna(0, inplace=True)\n",
      "/tmp/ipykernel_4088657/3102208485.py:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data_processed[col].fillna(0, inplace=True)\n",
      "/tmp/ipykernel_4088657/3102208485.py:16: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data_processed[col].fillna(0, inplace=True)\n",
      "/tmp/ipykernel_4088657/3102208485.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data_processed[col].fillna(0, inplace=True)\n",
      "/tmp/ipykernel_4088657/3102208485.py:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data_processed[col].fillna(0, inplace=True)\n",
      "/tmp/ipykernel_4088657/3102208485.py:16: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data_processed[col].fillna(0, inplace=True)\n",
      "/tmp/ipykernel_4088657/3102208485.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data_processed[col].fillna(0, inplace=True)\n",
      "/tmp/ipykernel_4088657/3102208485.py:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data_processed[col].fillna(0, inplace=True)\n",
      "/tmp/ipykernel_4088657/3102208485.py:16: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data_processed[col].fillna(0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "y=merged_df['composite_score']\n",
    "data_processed=pd.concat((merged_df,merged_test)).reset_index(drop=True).copy()\n",
    "data_processed = feature_engineering(data_processed)\n",
    "cat_cols, dummy_cols = get_cat_cols(data_processed)\n",
    "data_processed, dummy_feature_names=encode_cat_cols(data_processed, cat_cols, dummy_cols)\n",
    "data_processed=data_processed.drop(columns=['composite_score'],axis=1)\n",
    "\n",
    "for col in data_processed.columns: \n",
    "    if round((data_processed[col].isna().sum() /len(data_processed)*100), 2)>50:\n",
    "        data_processed.drop(columns=col, inplace=True)\n",
    "    elif round((data_processed[col].isna().sum() /len(data_processed)*100), 2)>5:\n",
    "        print(col,round((data_processed[col].isna().sum() /len(data_processed)*100), 2), '% missing')\n",
    "        data_processed[col].fillna(0, inplace=True)\n",
    "    else: \n",
    "        data_processed[col].fillna(0, inplace=True)\n",
    "        data_processed[col].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4088657/3657557729.py:14: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data['glob_hlth_03']=data['glob_hlth_03'].replace({'5. Poor':0, '4. Fair':1, '3. Good':2, '2. Very good': 3, '1. Excellent':4}).astype(float)\n",
      "/tmp/ipykernel_4088657/3657557729.py:15: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data['glob_hlth_12']=data['glob_hlth_12'].replace({'5. Poor':0, '4. Fair':1, '3. Good':2, '2. Very good': 3, '1. Excellent':4}).astype(float)\n",
      "/tmp/ipykernel_4088657/3657557729.py:17: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data['bmi_03']=data['bmi_03'].replace({'1. Underweight': 1, '2. Normal weight': 2, '3. Overweight':3, '4. Obese':4, '5. Morbidly obese':5}).astype(float)\n",
      "/tmp/ipykernel_4088657/3657557729.py:18: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data['bmi_12']=data['bmi_12'].replace({'1. Underweight': 1, '2. Normal weight': 2, '3. Overweight':3, '4. Obese':4, '5. Morbidly obese':5}).astype(float)\n",
      "/tmp/ipykernel_4088657/3657557729.py:22: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data['memory_12']=data['memory_12'].replace({'5. Poor':0, '4. Fair':1, '3. Good':2, '2. Very good': 3, '1. Excellent':4}).astype(float)\n",
      "/tmp/ipykernel_4088657/3657557729.py:23: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data['edu_gru_03']=data['edu_gru_03'].replace({'0. No education':0,'1. 1–5 years':1, '2. 6 years':2, '3. 7–9 years':3,'4. 10+ years':4}).astype(float)\n",
      "/tmp/ipykernel_4088657/3657557729.py:24: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data['edu_gru_12']=data['edu_gru_12'].replace({'0. No education':0,'1. 1–5 years':1, '2. 6 years':2, '3. 7–9 years':3,'4. 10+ years':4}).astype(float)\n",
      "/tmp/ipykernel_4088657/3657557729.py:25: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data['urban_03']=data['urban_03'].fillna(0).replace({ '1. 100,000+':2, '0. <100,000':1}).astype(float)\n"
     ]
    }
   ],
   "source": [
    "y=merged_df['composite_score']\n",
    "data=pd.concat((merged_df,merged_test)).reset_index(drop=True).copy()\n",
    "data = feature_engineering(data)\n",
    "data=data.drop(columns=['composite_score'],axis=1)\n",
    "\n",
    "# Get the columns with object datatype\n",
    "object_cols = data.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Convert the object columns to category dtype\n",
    "for col in object_cols:\n",
    "    #data[col] = data[col].astype('category').fillna(\"Missing\")\n",
    "    data[col] = pd.Categorical(data[col].fillna(\"Missing\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate train and test\n",
    "merged_df_processed=data_processed[:len(merged_df)]\n",
    "merged_test_processed=data_processed[len(merged_df):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate train and test\n",
    "merged_df=data[:len(merged_df)]\n",
    "merged_test=data[len(merged_df):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4343, 192)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4343, 344)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df_processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/tmp/ipykernel_4088657/812947124.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_X.drop(columns=['uid'], inplace=True)\n",
      "/tmp/ipykernel_4088657/812947124.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  val_X.drop(columns=['uid'], inplace=True)\n",
      "/tmp/ipykernel_4088657/812947124.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_X_processed.drop(columns=['uid'], inplace=True)\n",
      "/tmp/ipykernel_4088657/812947124.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  val_X_processed.drop(columns=['uid'], inplace=True)\n",
      "/u/spa-d4/grad/mfe261/Projects/PREPARE/envs/lib/python3.9/site-packages/lightgbm/engine.py:204: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001018 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2145\n",
      "[LightGBM] [Info] Number of data points in the train set: 3458, number of used features: 191\n",
      "[LightGBM] [Info] Start training from score 155.317235\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "Early stopping, best iteration is:\n",
      "[566]\tvalid_0's rmse: 41.4669\n",
      "0:\tlearn: 60.4168030\ttest: 61.5679009\tbest: 61.5679009 (0)\ttotal: 18.8ms\tremaining: 3m 8s\n",
      "100:\tlearn: 44.5937991\ttest: 47.6170005\tbest: 47.6170005 (100)\ttotal: 1.97s\tremaining: 3m 12s\n",
      "200:\tlearn: 38.1907064\ttest: 43.6678466\tbest: 43.6678466 (200)\ttotal: 3.98s\tremaining: 3m 13s\n",
      "300:\tlearn: 34.5961904\ttest: 42.3948259\tbest: 42.3948259 (300)\ttotal: 6.11s\tremaining: 3m 16s\n",
      "400:\tlearn: 32.2537629\ttest: 41.8220734\tbest: 41.8220734 (400)\ttotal: 8.29s\tremaining: 3m 18s\n",
      "500:\tlearn: 30.5841816\ttest: 41.5526941\tbest: 41.5526941 (500)\ttotal: 10.4s\tremaining: 3m 17s\n",
      "600:\tlearn: 29.2482164\ttest: 41.3566206\tbest: 41.3535612 (598)\ttotal: 12.5s\tremaining: 3m 15s\n",
      "700:\tlearn: 28.0909200\ttest: 41.2500941\tbest: 41.2488027 (695)\ttotal: 14.9s\tremaining: 3m 17s\n",
      "800:\tlearn: 27.0435351\ttest: 41.1895187\tbest: 41.1874303 (794)\ttotal: 17.2s\tremaining: 3m 17s\n",
      "900:\tlearn: 26.1814705\ttest: 41.1325024\tbest: 41.1315521 (899)\ttotal: 19.4s\tremaining: 3m 16s\n",
      "1000:\tlearn: 25.3526979\ttest: 41.0987300\tbest: 41.0970816 (998)\ttotal: 21.8s\tremaining: 3m 15s\n",
      "1100:\tlearn: 24.5247883\ttest: 41.0901029\tbest: 41.0692581 (1084)\ttotal: 24.3s\tremaining: 3m 16s\n",
      "1200:\tlearn: 23.8062630\ttest: 41.0995809\tbest: 41.0692581 (1084)\ttotal: 27s\tremaining: 3m 17s\n",
      "1300:\tlearn: 23.0178672\ttest: 41.0796329\tbest: 41.0692581 (1084)\ttotal: 29.6s\tremaining: 3m 17s\n",
      "1400:\tlearn: 22.2808869\ttest: 41.0798638\tbest: 41.0692581 (1084)\ttotal: 31.9s\tremaining: 3m 16s\n",
      "1500:\tlearn: 21.6107550\ttest: 41.0852537\tbest: 41.0692581 (1084)\ttotal: 34.3s\tremaining: 3m 14s\n",
      "Stopped by overfitting detector  (500 iterations wait)\n",
      "\n",
      "bestTest = 41.06925814\n",
      "bestIteration = 1084\n",
      "\n",
      "Shrink model to first 1085 iterations.\n",
      "[0]\tvalidation_0-rmse:61.55561\tvalidation_0-root_mean_squared_error:61.55561\n",
      "[100]\tvalidation_0-rmse:46.96961\tvalidation_0-root_mean_squared_error:46.96961\n",
      "[200]\tvalidation_0-rmse:43.45253\tvalidation_0-root_mean_squared_error:43.45253\n",
      "[300]\tvalidation_0-rmse:42.05645\tvalidation_0-root_mean_squared_error:42.05645\n",
      "[400]\tvalidation_0-rmse:41.42311\tvalidation_0-root_mean_squared_error:41.42311\n",
      "[500]\tvalidation_0-rmse:41.05388\tvalidation_0-root_mean_squared_error:41.05387\n",
      "[600]\tvalidation_0-rmse:40.89688\tvalidation_0-root_mean_squared_error:40.89688\n",
      "[700]\tvalidation_0-rmse:40.85803\tvalidation_0-root_mean_squared_error:40.85803\n",
      "[800]\tvalidation_0-rmse:40.85633\tvalidation_0-root_mean_squared_error:40.85634\n",
      "[900]\tvalidation_0-rmse:40.86394\tvalidation_0-root_mean_squared_error:40.86394\n",
      "[1000]\tvalidation_0-rmse:40.88035\tvalidation_0-root_mean_squared_error:40.88035\n",
      "[1100]\tvalidation_0-rmse:40.89068\tvalidation_0-root_mean_squared_error:40.89068\n",
      "[1200]\tvalidation_0-rmse:40.89061\tvalidation_0-root_mean_squared_error:40.89061\n",
      "[1234]\tvalidation_0-rmse:40.89138\tvalidation_0-root_mean_squared_error:40.89138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [02:06, 126.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest rmse:  41.748259451021795\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003860 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2123\n",
      "[LightGBM] [Info] Number of data points in the train set: 3481, number of used features: 191\n",
      "[LightGBM] [Info] Start training from score 157.507613\n",
      "Training until validation scores don't improve for 500 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4088657/812947124.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_X.drop(columns=['uid'], inplace=True)\n",
      "/tmp/ipykernel_4088657/812947124.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  val_X.drop(columns=['uid'], inplace=True)\n",
      "/tmp/ipykernel_4088657/812947124.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_X_processed.drop(columns=['uid'], inplace=True)\n",
      "/tmp/ipykernel_4088657/812947124.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  val_X_processed.drop(columns=['uid'], inplace=True)\n",
      "/u/spa-d4/grad/mfe261/Projects/PREPARE/envs/lib/python3.9/site-packages/lightgbm/engine.py:204: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[513]\tvalid_0's rmse: 40.8943\n",
      "0:\tlearn: 60.4857861\ttest: 61.3455246\tbest: 61.3455246 (0)\ttotal: 71ms\tremaining: 11m 50s\n",
      "100:\tlearn: 44.8053371\ttest: 47.3094533\tbest: 47.3094533 (100)\ttotal: 2.2s\tremaining: 3m 35s\n",
      "200:\tlearn: 38.5413460\ttest: 43.0965590\tbest: 43.0965590 (200)\ttotal: 4.38s\tremaining: 3m 33s\n",
      "300:\tlearn: 35.1087586\ttest: 41.6177492\tbest: 41.6177492 (300)\ttotal: 6.49s\tremaining: 3m 29s\n",
      "400:\tlearn: 32.6986038\ttest: 41.0700953\tbest: 41.0700953 (400)\ttotal: 8.79s\tremaining: 3m 30s\n",
      "500:\tlearn: 31.0166469\ttest: 40.7906204\tbest: 40.7906204 (500)\ttotal: 11.3s\tremaining: 3m 34s\n",
      "600:\tlearn: 29.8005535\ttest: 40.6769969\tbest: 40.6769969 (600)\ttotal: 13.3s\tremaining: 3m 28s\n",
      "700:\tlearn: 28.5662821\ttest: 40.5586240\tbest: 40.5581302 (694)\ttotal: 15.7s\tremaining: 3m 28s\n",
      "800:\tlearn: 27.4737915\ttest: 40.5081112\tbest: 40.5081112 (800)\ttotal: 18.1s\tremaining: 3m 27s\n",
      "900:\tlearn: 26.5661569\ttest: 40.4544409\tbest: 40.4525333 (896)\ttotal: 20.5s\tremaining: 3m 27s\n",
      "1000:\tlearn: 25.6205816\ttest: 40.4242128\tbest: 40.4205100 (989)\ttotal: 23s\tremaining: 3m 26s\n",
      "1100:\tlearn: 24.7148961\ttest: 40.4266303\tbest: 40.4155489 (1071)\ttotal: 25.3s\tremaining: 3m 24s\n",
      "1200:\tlearn: 23.9828354\ttest: 40.4280269\tbest: 40.4155489 (1071)\ttotal: 27.8s\tremaining: 3m 24s\n",
      "1300:\tlearn: 23.2736263\ttest: 40.4296692\tbest: 40.4113990 (1240)\ttotal: 30.4s\tremaining: 3m 22s\n",
      "1400:\tlearn: 22.5881647\ttest: 40.4036647\tbest: 40.3988055 (1384)\ttotal: 32.9s\tremaining: 3m 21s\n",
      "1500:\tlearn: 21.9595067\ttest: 40.3938118\tbest: 40.3911932 (1497)\ttotal: 35.3s\tremaining: 3m 19s\n",
      "1600:\tlearn: 21.3755217\ttest: 40.3933759\tbest: 40.3894350 (1546)\ttotal: 37.8s\tremaining: 3m 18s\n",
      "1700:\tlearn: 20.7689100\ttest: 40.3801583\tbest: 40.3753715 (1688)\ttotal: 40.2s\tremaining: 3m 16s\n",
      "1800:\tlearn: 20.2389817\ttest: 40.3784279\tbest: 40.3728848 (1739)\ttotal: 42.7s\tremaining: 3m 14s\n",
      "1900:\tlearn: 19.7233453\ttest: 40.3654879\tbest: 40.3630876 (1894)\ttotal: 45.4s\tremaining: 3m 13s\n",
      "2000:\tlearn: 19.2797558\ttest: 40.3689243\tbest: 40.3624221 (1930)\ttotal: 48.1s\tremaining: 3m 12s\n",
      "2100:\tlearn: 18.7726870\ttest: 40.3862797\tbest: 40.3624221 (1930)\ttotal: 50.8s\tremaining: 3m 11s\n",
      "2200:\tlearn: 18.3146295\ttest: 40.3937103\tbest: 40.3624221 (1930)\ttotal: 53.5s\tremaining: 3m 9s\n",
      "2300:\tlearn: 17.8320824\ttest: 40.4057940\tbest: 40.3624221 (1930)\ttotal: 56.2s\tremaining: 3m 7s\n",
      "2400:\tlearn: 17.4205388\ttest: 40.4125819\tbest: 40.3624221 (1930)\ttotal: 58.8s\tremaining: 3m 6s\n",
      "Stopped by overfitting detector  (500 iterations wait)\n",
      "\n",
      "bestTest = 40.36242206\n",
      "bestIteration = 1930\n",
      "\n",
      "Shrink model to first 1931 iterations.\n",
      "[0]\tvalidation_0-rmse:61.28281\tvalidation_0-root_mean_squared_error:61.28281\n",
      "[100]\tvalidation_0-rmse:47.04201\tvalidation_0-root_mean_squared_error:47.04201\n",
      "[200]\tvalidation_0-rmse:43.35827\tvalidation_0-root_mean_squared_error:43.35827\n",
      "[300]\tvalidation_0-rmse:42.05035\tvalidation_0-root_mean_squared_error:42.05035\n",
      "[400]\tvalidation_0-rmse:41.47768\tvalidation_0-root_mean_squared_error:41.47768\n",
      "[500]\tvalidation_0-rmse:41.20758\tvalidation_0-root_mean_squared_error:41.20758\n",
      "[600]\tvalidation_0-rmse:41.07760\tvalidation_0-root_mean_squared_error:41.07760\n",
      "[700]\tvalidation_0-rmse:40.99566\tvalidation_0-root_mean_squared_error:40.99565\n",
      "[800]\tvalidation_0-rmse:40.95850\tvalidation_0-root_mean_squared_error:40.95850\n",
      "[900]\tvalidation_0-rmse:40.95329\tvalidation_0-root_mean_squared_error:40.95329\n",
      "[1000]\tvalidation_0-rmse:40.97331\tvalidation_0-root_mean_squared_error:40.97331\n",
      "[1100]\tvalidation_0-rmse:41.00689\tvalidation_0-root_mean_squared_error:41.00689\n",
      "[1200]\tvalidation_0-rmse:41.04049\tvalidation_0-root_mean_squared_error:41.04049\n",
      "[1300]\tvalidation_0-rmse:41.10704\tvalidation_0-root_mean_squared_error:41.10704\n",
      "[1383]\tvalidation_0-rmse:41.15721\tvalidation_0-root_mean_squared_error:41.15720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [04:37, 141.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest rmse:  41.66423377920627\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000963 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2143\n",
      "[LightGBM] [Info] Number of data points in the train set: 3477, number of used features: 191\n",
      "[LightGBM] [Info] Start training from score 158.134311\n",
      "Training until validation scores don't improve for 500 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4088657/812947124.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_X.drop(columns=['uid'], inplace=True)\n",
      "/tmp/ipykernel_4088657/812947124.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  val_X.drop(columns=['uid'], inplace=True)\n",
      "/tmp/ipykernel_4088657/812947124.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_X_processed.drop(columns=['uid'], inplace=True)\n",
      "/tmp/ipykernel_4088657/812947124.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  val_X_processed.drop(columns=['uid'], inplace=True)\n",
      "/u/spa-d4/grad/mfe261/Projects/PREPARE/envs/lib/python3.9/site-packages/lightgbm/engine.py:204: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[719]\tvalid_0's rmse: 40.7245\n",
      "0:\tlearn: 61.4127208\ttest: 57.4644721\tbest: 57.4644721 (0)\ttotal: 19.6ms\tremaining: 3m 16s\n",
      "100:\tlearn: 44.9142543\ttest: 45.8914122\tbest: 45.8914122 (100)\ttotal: 2.07s\tremaining: 3m 22s\n",
      "200:\tlearn: 38.5028299\ttest: 42.8167070\tbest: 42.8167070 (200)\ttotal: 4.18s\tremaining: 3m 23s\n",
      "300:\tlearn: 35.1051353\ttest: 41.7391575\tbest: 41.7391575 (300)\ttotal: 6.35s\tremaining: 3m 24s\n",
      "400:\tlearn: 32.8633241\ttest: 41.2051487\tbest: 41.2051487 (400)\ttotal: 8.47s\tremaining: 3m 22s\n",
      "500:\tlearn: 31.3277293\ttest: 40.9920666\tbest: 40.9904539 (499)\ttotal: 10.7s\tremaining: 3m 22s\n",
      "600:\tlearn: 30.0056054\ttest: 40.8528742\tbest: 40.8526631 (599)\ttotal: 12.9s\tremaining: 3m 21s\n",
      "700:\tlearn: 28.8019366\ttest: 40.7679185\tbest: 40.7679185 (700)\ttotal: 15.1s\tremaining: 3m 19s\n",
      "800:\tlearn: 27.6942961\ttest: 40.6962643\tbest: 40.6962072 (797)\ttotal: 17.3s\tremaining: 3m 19s\n",
      "900:\tlearn: 26.7630937\ttest: 40.6603686\tbest: 40.6576572 (898)\ttotal: 19.6s\tremaining: 3m 18s\n",
      "1000:\tlearn: 25.9512212\ttest: 40.6526114\tbest: 40.6493429 (989)\ttotal: 22s\tremaining: 3m 17s\n",
      "1100:\tlearn: 25.1574189\ttest: 40.6342620\tbest: 40.6307237 (1098)\ttotal: 24.4s\tremaining: 3m 17s\n",
      "1200:\tlearn: 24.4605090\ttest: 40.5991645\tbest: 40.5985807 (1196)\ttotal: 26.8s\tremaining: 3m 16s\n",
      "1300:\tlearn: 23.7747399\ttest: 40.5637530\tbest: 40.5637530 (1300)\ttotal: 29.4s\tremaining: 3m 16s\n",
      "1400:\tlearn: 23.0739786\ttest: 40.5522813\tbest: 40.5520355 (1399)\ttotal: 32s\tremaining: 3m 16s\n",
      "1500:\tlearn: 22.3760496\ttest: 40.5320548\tbest: 40.5299732 (1491)\ttotal: 34.7s\tremaining: 3m 16s\n",
      "1600:\tlearn: 21.6839763\ttest: 40.5117897\tbest: 40.5102356 (1598)\ttotal: 37.6s\tremaining: 3m 17s\n",
      "1700:\tlearn: 21.0251791\ttest: 40.5042545\tbest: 40.5010250 (1675)\ttotal: 40.4s\tremaining: 3m 16s\n",
      "1800:\tlearn: 20.4473025\ttest: 40.4978126\tbest: 40.4925009 (1788)\ttotal: 43.5s\tremaining: 3m 18s\n",
      "1900:\tlearn: 19.8684394\ttest: 40.4981089\tbest: 40.4886231 (1859)\ttotal: 46.3s\tremaining: 3m 17s\n",
      "2000:\tlearn: 19.3597197\ttest: 40.4851531\tbest: 40.4830426 (1994)\ttotal: 49s\tremaining: 3m 15s\n",
      "2100:\tlearn: 18.8700868\ttest: 40.4875327\tbest: 40.4816988 (2008)\ttotal: 51.7s\tremaining: 3m 14s\n",
      "2200:\tlearn: 18.4090818\ttest: 40.4852018\tbest: 40.4794940 (2123)\ttotal: 54.4s\tremaining: 3m 12s\n",
      "2300:\tlearn: 17.9198529\ttest: 40.4943784\tbest: 40.4794940 (2123)\ttotal: 57.1s\tremaining: 3m 10s\n",
      "2400:\tlearn: 17.4877233\ttest: 40.4946742\tbest: 40.4794940 (2123)\ttotal: 59.8s\tremaining: 3m 9s\n",
      "2500:\tlearn: 17.0866965\ttest: 40.4928228\tbest: 40.4794940 (2123)\ttotal: 1m 2s\tremaining: 3m 7s\n",
      "2600:\tlearn: 16.6958250\ttest: 40.4963816\tbest: 40.4794940 (2123)\ttotal: 1m 5s\tremaining: 3m 5s\n",
      "Stopped by overfitting detector  (500 iterations wait)\n",
      "\n",
      "bestTest = 40.479494\n",
      "bestIteration = 2123\n",
      "\n",
      "Shrink model to first 2124 iterations.\n",
      "[0]\tvalidation_0-rmse:57.44412\tvalidation_0-root_mean_squared_error:57.44411\n",
      "[100]\tvalidation_0-rmse:45.36278\tvalidation_0-root_mean_squared_error:45.36278\n",
      "[200]\tvalidation_0-rmse:42.58645\tvalidation_0-root_mean_squared_error:42.58645\n",
      "[300]\tvalidation_0-rmse:41.66265\tvalidation_0-root_mean_squared_error:41.66264\n",
      "[400]\tvalidation_0-rmse:41.17070\tvalidation_0-root_mean_squared_error:41.17070\n",
      "[500]\tvalidation_0-rmse:40.89789\tvalidation_0-root_mean_squared_error:40.89789\n",
      "[600]\tvalidation_0-rmse:40.72739\tvalidation_0-root_mean_squared_error:40.72739\n",
      "[700]\tvalidation_0-rmse:40.62906\tvalidation_0-root_mean_squared_error:40.62906\n",
      "[800]\tvalidation_0-rmse:40.66737\tvalidation_0-root_mean_squared_error:40.66737\n",
      "[900]\tvalidation_0-rmse:40.67442\tvalidation_0-root_mean_squared_error:40.67442\n",
      "[1000]\tvalidation_0-rmse:40.70954\tvalidation_0-root_mean_squared_error:40.70954\n",
      "[1100]\tvalidation_0-rmse:40.75118\tvalidation_0-root_mean_squared_error:40.75118\n",
      "[1200]\tvalidation_0-rmse:40.77013\tvalidation_0-root_mean_squared_error:40.77013\n",
      "[1202]\tvalidation_0-rmse:40.77060\tvalidation_0-root_mean_squared_error:40.77060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [07:16, 149.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest rmse:  41.427066112917274\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001111 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2143\n",
      "[LightGBM] [Info] Number of data points in the train set: 3474, number of used features: 191\n",
      "[LightGBM] [Info] Start training from score 156.698043\n",
      "Training until validation scores don't improve for 500 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4088657/812947124.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_X.drop(columns=['uid'], inplace=True)\n",
      "/tmp/ipykernel_4088657/812947124.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  val_X.drop(columns=['uid'], inplace=True)\n",
      "/tmp/ipykernel_4088657/812947124.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_X_processed.drop(columns=['uid'], inplace=True)\n",
      "/tmp/ipykernel_4088657/812947124.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  val_X_processed.drop(columns=['uid'], inplace=True)\n",
      "/u/spa-d4/grad/mfe261/Projects/PREPARE/envs/lib/python3.9/site-packages/lightgbm/engine.py:204: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[641]\tvalid_0's rmse: 41.3587\n",
      "0:\tlearn: 60.7821425\ttest: 60.1902740\tbest: 60.1902740 (0)\ttotal: 18.3ms\tremaining: 3m 3s\n",
      "100:\tlearn: 45.0383170\ttest: 46.8658886\tbest: 46.8658886 (100)\ttotal: 1.91s\tremaining: 3m 6s\n",
      "200:\tlearn: 38.7973291\ttest: 42.9138207\tbest: 42.9138207 (200)\ttotal: 3.94s\tremaining: 3m 12s\n",
      "300:\tlearn: 35.3267313\ttest: 41.5486686\tbest: 41.5486686 (300)\ttotal: 6.25s\tremaining: 3m 21s\n",
      "400:\tlearn: 33.1527358\ttest: 40.9036930\tbest: 40.9029538 (399)\ttotal: 8.54s\tremaining: 3m 24s\n",
      "500:\tlearn: 31.4253161\ttest: 40.5662281\tbest: 40.5639344 (499)\ttotal: 10.7s\tremaining: 3m 22s\n",
      "600:\tlearn: 30.1282700\ttest: 40.4129507\tbest: 40.4116121 (597)\ttotal: 13s\tremaining: 3m 23s\n",
      "700:\tlearn: 29.0150620\ttest: 40.3082729\tbest: 40.3060203 (699)\ttotal: 15.4s\tremaining: 3m 24s\n",
      "800:\tlearn: 28.0039996\ttest: 40.2173545\tbest: 40.2164407 (798)\ttotal: 17.8s\tremaining: 3m 23s\n",
      "900:\tlearn: 27.0990307\ttest: 40.1462811\tbest: 40.1422933 (894)\ttotal: 20s\tremaining: 3m 22s\n",
      "1000:\tlearn: 26.2869699\ttest: 40.1282204\tbest: 40.1204270 (974)\ttotal: 22.6s\tremaining: 3m 23s\n",
      "1100:\tlearn: 25.5421341\ttest: 40.0850622\tbest: 40.0850622 (1100)\ttotal: 25.3s\tremaining: 3m 24s\n",
      "1200:\tlearn: 24.6864873\ttest: 40.0318958\tbest: 40.0318958 (1200)\ttotal: 27.8s\tremaining: 3m 24s\n",
      "1300:\tlearn: 24.0432593\ttest: 40.0126995\tbest: 40.0113020 (1295)\ttotal: 30.6s\tremaining: 3m 24s\n",
      "1400:\tlearn: 23.3927272\ttest: 39.9924927\tbest: 39.9924627 (1398)\ttotal: 33.2s\tremaining: 3m 23s\n",
      "1500:\tlearn: 22.7550661\ttest: 39.9891867\tbest: 39.9822359 (1441)\ttotal: 35.9s\tremaining: 3m 23s\n",
      "1600:\tlearn: 22.2556712\ttest: 39.9698589\tbest: 39.9697753 (1599)\ttotal: 38.5s\tremaining: 3m 21s\n",
      "1700:\tlearn: 21.7405539\ttest: 39.9653736\tbest: 39.9630155 (1668)\ttotal: 41.2s\tremaining: 3m 21s\n",
      "1800:\tlearn: 21.1485179\ttest: 39.9630254\tbest: 39.9525779 (1734)\ttotal: 43.8s\tremaining: 3m 19s\n",
      "1900:\tlearn: 20.6453275\ttest: 39.9505035\tbest: 39.9503834 (1896)\ttotal: 46.5s\tremaining: 3m 17s\n",
      "2000:\tlearn: 20.1642511\ttest: 39.9481482\tbest: 39.9442579 (1909)\ttotal: 49.2s\tremaining: 3m 16s\n",
      "2100:\tlearn: 19.6477170\ttest: 39.9464492\tbest: 39.9413925 (2079)\ttotal: 51.9s\tremaining: 3m 15s\n",
      "2200:\tlearn: 19.1792435\ttest: 39.9576972\tbest: 39.9413925 (2079)\ttotal: 54.7s\tremaining: 3m 13s\n",
      "2300:\tlearn: 18.7108987\ttest: 39.9619433\tbest: 39.9413925 (2079)\ttotal: 57.5s\tremaining: 3m 12s\n",
      "2400:\tlearn: 18.3041549\ttest: 39.9499952\tbest: 39.9413925 (2079)\ttotal: 1m\tremaining: 3m 10s\n",
      "2500:\tlearn: 17.9089209\ttest: 39.9478605\tbest: 39.9373620 (2446)\ttotal: 1m 3s\tremaining: 3m 8s\n",
      "2600:\tlearn: 17.4910296\ttest: 39.9479609\tbest: 39.9373620 (2446)\ttotal: 1m 5s\tremaining: 3m 6s\n",
      "2700:\tlearn: 17.0965090\ttest: 39.9497499\tbest: 39.9373620 (2446)\ttotal: 1m 8s\tremaining: 3m 4s\n",
      "2800:\tlearn: 16.7167918\ttest: 39.9545481\tbest: 39.9373620 (2446)\ttotal: 1m 11s\tremaining: 3m 2s\n",
      "2900:\tlearn: 16.3523691\ttest: 39.9670115\tbest: 39.9373620 (2446)\ttotal: 1m 13s\tremaining: 3m\n",
      "Stopped by overfitting detector  (500 iterations wait)\n",
      "\n",
      "bestTest = 39.93736203\n",
      "bestIteration = 2446\n",
      "\n",
      "Shrink model to first 2447 iterations.\n",
      "[0]\tvalidation_0-rmse:60.13788\tvalidation_0-root_mean_squared_error:60.13788\n",
      "[100]\tvalidation_0-rmse:47.03163\tvalidation_0-root_mean_squared_error:47.03164\n",
      "[200]\tvalidation_0-rmse:43.73816\tvalidation_0-root_mean_squared_error:43.73816\n",
      "[300]\tvalidation_0-rmse:42.25010\tvalidation_0-root_mean_squared_error:42.25010\n",
      "[400]\tvalidation_0-rmse:41.49644\tvalidation_0-root_mean_squared_error:41.49643\n",
      "[500]\tvalidation_0-rmse:41.11982\tvalidation_0-root_mean_squared_error:41.11983\n",
      "[600]\tvalidation_0-rmse:40.89930\tvalidation_0-root_mean_squared_error:40.89930\n",
      "[700]\tvalidation_0-rmse:40.77594\tvalidation_0-root_mean_squared_error:40.77594\n",
      "[800]\tvalidation_0-rmse:40.64666\tvalidation_0-root_mean_squared_error:40.64666\n",
      "[900]\tvalidation_0-rmse:40.57652\tvalidation_0-root_mean_squared_error:40.57652\n",
      "[1000]\tvalidation_0-rmse:40.52073\tvalidation_0-root_mean_squared_error:40.52073\n",
      "[1100]\tvalidation_0-rmse:40.47686\tvalidation_0-root_mean_squared_error:40.47686\n",
      "[1200]\tvalidation_0-rmse:40.43911\tvalidation_0-root_mean_squared_error:40.43911\n",
      "[1300]\tvalidation_0-rmse:40.39754\tvalidation_0-root_mean_squared_error:40.39754\n",
      "[1400]\tvalidation_0-rmse:40.37978\tvalidation_0-root_mean_squared_error:40.37978\n",
      "[1500]\tvalidation_0-rmse:40.36500\tvalidation_0-root_mean_squared_error:40.36500\n",
      "[1600]\tvalidation_0-rmse:40.35994\tvalidation_0-root_mean_squared_error:40.35994\n",
      "[1700]\tvalidation_0-rmse:40.36749\tvalidation_0-root_mean_squared_error:40.36749\n",
      "[1800]\tvalidation_0-rmse:40.36114\tvalidation_0-root_mean_squared_error:40.36114\n",
      "[1900]\tvalidation_0-rmse:40.37430\tvalidation_0-root_mean_squared_error:40.37430\n",
      "[2000]\tvalidation_0-rmse:40.39835\tvalidation_0-root_mean_squared_error:40.39835\n",
      "[2063]\tvalidation_0-rmse:40.40729\tvalidation_0-root_mean_squared_error:40.40729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [10:06, 157.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest rmse:  41.794125470922005\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000976 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2134\n",
      "[LightGBM] [Info] Number of data points in the train set: 3482, number of used features: 191\n",
      "[LightGBM] [Info] Start training from score 157.416140\n",
      "Training until validation scores don't improve for 500 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4088657/812947124.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_X.drop(columns=['uid'], inplace=True)\n",
      "/tmp/ipykernel_4088657/812947124.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  val_X.drop(columns=['uid'], inplace=True)\n",
      "/tmp/ipykernel_4088657/812947124.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_X_processed.drop(columns=['uid'], inplace=True)\n",
      "/tmp/ipykernel_4088657/812947124.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  val_X_processed.drop(columns=['uid'], inplace=True)\n",
      "/u/spa-d4/grad/mfe261/Projects/PREPARE/envs/lib/python3.9/site-packages/lightgbm/engine.py:204: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[869]\tvalid_0's rmse: 40.9353\n",
      "0:\tlearn: 60.0797906\ttest: 62.9789894\tbest: 62.9789894 (0)\ttotal: 19.3ms\tremaining: 3m 12s\n",
      "100:\tlearn: 44.8446074\ttest: 48.2909896\tbest: 48.2909896 (100)\ttotal: 2.05s\tremaining: 3m 20s\n",
      "200:\tlearn: 38.6815887\ttest: 43.5882581\tbest: 43.5882581 (200)\ttotal: 4.22s\tremaining: 3m 25s\n",
      "300:\tlearn: 35.2845062\ttest: 41.8294993\tbest: 41.8294993 (300)\ttotal: 6.37s\tremaining: 3m 25s\n",
      "400:\tlearn: 33.0506511\ttest: 41.0769311\tbest: 41.0769311 (400)\ttotal: 8.83s\tremaining: 3m 31s\n",
      "500:\tlearn: 31.5091834\ttest: 40.6672291\tbest: 40.6672291 (500)\ttotal: 11s\tremaining: 3m 29s\n",
      "600:\tlearn: 30.0792993\ttest: 40.4652895\tbest: 40.4652895 (600)\ttotal: 13.3s\tremaining: 3m 27s\n",
      "700:\tlearn: 29.0223375\ttest: 40.3624922\tbest: 40.3605087 (688)\ttotal: 15.5s\tremaining: 3m 25s\n",
      "800:\tlearn: 27.9885733\ttest: 40.2379044\tbest: 40.2379044 (800)\ttotal: 17.7s\tremaining: 3m 23s\n",
      "900:\tlearn: 26.9698613\ttest: 40.1478093\tbest: 40.1478093 (900)\ttotal: 20.3s\tremaining: 3m 25s\n",
      "1000:\tlearn: 26.0749420\ttest: 40.1021833\tbest: 40.0947234 (975)\ttotal: 23s\tremaining: 3m 26s\n",
      "1100:\tlearn: 25.3317543\ttest: 40.0396724\tbest: 40.0396724 (1100)\ttotal: 25.5s\tremaining: 3m 26s\n",
      "1200:\tlearn: 24.6766451\ttest: 40.0318525\tbest: 40.0272270 (1178)\ttotal: 28.1s\tremaining: 3m 25s\n",
      "1300:\tlearn: 24.0446581\ttest: 40.0199626\tbest: 40.0136378 (1284)\ttotal: 30.5s\tremaining: 3m 24s\n",
      "1400:\tlearn: 23.3250363\ttest: 40.0042543\tbest: 40.0036672 (1395)\ttotal: 33s\tremaining: 3m 22s\n",
      "1500:\tlearn: 22.6753745\ttest: 39.9735115\tbest: 39.9735115 (1500)\ttotal: 35.4s\tremaining: 3m 20s\n",
      "1600:\tlearn: 22.0414270\ttest: 39.9563773\tbest: 39.9563773 (1600)\ttotal: 38s\tremaining: 3m 19s\n",
      "1700:\tlearn: 21.4179185\ttest: 39.9730193\tbest: 39.9547869 (1671)\ttotal: 40.4s\tremaining: 3m 16s\n",
      "1800:\tlearn: 20.7790293\ttest: 39.9687526\tbest: 39.9547869 (1671)\ttotal: 42.9s\tremaining: 3m 15s\n",
      "1900:\tlearn: 20.1792112\ttest: 39.9571159\tbest: 39.9498277 (1875)\ttotal: 45.4s\tremaining: 3m 13s\n",
      "2000:\tlearn: 19.5975169\ttest: 39.9658346\tbest: 39.9498277 (1875)\ttotal: 47.9s\tremaining: 3m 11s\n",
      "2100:\tlearn: 19.0189313\ttest: 39.9774984\tbest: 39.9498277 (1875)\ttotal: 50.4s\tremaining: 3m 9s\n",
      "2200:\tlearn: 18.5242990\ttest: 39.9807480\tbest: 39.9498277 (1875)\ttotal: 53.1s\tremaining: 3m 8s\n",
      "2300:\tlearn: 17.9789154\ttest: 39.9659026\tbest: 39.9498277 (1875)\ttotal: 55.6s\tremaining: 3m 6s\n",
      "Stopped by overfitting detector  (500 iterations wait)\n",
      "\n",
      "bestTest = 39.94982767\n",
      "bestIteration = 1875\n",
      "\n",
      "Shrink model to first 1876 iterations.\n",
      "[0]\tvalidation_0-rmse:62.91206\tvalidation_0-root_mean_squared_error:62.91206\n",
      "[100]\tvalidation_0-rmse:48.24677\tvalidation_0-root_mean_squared_error:48.24677\n",
      "[200]\tvalidation_0-rmse:43.85428\tvalidation_0-root_mean_squared_error:43.85428\n",
      "[300]\tvalidation_0-rmse:42.12208\tvalidation_0-root_mean_squared_error:42.12208\n",
      "[400]\tvalidation_0-rmse:41.35601\tvalidation_0-root_mean_squared_error:41.35601\n",
      "[500]\tvalidation_0-rmse:40.94695\tvalidation_0-root_mean_squared_error:40.94694\n",
      "[600]\tvalidation_0-rmse:40.66176\tvalidation_0-root_mean_squared_error:40.66175\n",
      "[700]\tvalidation_0-rmse:40.56541\tvalidation_0-root_mean_squared_error:40.56541\n",
      "[800]\tvalidation_0-rmse:40.48602\tvalidation_0-root_mean_squared_error:40.48602\n",
      "[900]\tvalidation_0-rmse:40.42491\tvalidation_0-root_mean_squared_error:40.42491\n",
      "[1000]\tvalidation_0-rmse:40.39440\tvalidation_0-root_mean_squared_error:40.39440\n",
      "[1100]\tvalidation_0-rmse:40.37412\tvalidation_0-root_mean_squared_error:40.37413\n",
      "[1200]\tvalidation_0-rmse:40.36361\tvalidation_0-root_mean_squared_error:40.36361\n",
      "[1300]\tvalidation_0-rmse:40.33042\tvalidation_0-root_mean_squared_error:40.33042\n",
      "[1400]\tvalidation_0-rmse:40.31313\tvalidation_0-root_mean_squared_error:40.31313\n",
      "[1500]\tvalidation_0-rmse:40.30478\tvalidation_0-root_mean_squared_error:40.30478\n",
      "[1600]\tvalidation_0-rmse:40.28154\tvalidation_0-root_mean_squared_error:40.28154\n",
      "[1700]\tvalidation_0-rmse:40.27780\tvalidation_0-root_mean_squared_error:40.27780\n",
      "[1800]\tvalidation_0-rmse:40.28298\tvalidation_0-root_mean_squared_error:40.28298\n",
      "[1900]\tvalidation_0-rmse:40.29059\tvalidation_0-root_mean_squared_error:40.29059\n",
      "[2000]\tvalidation_0-rmse:40.29394\tvalidation_0-root_mean_squared_error:40.29393\n",
      "[2100]\tvalidation_0-rmse:40.28009\tvalidation_0-root_mean_squared_error:40.28009\n",
      "[2169]\tvalidation_0-rmse:40.29098\tvalidation_0-root_mean_squared_error:40.29098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [12:39, 151.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest rmse:  40.820906211712305\n",
      "Average MSE across folds: 40.19116875561459\n",
      "Optimized weights per fold: [array([0.10237139, 0.42061016, 0.24973412, 0.22728433]), array([1.60207136e-01, 7.45050522e-01, 1.73531489e-12, 9.47423422e-02]), array([0.29020075, 0.3958741 , 0.25102614, 0.06289901]), array([0.        , 0.84613693, 0.13997827, 0.0138848 ]), array([1.24265178e-18, 6.76841564e-01, 1.50446706e-01, 1.72711730e-01])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the number of folds\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Placeholder for storing results\n",
    "fold_results = []\n",
    "optimized_weights_list = []\n",
    "unique_uids = merged_df['uid'].unique()  # Extract unique uids\n",
    "\n",
    "for train_ids, val_ids in tqdm(KFold(n_splits=5, shuffle=True, random_state=42).split(unique_uids)):\n",
    "    # Split the data\n",
    "    train_uids, val_uids = unique_uids[train_ids], unique_uids[val_ids]\n",
    "    keep_train = merged_df['uid'].isin(train_uids)\n",
    "    keep_val = merged_df['uid'].isin(val_uids)\n",
    "    \n",
    "    train_X, val_X = merged_df[keep_train], merged_df[keep_val]\n",
    "    train_X.drop(columns=['uid'], inplace=True)\n",
    "    val_X.drop(columns=['uid'], inplace=True)\n",
    "    train_y, val_y = y[keep_train], y[keep_val]\n",
    "    cats = train_X.select_dtypes(include=['object', 'category']).columns.to_list()\n",
    "    \n",
    "    \n",
    "    train_X_processed, val_X_processed = merged_df_processed[keep_train], merged_df_processed[keep_val]\n",
    "    train_X_processed.drop(columns=['uid'], inplace=True)\n",
    "    val_X_processed.drop(columns=['uid'], inplace=True)\n",
    "\n",
    "    # Train LightGBM\n",
    "    train_data = lgb.Dataset(train_X, label=train_y, categorical_feature='auto')\n",
    "    val_data = lgb.Dataset(val_X, label=val_y, categorical_feature='auto')\n",
    "\n",
    "    # Define parameters\n",
    "    params = {\n",
    "        'objective': 'regression',  # default for regression\n",
    "        'metric': 'rmse',\n",
    "        'learning_rate': 0.01,\n",
    "        'n_estimators': 10000,\n",
    "        'random_seed': 42\n",
    "    }\n",
    "\n",
    "    # Train the model with early stopping\n",
    "    model1 = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        valid_sets=[val_data],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=500, verbose=True),\n",
    "        ]\n",
    "    )\n",
    "    pred1 = model1.predict(val_X, num_iteration=model1.best_iteration)\n",
    "\n",
    "    # Train CatBoost\n",
    "    model2 = CatBoostRegressor(\n",
    "        iterations=10000, learning_rate=0.01, depth=10, loss_function='RMSE',\n",
    "        cat_features=cats,\n",
    "        verbose=100, early_stopping_rounds=500\n",
    "    )\n",
    "    model2.fit(train_X, train_y, eval_set=(val_X, val_y))\n",
    "    pred2 = model2.predict(val_X)\n",
    "    \n",
    "    # Train XGBoost\n",
    "    model3 = XGBRegressor(n_estimators=10000, learning_rate=0.01,\n",
    "        max_depth=3, random_state=42, \n",
    "        enable_categorical=True,\n",
    "        eval_metric=root_mean_squared_error,\n",
    "        early_stopping_rounds=500)\n",
    "    model3.fit(train_X, train_y, eval_set=[(val_X, val_y)], verbose=100)\n",
    "    pred3 = model3.predict(val_X)\n",
    "    \n",
    "    # Train Random Forest\n",
    "    model4 = RandomForestRegressor(n_estimators=1000, random_state=42)\n",
    "    model4.fit(train_X_processed, train_y)\n",
    "    pred4 = model4.predict(val_X_processed)\n",
    "    print(\"RandomForest rmse: \", root_mean_squared_error(val_y, pred4))\n",
    "\n",
    "    # Define loss function for weight optimization\n",
    "    def loss_function(weights):\n",
    "        w1, w2, w3, w4 = weights\n",
    "        combined_predictions = w1 * pred1 + w2 * pred2 + w3 * pred3 + w4 * pred4\n",
    "        mse = np.mean((combined_predictions - val_y) ** 2)\n",
    "        return mse\n",
    "\n",
    "    # Initial weights\n",
    "    initial_weights = [1/4, 1/4, 1/4, 1/4]\n",
    "\n",
    "    # Constraints: weights must sum to 1\n",
    "    constraints = {'type': 'eq', 'fun': lambda w: w[0] + w[1] + w[2] + w[3] - 1}\n",
    "\n",
    "    # Bounds: weights must be between 0 and 1\n",
    "    bounds = [(0, 1), (0, 1), (0, 1), (0, 1)]\n",
    "\n",
    "    # Optimize weights\n",
    "    result = minimize(loss_function, initial_weights, constraints=constraints, bounds=bounds)\n",
    "    optimized_weights = result.x\n",
    "\n",
    "    # Combine predictions using optimized weights\n",
    "    final_predictions = (\n",
    "        optimized_weights[0] * pred1 +\n",
    "        optimized_weights[1] * pred2 +\n",
    "        optimized_weights[2] * pred3 +\n",
    "        optimized_weights[3] * pred4\n",
    "    )\n",
    "\n",
    "    # Store results\n",
    "    fold_mse = root_mean_squared_error(val_y, final_predictions)  # RMSE\n",
    "    fold_results.append(fold_mse)\n",
    "    optimized_weights_list.append(optimized_weights)\n",
    "\n",
    "# Display results\n",
    "print(f\"Average MSE across folds: {np.mean(fold_results)}\")\n",
    "print(f\"Optimized weights per fold: {optimized_weights_list}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average MSE across folds: 40.19116875561459\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average MSE across folds: {np.mean(fold_results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.11606159, 0.53419643, 0.18496891, 0.16477308])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(optimized_weights_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/spa-d4/grad/mfe261/Projects/PREPARE/envs/lib/python3.9/site-packages/lightgbm/engine.py:204: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.067953 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2030\n",
      "[LightGBM] [Info] Number of data points in the train set: 4343, number of used features: 195\n",
      "[LightGBM] [Info] Start training from score 157.016809\n",
      "0:\tlearn: 60.6706359\ttotal: 5.27ms\tremaining: 52.7s\n",
      "100:\tlearn: 45.1469297\ttotal: 2.03s\tremaining: 3m 19s\n",
      "200:\tlearn: 39.1364111\ttotal: 4.27s\tremaining: 3m 28s\n",
      "300:\tlearn: 35.8111622\ttotal: 6.59s\tremaining: 3m 32s\n",
      "400:\tlearn: 33.7047091\ttotal: 8.94s\tremaining: 3m 33s\n",
      "500:\tlearn: 32.1122361\ttotal: 11.2s\tremaining: 3m 31s\n",
      "600:\tlearn: 30.7861504\ttotal: 13.4s\tremaining: 3m 28s\n",
      "700:\tlearn: 29.6481136\ttotal: 15.7s\tremaining: 3m 28s\n",
      "800:\tlearn: 28.6747853\ttotal: 18s\tremaining: 3m 27s\n",
      "900:\tlearn: 27.7920805\ttotal: 20.5s\tremaining: 3m 26s\n",
      "1000:\tlearn: 26.9744507\ttotal: 23s\tremaining: 3m 26s\n",
      "1100:\tlearn: 26.2333420\ttotal: 25.6s\tremaining: 3m 27s\n",
      "1200:\tlearn: 25.5219533\ttotal: 28.3s\tremaining: 3m 27s\n",
      "1300:\tlearn: 24.9012612\ttotal: 31s\tremaining: 3m 27s\n",
      "1400:\tlearn: 24.2192386\ttotal: 33.6s\tremaining: 3m 26s\n",
      "1500:\tlearn: 23.5690407\ttotal: 36.4s\tremaining: 3m 25s\n",
      "1600:\tlearn: 23.0208701\ttotal: 39.1s\tremaining: 3m 24s\n",
      "1700:\tlearn: 22.4183496\ttotal: 41.8s\tremaining: 3m 23s\n",
      "1800:\tlearn: 21.8250388\ttotal: 44.5s\tremaining: 3m 22s\n",
      "1900:\tlearn: 21.2548819\ttotal: 47.3s\tremaining: 3m 21s\n",
      "2000:\tlearn: 20.7353817\ttotal: 50s\tremaining: 3m 20s\n",
      "2100:\tlearn: 20.2018202\ttotal: 52.7s\tremaining: 3m 18s\n",
      "2200:\tlearn: 19.7175311\ttotal: 55.5s\tremaining: 3m 16s\n",
      "2300:\tlearn: 19.2933891\ttotal: 58.2s\tremaining: 3m 14s\n",
      "2400:\tlearn: 18.8834159\ttotal: 1m 1s\tremaining: 3m 13s\n",
      "2500:\tlearn: 18.4826053\ttotal: 1m 3s\tremaining: 3m 11s\n",
      "2600:\tlearn: 18.1223336\ttotal: 1m 6s\tremaining: 3m 9s\n",
      "2700:\tlearn: 17.8230903\ttotal: 1m 9s\tremaining: 3m 7s\n",
      "2800:\tlearn: 17.4829108\ttotal: 1m 12s\tremaining: 3m 5s\n",
      "2900:\tlearn: 17.1565442\ttotal: 1m 14s\tremaining: 3m 2s\n",
      "3000:\tlearn: 16.8167499\ttotal: 1m 17s\tremaining: 3m\n",
      "3100:\tlearn: 16.4934565\ttotal: 1m 20s\tremaining: 2m 58s\n",
      "3200:\tlearn: 16.1946755\ttotal: 1m 22s\tremaining: 2m 56s\n",
      "3300:\tlearn: 15.8919306\ttotal: 1m 25s\tremaining: 2m 53s\n",
      "3400:\tlearn: 15.5935752\ttotal: 1m 28s\tremaining: 2m 51s\n",
      "3500:\tlearn: 15.3398997\ttotal: 1m 31s\tremaining: 2m 49s\n",
      "3600:\tlearn: 15.0667628\ttotal: 1m 33s\tremaining: 2m 46s\n",
      "3700:\tlearn: 14.8179294\ttotal: 1m 36s\tremaining: 2m 44s\n",
      "3800:\tlearn: 14.5927804\ttotal: 1m 39s\tremaining: 2m 42s\n",
      "3900:\tlearn: 14.3565150\ttotal: 1m 42s\tremaining: 2m 40s\n",
      "4000:\tlearn: 14.1295224\ttotal: 1m 45s\tremaining: 2m 37s\n",
      "4100:\tlearn: 13.8956108\ttotal: 1m 48s\tremaining: 2m 35s\n",
      "4200:\tlearn: 13.6791863\ttotal: 1m 51s\tremaining: 2m 33s\n",
      "4300:\tlearn: 13.4319041\ttotal: 1m 53s\tremaining: 2m 30s\n",
      "4400:\tlearn: 13.2479084\ttotal: 1m 56s\tremaining: 2m 28s\n",
      "4500:\tlearn: 13.0314234\ttotal: 1m 59s\tremaining: 2m 26s\n",
      "4600:\tlearn: 12.8403881\ttotal: 2m 2s\tremaining: 2m 23s\n",
      "4700:\tlearn: 12.6260784\ttotal: 2m 5s\tremaining: 2m 21s\n",
      "4800:\tlearn: 12.4249301\ttotal: 2m 8s\tremaining: 2m 18s\n",
      "4900:\tlearn: 12.1974478\ttotal: 2m 10s\tremaining: 2m 16s\n",
      "5000:\tlearn: 11.9820822\ttotal: 2m 13s\tremaining: 2m 13s\n",
      "5100:\tlearn: 11.7708254\ttotal: 2m 16s\tremaining: 2m 11s\n",
      "5200:\tlearn: 11.5620095\ttotal: 2m 19s\tremaining: 2m 8s\n",
      "5300:\tlearn: 11.3949467\ttotal: 2m 22s\tremaining: 2m 6s\n",
      "5400:\tlearn: 11.2067626\ttotal: 2m 25s\tremaining: 2m 3s\n",
      "5500:\tlearn: 11.0063516\ttotal: 2m 27s\tremaining: 2m 1s\n",
      "5600:\tlearn: 10.8343975\ttotal: 2m 30s\tremaining: 1m 58s\n",
      "5700:\tlearn: 10.6738791\ttotal: 2m 33s\tremaining: 1m 55s\n",
      "5800:\tlearn: 10.5216598\ttotal: 2m 36s\tremaining: 1m 53s\n",
      "5900:\tlearn: 10.3404130\ttotal: 2m 39s\tremaining: 1m 50s\n",
      "6000:\tlearn: 10.1929364\ttotal: 2m 42s\tremaining: 1m 48s\n",
      "6100:\tlearn: 10.0316053\ttotal: 2m 45s\tremaining: 1m 45s\n",
      "6200:\tlearn: 9.8862555\ttotal: 2m 47s\tremaining: 1m 42s\n",
      "6300:\tlearn: 9.7282211\ttotal: 2m 50s\tremaining: 1m 40s\n",
      "6400:\tlearn: 9.5857856\ttotal: 2m 53s\tremaining: 1m 37s\n",
      "6500:\tlearn: 9.4332055\ttotal: 2m 56s\tremaining: 1m 34s\n",
      "6600:\tlearn: 9.3185877\ttotal: 2m 59s\tremaining: 1m 32s\n",
      "6700:\tlearn: 9.1995161\ttotal: 3m 2s\tremaining: 1m 29s\n",
      "6800:\tlearn: 9.0612500\ttotal: 3m 5s\tremaining: 1m 27s\n",
      "6900:\tlearn: 8.9414655\ttotal: 3m 7s\tremaining: 1m 24s\n",
      "7000:\tlearn: 8.8002136\ttotal: 3m 10s\tremaining: 1m 21s\n",
      "7100:\tlearn: 8.6881536\ttotal: 3m 13s\tremaining: 1m 19s\n",
      "7200:\tlearn: 8.5860575\ttotal: 3m 16s\tremaining: 1m 16s\n",
      "7300:\tlearn: 8.4729600\ttotal: 3m 19s\tremaining: 1m 13s\n",
      "7400:\tlearn: 8.3443196\ttotal: 3m 22s\tremaining: 1m 10s\n",
      "7500:\tlearn: 8.2262028\ttotal: 3m 24s\tremaining: 1m 8s\n",
      "7600:\tlearn: 8.1275808\ttotal: 3m 27s\tremaining: 1m 5s\n",
      "7700:\tlearn: 8.0103429\ttotal: 3m 30s\tremaining: 1m 2s\n",
      "7800:\tlearn: 7.9118349\ttotal: 3m 33s\tremaining: 1m\n",
      "7900:\tlearn: 7.8076070\ttotal: 3m 36s\tremaining: 57.5s\n",
      "8000:\tlearn: 7.7033755\ttotal: 3m 39s\tremaining: 54.8s\n",
      "8100:\tlearn: 7.6126451\ttotal: 3m 42s\tremaining: 52.1s\n",
      "8200:\tlearn: 7.5105898\ttotal: 3m 44s\tremaining: 49.3s\n",
      "8300:\tlearn: 7.4036451\ttotal: 3m 47s\tremaining: 46.6s\n",
      "8400:\tlearn: 7.3147367\ttotal: 3m 51s\tremaining: 44s\n",
      "8500:\tlearn: 7.2267071\ttotal: 3m 54s\tremaining: 41.3s\n",
      "8600:\tlearn: 7.1303000\ttotal: 3m 56s\tremaining: 38.5s\n",
      "8700:\tlearn: 7.0332901\ttotal: 3m 59s\tremaining: 35.8s\n",
      "8800:\tlearn: 6.9415078\ttotal: 4m 2s\tremaining: 33.1s\n",
      "8900:\tlearn: 6.8573814\ttotal: 4m 5s\tremaining: 30.3s\n",
      "9000:\tlearn: 6.7682558\ttotal: 4m 8s\tremaining: 27.6s\n",
      "9100:\tlearn: 6.6880863\ttotal: 4m 11s\tremaining: 24.8s\n",
      "9200:\tlearn: 6.5966540\ttotal: 4m 13s\tremaining: 22s\n",
      "9300:\tlearn: 6.5259766\ttotal: 4m 16s\tremaining: 19.3s\n",
      "9400:\tlearn: 6.4256777\ttotal: 4m 19s\tremaining: 16.5s\n",
      "9500:\tlearn: 6.3494734\ttotal: 4m 22s\tremaining: 13.8s\n",
      "9600:\tlearn: 6.2831951\ttotal: 4m 25s\tremaining: 11s\n",
      "9700:\tlearn: 6.1957445\ttotal: 4m 27s\tremaining: 8.26s\n",
      "9800:\tlearn: 6.1135583\ttotal: 4m 30s\tremaining: 5.5s\n",
      "9900:\tlearn: 6.0433805\ttotal: 4m 33s\tremaining: 2.73s\n",
      "9999:\tlearn: 5.9743884\ttotal: 4m 36s\tremaining: 0us\n",
      "Final blended predictions for the test dataset:\n",
      "[186 207 207 ... 200 180 154]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average weights from cross-validation\n",
    "average_weights = np.mean(optimized_weights_list, axis=0)\n",
    "\n",
    "# Train models on the entire training dataset\n",
    "train_data = lgb.Dataset(merged_df, label=y, categorical_feature='auto')\n",
    "\n",
    "params = {\n",
    "    'objective': 'regression',  # default for regression\n",
    "    'metric': 'rmse',\n",
    "    'learning_rate': 0.01,\n",
    "    'n_estimators': 10000,\n",
    "    'random_seed': 42\n",
    "}\n",
    "final_model1 = lgb.train(\n",
    "    params,\n",
    "    train_data\n",
    ")\n",
    "\n",
    "final_model2 = CatBoostRegressor(\n",
    "    iterations=10000, learning_rate=0.01, depth=10, loss_function='RMSE',\n",
    "    cat_features=merged_df.select_dtypes(include=['object', 'category']).columns.to_list(),\n",
    "    verbose=100\n",
    ")\n",
    "final_model2.fit(merged_df, y)\n",
    "\n",
    "final_model3 = XGBRegressor(n_estimators=10000, learning_rate=0.01,\n",
    "        max_depth=3, random_state=42, \n",
    "        enable_categorical=True,\n",
    "        eval_metric=root_mean_squared_error)\n",
    "final_model3.fit(merged_df, y)\n",
    "\n",
    "# Train Random Forest\n",
    "final_model4 = RandomForestRegressor(n_estimators=1000, random_state=42)\n",
    "final_model4.fit(merged_df_processed, y)\n",
    "\n",
    "# Generate predictions on the test dataset\n",
    "test_pred1 = final_model1.predict(merged_test)\n",
    "test_pred2 = final_model2.predict(merged_test)\n",
    "test_pred3 = final_model3.predict(merged_test)\n",
    "test_pred4 = final_model4.predict(merged_test_processed)\n",
    "\n",
    "# Combine the predictions using the average weights\n",
    "final_test_predictions = (\n",
    "    average_weights[0] * test_pred1 + average_weights[1] * test_pred2 + average_weights[2] * test_pred3 + average_weights[3] * test_pred4\n",
    ")\n",
    "\n",
    "# Optionally round predictions if required (e.g., for classification tasks)\n",
    "final_test_predictions = np.round(final_test_predictions).astype(int)\n",
    "\n",
    "# Display final predictions\n",
    "print(\"Final blended predictions for the test dataset:\")\n",
    "print(final_test_predictions)\n",
    "\n",
    "ss['composite_score']=final_test_predictions\n",
    "#generate submission\n",
    "ss.to_csv('../dataset/LGBM_and_CatBoost_and_XGBoost_RandomForest_engineering.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
